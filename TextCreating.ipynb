{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Level LSTM in PyTorch\n",
    "\n",
    "In this notebook, represent LSTM with PyTorch. The network will train character by character on some text, then generate new text character by character. As an example, It will be trained on Anna Karenina. **This model will be able to generate new text based on the text from the book!**\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "In the cells, below, I'm creating a couple **dictionaries** to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(0, 'C'), (1, '4'), (2, 'Q'), (3, 't'), (4, '2'), (5, 'X'), (6, 'R'), (7, 'l'), (8, '*'), (9, '0'), (10, 'x'), (11, '('), (12, 'v'), (13, 'h'), (14, '-'), (15, 'I'), (16, 'U'), (17, '_'), (18, 'J'), (19, '@'), (20, '$'), (21, '\\n'), (22, '5'), (23, 'g'), (24, ')'), (25, 'c'), (26, 'f'), (27, 'a'), (28, 's'), (29, '.'), (30, 'Y'), (31, '?'), (32, '3'), (33, '/'), (34, 'w'), (35, 'A'), (36, 'D'), (37, 'H'), (38, 'F'), (39, 'o'), (40, '8'), (41, '7'), (42, '`'), (43, 'G'), (44, '!'), (45, '1'), (46, ','), (47, 'W'), (48, 'n'), (49, 'E'), (50, '&'), (51, \"'\"), (52, 'Z'), (53, ';'), (54, 'k'), (55, 'q'), (56, 'u'), (57, '\"'), (58, 'L'), (59, '9'), (60, 'd'), (61, ' '), (62, '6'), (63, 'i'), (64, 'y'), (65, 'j'), (66, ':'), (67, 'r'), (68, 'B'), (69, 'S'), (70, 'N'), (71, 'p'), (72, 'e'), (73, 'z'), (74, 'V'), (75, 'P'), (76, 'M'), (77, 'T'), (78, 'K'), (79, 'b'), (80, '%'), (81, 'O'), (82, 'm')])\n"
     ]
    }
   ],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# 1. int2char, maps integers to characters\n",
    "# 2. char2int, maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "print(int2char.items())\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoded text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 13, 27, 71,  3, 72, 67, 61, 45, 21, 21, 21, 37, 27, 71, 71, 64,\n",
       "       61, 26, 27, 82, 63,  7, 63, 72, 28, 61, 27, 67, 72, 61, 27,  7,  7,\n",
       "       61, 27,  7, 63, 54, 72, 53, 61, 72, 12, 72, 67, 64, 61, 56, 48, 13,\n",
       "       27, 71, 71, 64, 61, 26, 27, 82, 63,  7, 64, 61, 63, 28, 61, 56, 48,\n",
       "       13, 27, 71, 71, 64, 61, 63, 48, 61, 63,  3, 28, 61, 39, 34, 48, 21,\n",
       "       34, 27, 64, 29, 21, 21, 49, 12, 72, 67, 64,  3, 13, 63, 48])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data\n",
    "Network exepts **one-hot encoded characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "Before training data will be divided into mini-batches.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches that can be made\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Implementation\n",
    "\n",
    "Now I'll make some data sets to check out what's going on with batch data. Here, as an example, with batch size of 8 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 0 13 27 71  3 72 67 61 45 21]\n",
      " [28 39 48 61  3 13 27  3 61 27]\n",
      " [72 48 60 61 39 67 61 27 61 26]\n",
      " [28 61  3 13 72 61 25 13 63 72]\n",
      " [61 28 27 34 61 13 72 67 61  3]\n",
      " [25 56 28 28 63 39 48 61 27 48]\n",
      " [61 35 48 48 27 61 13 27 60 61]\n",
      " [81 79  7 39 48 28 54 64 29 61]]\n",
      "\n",
      "y\n",
      " [[13 27 71  3 72 67 61 45 21 21]\n",
      " [39 48 61  3 13 27  3 61 27  3]\n",
      " [48 60 61 39 67 61 27 61 26 39]\n",
      " [61  3 13 72 61 25 13 63 72 26]\n",
      " [28 27 34 61 13 72 67 61  3 72]\n",
      " [56 28 28 63 39 48 61 27 48 60]\n",
      " [35 48 48 27 61 13 27 60 61 28]\n",
      " [79  7 39 48 28 54 64 29 61 57]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "\n",
    "# data should be shifted over one step for y\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Defining the network with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "```                           \n",
    "`input_size` is the number of characters this cell expects to see as sequential input\n",
    "`n_hidden` is the number of units in the hidden layers in the cell    \n",
    "`n_layers` is the number of layers    \n",
    "`dropout_prob` is the dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ##  Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ##  pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/27... Step: 10... Loss: 3.2473... Val Loss: 3.2347\n",
      "Epoch: 1/27... Step: 20... Loss: 3.1577... Val Loss: 3.1467\n",
      "Epoch: 1/27... Step: 30... Loss: 3.1138... Val Loss: 3.1346\n",
      "Epoch: 1/27... Step: 40... Loss: 3.1067... Val Loss: 3.1283\n",
      "Epoch: 1/27... Step: 50... Loss: 3.1263... Val Loss: 3.1276\n",
      "Epoch: 1/27... Step: 60... Loss: 3.1081... Val Loss: 3.1238\n",
      "Epoch: 1/27... Step: 70... Loss: 3.0536... Val Loss: 3.1232\n",
      "Epoch: 1/27... Step: 80... Loss: 3.1011... Val Loss: 3.1213\n",
      "Epoch: 1/27... Step: 90... Loss: 3.0660... Val Loss: 3.1112\n",
      "Epoch: 1/27... Step: 100... Loss: 3.0696... Val Loss: 3.0904\n",
      "Epoch: 1/27... Step: 110... Loss: 2.9959... Val Loss: 3.0407\n",
      "Epoch: 1/27... Step: 120... Loss: 2.9881... Val Loss: 2.9539\n",
      "Epoch: 1/27... Step: 130... Loss: 2.9240... Val Loss: 2.9243\n",
      "Epoch: 1/27... Step: 140... Loss: 2.7555... Val Loss: 2.8232\n",
      "Epoch: 1/27... Step: 150... Loss: 2.6804... Val Loss: 2.7172\n",
      "Epoch: 1/27... Step: 160... Loss: 2.6114... Val Loss: 2.6272\n",
      "Epoch: 1/27... Step: 170... Loss: 2.6264... Val Loss: 2.5802\n",
      "Epoch: 1/27... Step: 180... Loss: 2.5983... Val Loss: 2.5369\n",
      "Epoch: 1/27... Step: 190... Loss: 2.5044... Val Loss: 2.4901\n",
      "Epoch: 1/27... Step: 200... Loss: 2.4512... Val Loss: 2.4703\n",
      "Epoch: 1/27... Step: 210... Loss: 2.4113... Val Loss: 2.4438\n",
      "Epoch: 1/27... Step: 220... Loss: 2.4120... Val Loss: 2.4200\n",
      "Epoch: 1/27... Step: 230... Loss: 2.3739... Val Loss: 2.3949\n",
      "Epoch: 1/27... Step: 240... Loss: 2.3770... Val Loss: 2.3777\n",
      "Epoch: 1/27... Step: 250... Loss: 2.3984... Val Loss: 2.3876\n",
      "Epoch: 1/27... Step: 260... Loss: 2.3623... Val Loss: 2.3519\n",
      "Epoch: 1/27... Step: 270... Loss: 2.2465... Val Loss: 2.3219\n",
      "Epoch: 1/27... Step: 280... Loss: 2.3101... Val Loss: 2.3079\n",
      "Epoch: 1/27... Step: 290... Loss: 2.2520... Val Loss: 2.2856\n",
      "Epoch: 1/27... Step: 300... Loss: 2.3409... Val Loss: 2.2726\n",
      "Epoch: 1/27... Step: 310... Loss: 2.2395... Val Loss: 2.2614\n",
      "Epoch: 1/27... Step: 320... Loss: 2.1979... Val Loss: 2.2392\n",
      "Epoch: 1/27... Step: 330... Loss: 2.2198... Val Loss: 2.2183\n",
      "Epoch: 1/27... Step: 340... Loss: 2.1953... Val Loss: 2.2205\n",
      "Epoch: 1/27... Step: 350... Loss: 2.1991... Val Loss: 2.2129\n",
      "Epoch: 1/27... Step: 360... Loss: 2.1704... Val Loss: 2.1957\n",
      "Epoch: 1/27... Step: 370... Loss: 2.2500... Val Loss: 2.1774\n",
      "Epoch: 1/27... Step: 380... Loss: 2.1710... Val Loss: 2.1637\n",
      "Epoch: 1/27... Step: 390... Loss: 2.1443... Val Loss: 2.1528\n",
      "Epoch: 1/27... Step: 400... Loss: 2.0954... Val Loss: 2.1444\n",
      "Epoch: 1/27... Step: 410... Loss: 2.1662... Val Loss: 2.1375\n",
      "Epoch: 1/27... Step: 420... Loss: 2.1059... Val Loss: 2.1194\n",
      "Epoch: 1/27... Step: 430... Loss: 2.1750... Val Loss: 2.1018\n",
      "Epoch: 1/27... Step: 440... Loss: 2.0825... Val Loss: 2.0884\n",
      "Epoch: 1/27... Step: 450... Loss: 2.0798... Val Loss: 2.0867\n",
      "Epoch: 1/27... Step: 460... Loss: 2.0702... Val Loss: 2.0718\n",
      "Epoch: 1/27... Step: 470... Loss: 2.1226... Val Loss: 2.0649\n",
      "Epoch: 1/27... Step: 480... Loss: 1.9876... Val Loss: 2.0539\n",
      "Epoch: 1/27... Step: 490... Loss: 2.0282... Val Loss: 2.0474\n",
      "Epoch: 1/27... Step: 500... Loss: 2.0353... Val Loss: 2.0429\n",
      "Epoch: 1/27... Step: 510... Loss: 2.0084... Val Loss: 2.0244\n",
      "Epoch: 1/27... Step: 520... Loss: 2.0615... Val Loss: 2.0184\n",
      "Epoch: 1/27... Step: 530... Loss: 1.9486... Val Loss: 2.0024\n",
      "Epoch: 1/27... Step: 540... Loss: 1.9338... Val Loss: 2.0053\n",
      "Epoch: 1/27... Step: 550... Loss: 1.9609... Val Loss: 1.9947\n",
      "Epoch: 1/27... Step: 560... Loss: 1.9801... Val Loss: 1.9877\n",
      "Epoch: 1/27... Step: 570... Loss: 2.0093... Val Loss: 1.9882\n",
      "Epoch: 1/27... Step: 580... Loss: 1.8881... Val Loss: 1.9678\n",
      "Epoch: 1/27... Step: 590... Loss: 1.8812... Val Loss: 1.9601\n",
      "Epoch: 1/27... Step: 600... Loss: 1.9071... Val Loss: 1.9495\n",
      "Epoch: 1/27... Step: 610... Loss: 1.9295... Val Loss: 1.9495\n",
      "Epoch: 1/27... Step: 620... Loss: 1.9411... Val Loss: 1.9390\n",
      "Epoch: 1/27... Step: 630... Loss: 1.9944... Val Loss: 1.9314\n",
      "Epoch: 1/27... Step: 640... Loss: 1.8752... Val Loss: 1.9318\n",
      "Epoch: 1/27... Step: 650... Loss: 1.8938... Val Loss: 1.9230\n",
      "Epoch: 1/27... Step: 660... Loss: 1.9323... Val Loss: 1.9205\n",
      "Epoch: 1/27... Step: 670... Loss: 1.9176... Val Loss: 1.9081\n",
      "Epoch: 1/27... Step: 680... Loss: 1.9335... Val Loss: 1.8983\n",
      "Epoch: 1/27... Step: 690... Loss: 1.9550... Val Loss: 1.8919\n",
      "Epoch: 1/27... Step: 700... Loss: 1.8919... Val Loss: 1.8884\n",
      "Epoch: 1/27... Step: 710... Loss: 1.8365... Val Loss: 1.8777\n",
      "Epoch: 1/27... Step: 720... Loss: 1.9775... Val Loss: 1.8731\n",
      "Epoch: 1/27... Step: 730... Loss: 1.8329... Val Loss: 1.8781\n",
      "Epoch: 1/27... Step: 740... Loss: 1.8497... Val Loss: 1.8649\n",
      "Epoch: 1/27... Step: 750... Loss: 1.9402... Val Loss: 1.8568\n",
      "Epoch: 1/27... Step: 760... Loss: 1.7517... Val Loss: 1.8576\n",
      "Epoch: 1/27... Step: 770... Loss: 1.8029... Val Loss: 1.8393\n",
      "Epoch: 1/27... Step: 780... Loss: 1.9047... Val Loss: 1.8403\n",
      "Epoch: 1/27... Step: 790... Loss: 1.8255... Val Loss: 1.8326\n",
      "Epoch: 1/27... Step: 800... Loss: 1.8132... Val Loss: 1.8268\n",
      "Epoch: 1/27... Step: 810... Loss: 1.8370... Val Loss: 1.8262\n",
      "Epoch: 1/27... Step: 820... Loss: 1.8609... Val Loss: 1.8228\n",
      "Epoch: 1/27... Step: 830... Loss: 1.8276... Val Loss: 1.8148\n",
      "Epoch: 1/27... Step: 840... Loss: 1.7385... Val Loss: 1.8047\n",
      "Epoch: 1/27... Step: 850... Loss: 1.7812... Val Loss: 1.7996\n",
      "Epoch: 1/27... Step: 860... Loss: 1.7509... Val Loss: 1.7968\n",
      "Epoch: 1/27... Step: 870... Loss: 1.6899... Val Loss: 1.7921\n",
      "Epoch: 1/27... Step: 880... Loss: 1.8636... Val Loss: 1.7874\n",
      "Epoch: 1/27... Step: 890... Loss: 1.8281... Val Loss: 1.7840\n",
      "Epoch: 1/27... Step: 900... Loss: 1.7650... Val Loss: 1.7746\n",
      "Epoch: 1/27... Step: 910... Loss: 1.7725... Val Loss: 1.7725\n",
      "Epoch: 1/27... Step: 920... Loss: 1.8691... Val Loss: 1.7679\n",
      "Epoch: 1/27... Step: 930... Loss: 1.7324... Val Loss: 1.7664\n",
      "Epoch: 1/27... Step: 940... Loss: 1.7705... Val Loss: 1.7602\n",
      "Epoch: 1/27... Step: 950... Loss: 1.7476... Val Loss: 1.7560\n",
      "Epoch: 1/27... Step: 960... Loss: 1.8399... Val Loss: 1.7557\n",
      "Epoch: 1/27... Step: 970... Loss: 1.6471... Val Loss: 1.7467\n",
      "Epoch: 1/27... Step: 980... Loss: 1.6666... Val Loss: 1.7491\n",
      "Epoch: 1/27... Step: 990... Loss: 1.7896... Val Loss: 1.7434\n",
      "Epoch: 1/27... Step: 1000... Loss: 1.7607... Val Loss: 1.7339\n",
      "Epoch: 1/27... Step: 1010... Loss: 1.7743... Val Loss: 1.7286\n",
      "Epoch: 1/27... Step: 1020... Loss: 1.7438... Val Loss: 1.7278\n",
      "Epoch: 1/27... Step: 1030... Loss: 1.6691... Val Loss: 1.7267\n",
      "Epoch: 1/27... Step: 1040... Loss: 1.6358... Val Loss: 1.7189\n",
      "Epoch: 1/27... Step: 1050... Loss: 1.6913... Val Loss: 1.7237\n",
      "Epoch: 1/27... Step: 1060... Loss: 1.7088... Val Loss: 1.7282\n",
      "Epoch: 1/27... Step: 1070... Loss: 1.6971... Val Loss: 1.7229\n",
      "Epoch: 1/27... Step: 1080... Loss: 1.7015... Val Loss: 1.7122\n",
      "Epoch: 1/27... Step: 1090... Loss: 1.5742... Val Loss: 1.7102\n",
      "Epoch: 1/27... Step: 1100... Loss: 1.6550... Val Loss: 1.7047\n",
      "Epoch: 1/27... Step: 1110... Loss: 1.6483... Val Loss: 1.7018\n",
      "Epoch: 1/27... Step: 1120... Loss: 1.6641... Val Loss: 1.6982\n",
      "Epoch: 1/27... Step: 1130... Loss: 1.6092... Val Loss: 1.6934\n",
      "Epoch: 1/27... Step: 1140... Loss: 1.5887... Val Loss: 1.6914\n",
      "Epoch: 1/27... Step: 1150... Loss: 1.6471... Val Loss: 1.6843\n",
      "Epoch: 1/27... Step: 1160... Loss: 1.6327... Val Loss: 1.6850\n",
      "Epoch: 1/27... Step: 1170... Loss: 1.7042... Val Loss: 1.6792\n",
      "Epoch: 1/27... Step: 1180... Loss: 1.6994... Val Loss: 1.6716\n",
      "Epoch: 1/27... Step: 1190... Loss: 1.6337... Val Loss: 1.6760\n",
      "Epoch: 1/27... Step: 1200... Loss: 1.6943... Val Loss: 1.6730\n",
      "Epoch: 1/27... Step: 1210... Loss: 1.5614... Val Loss: 1.6724\n",
      "Epoch: 1/27... Step: 1220... Loss: 1.6626... Val Loss: 1.6678\n",
      "Epoch: 1/27... Step: 1230... Loss: 1.6529... Val Loss: 1.6672\n",
      "Epoch: 1/27... Step: 1240... Loss: 1.6425... Val Loss: 1.6641\n",
      "Epoch: 1/27... Step: 1250... Loss: 1.6472... Val Loss: 1.6657\n",
      "Epoch: 1/27... Step: 1260... Loss: 1.6338... Val Loss: 1.6568\n",
      "Epoch: 1/27... Step: 1270... Loss: 1.6172... Val Loss: 1.6547\n",
      "Epoch: 1/27... Step: 1280... Loss: 1.5464... Val Loss: 1.6517\n",
      "Epoch: 1/27... Step: 1290... Loss: 1.6818... Val Loss: 1.6533\n",
      "Epoch: 1/27... Step: 1300... Loss: 1.6193... Val Loss: 1.6461\n",
      "Epoch: 1/27... Step: 1310... Loss: 1.6599... Val Loss: 1.6453\n",
      "Epoch: 1/27... Step: 1320... Loss: 1.6124... Val Loss: 1.6437\n",
      "Epoch: 1/27... Step: 1330... Loss: 1.5780... Val Loss: 1.6403\n",
      "Epoch: 1/27... Step: 1340... Loss: 1.6784... Val Loss: 1.6392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/27... Step: 1350... Loss: 1.6281... Val Loss: 1.6388\n",
      "Epoch: 1/27... Step: 1360... Loss: 1.5561... Val Loss: 1.6339\n",
      "Epoch: 1/27... Step: 1370... Loss: 1.5816... Val Loss: 1.6347\n",
      "Epoch: 1/27... Step: 1380... Loss: 1.7323... Val Loss: 1.6306\n",
      "Epoch: 1/27... Step: 1390... Loss: 1.6063... Val Loss: 1.6256\n",
      "Epoch: 2/27... Step: 1400... Loss: 1.6051... Val Loss: 1.6303\n",
      "Epoch: 2/27... Step: 1410... Loss: 1.5341... Val Loss: 1.6229\n",
      "Epoch: 2/27... Step: 1420... Loss: 1.6312... Val Loss: 1.6279\n",
      "Epoch: 2/27... Step: 1430... Loss: 1.5922... Val Loss: 1.6187\n",
      "Epoch: 2/27... Step: 1440... Loss: 1.5881... Val Loss: 1.6225\n",
      "Epoch: 2/27... Step: 1450... Loss: 1.6041... Val Loss: 1.6116\n",
      "Epoch: 2/27... Step: 1460... Loss: 1.5796... Val Loss: 1.6134\n",
      "Epoch: 2/27... Step: 1470... Loss: 1.5394... Val Loss: 1.6151\n",
      "Epoch: 2/27... Step: 1480... Loss: 1.6056... Val Loss: 1.6099\n",
      "Epoch: 2/27... Step: 1490... Loss: 1.6614... Val Loss: 1.6075\n",
      "Epoch: 2/27... Step: 1500... Loss: 1.5308... Val Loss: 1.6076\n",
      "Epoch: 2/27... Step: 1510... Loss: 1.5183... Val Loss: 1.6033\n",
      "Epoch: 2/27... Step: 1520... Loss: 1.5421... Val Loss: 1.6075\n",
      "Epoch: 2/27... Step: 1530... Loss: 1.4944... Val Loss: 1.6062\n",
      "Epoch: 2/27... Step: 1540... Loss: 1.6003... Val Loss: 1.5978\n",
      "Epoch: 2/27... Step: 1550... Loss: 1.6715... Val Loss: 1.5993\n",
      "Epoch: 2/27... Step: 1560... Loss: 1.5859... Val Loss: 1.5950\n",
      "Epoch: 2/27... Step: 1570... Loss: 1.5348... Val Loss: 1.5961\n",
      "Epoch: 2/27... Step: 1580... Loss: 1.5921... Val Loss: 1.5913\n",
      "Epoch: 2/27... Step: 1590... Loss: 1.5900... Val Loss: 1.5933\n",
      "Epoch: 2/27... Step: 1600... Loss: 1.5482... Val Loss: 1.5927\n",
      "Epoch: 2/27... Step: 1610... Loss: 1.5587... Val Loss: 1.5864\n",
      "Epoch: 2/27... Step: 1620... Loss: 1.5632... Val Loss: 1.5884\n",
      "Epoch: 2/27... Step: 1630... Loss: 1.4897... Val Loss: 1.5832\n",
      "Epoch: 2/27... Step: 1640... Loss: 1.5394... Val Loss: 1.5803\n",
      "Epoch: 2/27... Step: 1650... Loss: 1.6123... Val Loss: 1.5763\n",
      "Epoch: 2/27... Step: 1660... Loss: 1.5324... Val Loss: 1.5756\n",
      "Epoch: 2/27... Step: 1670... Loss: 1.5262... Val Loss: 1.5774\n",
      "Epoch: 2/27... Step: 1680... Loss: 1.5279... Val Loss: 1.5742\n",
      "Epoch: 2/27... Step: 1690... Loss: 1.6208... Val Loss: 1.5718\n",
      "Epoch: 2/27... Step: 1700... Loss: 1.5159... Val Loss: 1.5740\n",
      "Epoch: 2/27... Step: 1710... Loss: 1.5332... Val Loss: 1.5843\n",
      "Epoch: 2/27... Step: 1720... Loss: 1.5224... Val Loss: 1.5670\n",
      "Epoch: 2/27... Step: 1730... Loss: 1.5654... Val Loss: 1.5717\n",
      "Epoch: 2/27... Step: 1740... Loss: 1.6342... Val Loss: 1.5765\n",
      "Epoch: 2/27... Step: 1750... Loss: 1.5434... Val Loss: 1.5640\n",
      "Epoch: 2/27... Step: 1760... Loss: 1.5558... Val Loss: 1.5668\n",
      "Epoch: 2/27... Step: 1770... Loss: 1.6244... Val Loss: 1.5617\n",
      "Epoch: 2/27... Step: 1780... Loss: 1.5501... Val Loss: 1.5604\n",
      "Epoch: 2/27... Step: 1790... Loss: 1.5387... Val Loss: 1.5591\n",
      "Epoch: 2/27... Step: 1800... Loss: 1.5262... Val Loss: 1.5575\n",
      "Epoch: 2/27... Step: 1810... Loss: 1.4981... Val Loss: 1.5521\n",
      "Epoch: 2/27... Step: 1820... Loss: 1.4966... Val Loss: 1.5588\n",
      "Epoch: 2/27... Step: 1830... Loss: 1.5716... Val Loss: 1.5510\n",
      "Epoch: 2/27... Step: 1840... Loss: 1.4472... Val Loss: 1.5510\n",
      "Epoch: 2/27... Step: 1850... Loss: 1.5167... Val Loss: 1.5568\n",
      "Epoch: 2/27... Step: 1860... Loss: 1.5526... Val Loss: 1.5475\n",
      "Epoch: 2/27... Step: 1870... Loss: 1.4368... Val Loss: 1.5437\n",
      "Epoch: 2/27... Step: 1880... Loss: 1.4664... Val Loss: 1.5420\n",
      "Epoch: 2/27... Step: 1890... Loss: 1.5554... Val Loss: 1.5434\n",
      "Epoch: 2/27... Step: 1900... Loss: 1.5314... Val Loss: 1.5445\n",
      "Epoch: 2/27... Step: 1910... Loss: 1.5956... Val Loss: 1.5416\n",
      "Epoch: 2/27... Step: 1920... Loss: 1.5212... Val Loss: 1.5389\n",
      "Epoch: 2/27... Step: 1930... Loss: 1.4622... Val Loss: 1.5424\n",
      "Epoch: 2/27... Step: 1940... Loss: 1.4401... Val Loss: 1.5395\n",
      "Epoch: 2/27... Step: 1950... Loss: 1.4333... Val Loss: 1.5383\n",
      "Epoch: 2/27... Step: 1960... Loss: 1.4235... Val Loss: 1.5442\n",
      "Epoch: 2/27... Step: 1970... Loss: 1.4233... Val Loss: 1.5398\n",
      "Epoch: 2/27... Step: 1980... Loss: 1.5103... Val Loss: 1.5345\n",
      "Epoch: 2/27... Step: 1990... Loss: 1.4776... Val Loss: 1.5360\n",
      "Epoch: 2/27... Step: 2000... Loss: 1.5340... Val Loss: 1.5332\n",
      "Epoch: 2/27... Step: 2010... Loss: 1.4621... Val Loss: 1.5346\n",
      "Epoch: 2/27... Step: 2020... Loss: 1.5537... Val Loss: 1.5281\n",
      "Epoch: 2/27... Step: 2030... Loss: 1.4940... Val Loss: 1.5307\n",
      "Epoch: 2/27... Step: 2040... Loss: 1.5382... Val Loss: 1.5267\n",
      "Epoch: 2/27... Step: 2050... Loss: 1.4623... Val Loss: 1.5311\n",
      "Epoch: 2/27... Step: 2060... Loss: 1.5095... Val Loss: 1.5267\n",
      "Epoch: 2/27... Step: 2070... Loss: 1.5364... Val Loss: 1.5452\n",
      "Epoch: 2/27... Step: 2080... Loss: 1.5002... Val Loss: 1.5295\n",
      "Epoch: 2/27... Step: 2090... Loss: 1.5025... Val Loss: 1.5282\n",
      "Epoch: 2/27... Step: 2100... Loss: 1.5117... Val Loss: 1.5247\n",
      "Epoch: 2/27... Step: 2110... Loss: 1.4720... Val Loss: 1.5351\n",
      "Epoch: 2/27... Step: 2120... Loss: 1.5297... Val Loss: 1.5215\n",
      "Epoch: 2/27... Step: 2130... Loss: 1.4569... Val Loss: 1.5226\n",
      "Epoch: 2/27... Step: 2140... Loss: 1.5066... Val Loss: 1.5251\n",
      "Epoch: 2/27... Step: 2150... Loss: 1.4793... Val Loss: 1.5160\n",
      "Epoch: 2/27... Step: 2160... Loss: 1.4936... Val Loss: 1.5327\n",
      "Epoch: 2/27... Step: 2170... Loss: 1.4703... Val Loss: 1.5191\n",
      "Epoch: 2/27... Step: 2180... Loss: 1.5067... Val Loss: 1.5206\n",
      "Epoch: 2/27... Step: 2190... Loss: 1.4868... Val Loss: 1.5150\n",
      "Epoch: 2/27... Step: 2200... Loss: 1.4857... Val Loss: 1.5099\n",
      "Epoch: 2/27... Step: 2210... Loss: 1.4350... Val Loss: 1.5116\n",
      "Epoch: 2/27... Step: 2220... Loss: 1.4326... Val Loss: 1.5087\n",
      "Epoch: 2/27... Step: 2230... Loss: 1.4546... Val Loss: 1.5075\n",
      "Epoch: 2/27... Step: 2240... Loss: 1.5870... Val Loss: 1.5060\n",
      "Epoch: 2/27... Step: 2250... Loss: 1.4421... Val Loss: 1.5046\n",
      "Epoch: 2/27... Step: 2260... Loss: 1.4275... Val Loss: 1.5074\n",
      "Epoch: 2/27... Step: 2270... Loss: 1.4330... Val Loss: 1.5114\n",
      "Epoch: 2/27... Step: 2280... Loss: 1.5252... Val Loss: 1.5059\n",
      "Epoch: 2/27... Step: 2290... Loss: 1.4101... Val Loss: 1.5088\n",
      "Epoch: 2/27... Step: 2300... Loss: 1.4731... Val Loss: 1.5180\n",
      "Epoch: 2/27... Step: 2310... Loss: 1.4061... Val Loss: 1.5053\n",
      "Epoch: 2/27... Step: 2320... Loss: 1.4233... Val Loss: 1.4936\n",
      "Epoch: 2/27... Step: 2330... Loss: 1.4670... Val Loss: 1.4987\n",
      "Epoch: 2/27... Step: 2340... Loss: 1.4938... Val Loss: 1.4963\n",
      "Epoch: 2/27... Step: 2350... Loss: 1.4732... Val Loss: 1.4935\n",
      "Epoch: 2/27... Step: 2360... Loss: 1.4640... Val Loss: 1.4940\n",
      "Epoch: 2/27... Step: 2370... Loss: 1.4674... Val Loss: 1.4987\n",
      "Epoch: 2/27... Step: 2380... Loss: 1.5163... Val Loss: 1.4946\n",
      "Epoch: 2/27... Step: 2390... Loss: 1.4941... Val Loss: 1.4947\n",
      "Epoch: 2/27... Step: 2400... Loss: 1.4382... Val Loss: 1.4915\n",
      "Epoch: 2/27... Step: 2410... Loss: 1.4070... Val Loss: 1.4929\n",
      "Epoch: 2/27... Step: 2420... Loss: 1.4299... Val Loss: 1.4915\n",
      "Epoch: 2/27... Step: 2430... Loss: 1.4887... Val Loss: 1.4934\n",
      "Epoch: 2/27... Step: 2440... Loss: 1.4568... Val Loss: 1.4873\n",
      "Epoch: 2/27... Step: 2450... Loss: 1.3921... Val Loss: 1.4893\n",
      "Epoch: 2/27... Step: 2460... Loss: 1.4712... Val Loss: 1.4864\n",
      "Epoch: 2/27... Step: 2470... Loss: 1.4270... Val Loss: 1.4920\n",
      "Epoch: 2/27... Step: 2480... Loss: 1.4857... Val Loss: 1.4881\n",
      "Epoch: 2/27... Step: 2490... Loss: 1.3938... Val Loss: 1.4913\n",
      "Epoch: 2/27... Step: 2500... Loss: 1.3453... Val Loss: 1.4828\n",
      "Epoch: 2/27... Step: 2510... Loss: 1.4611... Val Loss: 1.4921\n",
      "Epoch: 2/27... Step: 2520... Loss: 1.4450... Val Loss: 1.4828\n",
      "Epoch: 2/27... Step: 2530... Loss: 1.4047... Val Loss: 1.4812\n",
      "Epoch: 2/27... Step: 2540... Loss: 1.4153... Val Loss: 1.4797\n",
      "Epoch: 2/27... Step: 2550... Loss: 1.3902... Val Loss: 1.4765\n",
      "Epoch: 2/27... Step: 2560... Loss: 1.4292... Val Loss: 1.4777\n",
      "Epoch: 2/27... Step: 2570... Loss: 1.4197... Val Loss: 1.4759\n",
      "Epoch: 2/27... Step: 2580... Loss: 1.4535... Val Loss: 1.4841\n",
      "Epoch: 2/27... Step: 2590... Loss: 1.4339... Val Loss: 1.4828\n",
      "Epoch: 2/27... Step: 2600... Loss: 1.3575... Val Loss: 1.4780\n",
      "Epoch: 2/27... Step: 2610... Loss: 1.3676... Val Loss: 1.4810\n",
      "Epoch: 2/27... Step: 2620... Loss: 1.4647... Val Loss: 1.4746\n",
      "Epoch: 2/27... Step: 2630... Loss: 1.4248... Val Loss: 1.4762\n",
      "Epoch: 2/27... Step: 2640... Loss: 1.4331... Val Loss: 1.4783\n",
      "Epoch: 2/27... Step: 2650... Loss: 1.4517... Val Loss: 1.4732\n",
      "Epoch: 2/27... Step: 2660... Loss: 1.4449... Val Loss: 1.4720\n",
      "Epoch: 2/27... Step: 2670... Loss: 1.5178... Val Loss: 1.4789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/27... Step: 2680... Loss: 1.3975... Val Loss: 1.4717\n",
      "Epoch: 2/27... Step: 2690... Loss: 1.3562... Val Loss: 1.4736\n",
      "Epoch: 2/27... Step: 2700... Loss: 1.4403... Val Loss: 1.4745\n",
      "Epoch: 2/27... Step: 2710... Loss: 1.4769... Val Loss: 1.4704\n",
      "Epoch: 2/27... Step: 2720... Loss: 1.2886... Val Loss: 1.4718\n",
      "Epoch: 2/27... Step: 2730... Loss: 1.5179... Val Loss: 1.4657\n",
      "Epoch: 2/27... Step: 2740... Loss: 1.4060... Val Loss: 1.4675\n",
      "Epoch: 2/27... Step: 2750... Loss: 1.3578... Val Loss: 1.4811\n",
      "Epoch: 2/27... Step: 2760... Loss: 1.3188... Val Loss: 1.4699\n",
      "Epoch: 2/27... Step: 2770... Loss: 1.3725... Val Loss: 1.4720\n",
      "Epoch: 2/27... Step: 2780... Loss: 1.3863... Val Loss: 1.4708\n",
      "Epoch: 2/27... Step: 2790... Loss: 1.9398... Val Loss: 1.4647\n",
      "Epoch: 3/27... Step: 2800... Loss: 1.4492... Val Loss: 1.4854\n",
      "Epoch: 3/27... Step: 2810... Loss: 1.4151... Val Loss: 1.4635\n",
      "Epoch: 3/27... Step: 2820... Loss: 1.4288... Val Loss: 1.4685\n",
      "Epoch: 3/27... Step: 2830... Loss: 1.3938... Val Loss: 1.4616\n",
      "Epoch: 3/27... Step: 2840... Loss: 1.4373... Val Loss: 1.4590\n",
      "Epoch: 3/27... Step: 2850... Loss: 1.4272... Val Loss: 1.4582\n",
      "Epoch: 3/27... Step: 2860... Loss: 1.3512... Val Loss: 1.4659\n",
      "Epoch: 3/27... Step: 2870... Loss: 1.3623... Val Loss: 1.4622\n",
      "Epoch: 3/27... Step: 2880... Loss: 1.3794... Val Loss: 1.4640\n",
      "Epoch: 3/27... Step: 2890... Loss: 1.4211... Val Loss: 1.4621\n",
      "Epoch: 3/27... Step: 2900... Loss: 1.4400... Val Loss: 1.4606\n",
      "Epoch: 3/27... Step: 2910... Loss: 1.4025... Val Loss: 1.4583\n",
      "Epoch: 3/27... Step: 2920... Loss: 1.4672... Val Loss: 1.4584\n",
      "Epoch: 3/27... Step: 2930... Loss: 1.3787... Val Loss: 1.4609\n",
      "Epoch: 3/27... Step: 2940... Loss: 1.3858... Val Loss: 1.4605\n",
      "Epoch: 3/27... Step: 2950... Loss: 1.3299... Val Loss: 1.4570\n",
      "Epoch: 3/27... Step: 2960... Loss: 1.4095... Val Loss: 1.4548\n",
      "Epoch: 3/27... Step: 2970... Loss: 1.4561... Val Loss: 1.4545\n",
      "Epoch: 3/27... Step: 2980... Loss: 1.3785... Val Loss: 1.4533\n",
      "Epoch: 3/27... Step: 2990... Loss: 1.3744... Val Loss: 1.4533\n",
      "Epoch: 3/27... Step: 3000... Loss: 1.3493... Val Loss: 1.4521\n",
      "Epoch: 3/27... Step: 3010... Loss: 1.4758... Val Loss: 1.4539\n",
      "Epoch: 3/27... Step: 3020... Loss: 1.3753... Val Loss: 1.4524\n",
      "Epoch: 3/27... Step: 3030... Loss: 1.4197... Val Loss: 1.4591\n",
      "Epoch: 3/27... Step: 3040... Loss: 1.4020... Val Loss: 1.4506\n",
      "Epoch: 3/27... Step: 3050... Loss: 1.3517... Val Loss: 1.4508\n",
      "Epoch: 3/27... Step: 3060... Loss: 1.2798... Val Loss: 1.4491\n",
      "Epoch: 3/27... Step: 3070... Loss: 1.4134... Val Loss: 1.4467\n",
      "Epoch: 3/27... Step: 3080... Loss: 1.4112... Val Loss: 1.4489\n",
      "Epoch: 3/27... Step: 3090... Loss: 1.4602... Val Loss: 1.4501\n",
      "Epoch: 3/27... Step: 3100... Loss: 1.3941... Val Loss: 1.4507\n",
      "Epoch: 3/27... Step: 3110... Loss: 1.3448... Val Loss: 1.4498\n",
      "Epoch: 3/27... Step: 3120... Loss: 1.4302... Val Loss: 1.4451\n",
      "Epoch: 3/27... Step: 3130... Loss: 1.4033... Val Loss: 1.4441\n",
      "Epoch: 3/27... Step: 3140... Loss: 1.3973... Val Loss: 1.4460\n",
      "Epoch: 3/27... Step: 3150... Loss: 1.3808... Val Loss: 1.4424\n",
      "Epoch: 3/27... Step: 3160... Loss: 1.4732... Val Loss: 1.4482\n",
      "Epoch: 3/27... Step: 3170... Loss: 1.3883... Val Loss: 1.4461\n",
      "Epoch: 3/27... Step: 3180... Loss: 1.3905... Val Loss: 1.4412\n",
      "Epoch: 3/27... Step: 3190... Loss: 1.3500... Val Loss: 1.4468\n",
      "Epoch: 3/27... Step: 3200... Loss: 1.4126... Val Loss: 1.4462\n",
      "Epoch: 3/27... Step: 3210... Loss: 1.4078... Val Loss: 1.4368\n",
      "Epoch: 3/27... Step: 3220... Loss: 1.3828... Val Loss: 1.4407\n",
      "Epoch: 3/27... Step: 3230... Loss: 1.4307... Val Loss: 1.4408\n",
      "Epoch: 3/27... Step: 3240... Loss: 1.4082... Val Loss: 1.4361\n",
      "Epoch: 3/27... Step: 3250... Loss: 1.3899... Val Loss: 1.4371\n",
      "Epoch: 3/27... Step: 3260... Loss: 1.4231... Val Loss: 1.4376\n",
      "Epoch: 3/27... Step: 3270... Loss: 1.3372... Val Loss: 1.4341\n",
      "Epoch: 3/27... Step: 3280... Loss: 1.3175... Val Loss: 1.4336\n",
      "Epoch: 3/27... Step: 3290... Loss: 1.4157... Val Loss: 1.4334\n",
      "Epoch: 3/27... Step: 3300... Loss: 1.3648... Val Loss: 1.4337\n",
      "Epoch: 3/27... Step: 3310... Loss: 1.4330... Val Loss: 1.4338\n",
      "Epoch: 3/27... Step: 3320... Loss: 1.3210... Val Loss: 1.4315\n",
      "Epoch: 3/27... Step: 3330... Loss: 1.3219... Val Loss: 1.4388\n",
      "Epoch: 3/27... Step: 3340... Loss: 1.3713... Val Loss: 1.4408\n",
      "Epoch: 3/27... Step: 3350... Loss: 1.3994... Val Loss: 1.4344\n",
      "Epoch: 3/27... Step: 3360... Loss: 1.4859... Val Loss: 1.4364\n",
      "Epoch: 3/27... Step: 3370... Loss: 1.3791... Val Loss: 1.4366\n",
      "Epoch: 3/27... Step: 3380... Loss: 1.3616... Val Loss: 1.4318\n",
      "Epoch: 3/27... Step: 3390... Loss: 1.3817... Val Loss: 1.4336\n",
      "Epoch: 3/27... Step: 3400... Loss: 1.3859... Val Loss: 1.4325\n",
      "Epoch: 3/27... Step: 3410... Loss: 1.3761... Val Loss: 1.4367\n",
      "Epoch: 3/27... Step: 3420... Loss: 1.4734... Val Loss: 1.4313\n",
      "Epoch: 3/27... Step: 3430... Loss: 1.3412... Val Loss: 1.4343\n",
      "Epoch: 3/27... Step: 3440... Loss: 1.3316... Val Loss: 1.4323\n",
      "Epoch: 3/27... Step: 3450... Loss: 1.3682... Val Loss: 1.4309\n",
      "Epoch: 3/27... Step: 3460... Loss: 1.3495... Val Loss: 1.4372\n",
      "Epoch: 3/27... Step: 3470... Loss: 1.3961... Val Loss: 1.4274\n",
      "Epoch: 3/27... Step: 3480... Loss: 1.4194... Val Loss: 1.4297\n",
      "Epoch: 3/27... Step: 3490... Loss: 1.4560... Val Loss: 1.4330\n",
      "Epoch: 3/27... Step: 3500... Loss: 1.3779... Val Loss: 1.4270\n",
      "Epoch: 3/27... Step: 3510... Loss: 1.4192... Val Loss: 1.4315\n",
      "Epoch: 3/27... Step: 3520... Loss: 1.3359... Val Loss: 1.4286\n",
      "Epoch: 3/27... Step: 3530... Loss: 1.3462... Val Loss: 1.4252\n",
      "Epoch: 3/27... Step: 3540... Loss: 1.4099... Val Loss: 1.4293\n",
      "Epoch: 3/27... Step: 3550... Loss: 1.3206... Val Loss: 1.4270\n",
      "Epoch: 3/27... Step: 3560... Loss: 1.3090... Val Loss: 1.4289\n",
      "Epoch: 3/27... Step: 3570... Loss: 1.4673... Val Loss: 1.4330\n",
      "Epoch: 3/27... Step: 3580... Loss: 1.3797... Val Loss: 1.4268\n",
      "Epoch: 3/27... Step: 3590... Loss: 1.4010... Val Loss: 1.4219\n",
      "Epoch: 3/27... Step: 3600... Loss: 1.3778... Val Loss: 1.4218\n",
      "Epoch: 3/27... Step: 3610... Loss: 1.4186... Val Loss: 1.4265\n",
      "Epoch: 3/27... Step: 3620... Loss: 1.4024... Val Loss: 1.4206\n",
      "Epoch: 3/27... Step: 3630... Loss: 1.3086... Val Loss: 1.4186\n",
      "Epoch: 3/27... Step: 3640... Loss: 1.3842... Val Loss: 1.4170\n",
      "Epoch: 3/27... Step: 3650... Loss: 1.3278... Val Loss: 1.4191\n",
      "Epoch: 3/27... Step: 3660... Loss: 1.3093... Val Loss: 1.4204\n",
      "Epoch: 3/27... Step: 3670... Loss: 1.4224... Val Loss: 1.4185\n",
      "Epoch: 3/27... Step: 3680... Loss: 1.3929... Val Loss: 1.4200\n",
      "Epoch: 3/27... Step: 3690... Loss: 1.3425... Val Loss: 1.4157\n",
      "Epoch: 3/27... Step: 3700... Loss: 1.3848... Val Loss: 1.4182\n",
      "Epoch: 3/27... Step: 3710... Loss: 1.4384... Val Loss: 1.4148\n",
      "Epoch: 3/27... Step: 3720... Loss: 1.3189... Val Loss: 1.4130\n",
      "Epoch: 3/27... Step: 3730... Loss: 1.3693... Val Loss: 1.4155\n",
      "Epoch: 3/27... Step: 3740... Loss: 1.3358... Val Loss: 1.4120\n",
      "Epoch: 3/27... Step: 3750... Loss: 1.4562... Val Loss: 1.4158\n",
      "Epoch: 3/27... Step: 3760... Loss: 1.3040... Val Loss: 1.4145\n",
      "Epoch: 3/27... Step: 3770... Loss: 1.3402... Val Loss: 1.4236\n",
      "Epoch: 3/27... Step: 3780... Loss: 1.4167... Val Loss: 1.4189\n",
      "Epoch: 3/27... Step: 3790... Loss: 1.3575... Val Loss: 1.4153\n",
      "Epoch: 3/27... Step: 3800... Loss: 1.3647... Val Loss: 1.4128\n",
      "Epoch: 3/27... Step: 3810... Loss: 1.3781... Val Loss: 1.4145\n",
      "Epoch: 3/27... Step: 3820... Loss: 1.3402... Val Loss: 1.4177\n",
      "Epoch: 3/27... Step: 3830... Loss: 1.2812... Val Loss: 1.4129\n",
      "Epoch: 3/27... Step: 3840... Loss: 1.3048... Val Loss: 1.4123\n",
      "Epoch: 3/27... Step: 3850... Loss: 1.3618... Val Loss: 1.4251\n",
      "Epoch: 3/27... Step: 3860... Loss: 1.3828... Val Loss: 1.4179\n",
      "Epoch: 3/27... Step: 3870... Loss: 1.3817... Val Loss: 1.4153\n",
      "Epoch: 3/27... Step: 3880... Loss: 1.2532... Val Loss: 1.4169\n",
      "Epoch: 3/27... Step: 3890... Loss: 1.3448... Val Loss: 1.4148\n",
      "Epoch: 3/27... Step: 3900... Loss: 1.3752... Val Loss: 1.4105\n",
      "Epoch: 3/27... Step: 3910... Loss: 1.3478... Val Loss: 1.4102\n",
      "Epoch: 3/27... Step: 3920... Loss: 1.2891... Val Loss: 1.4080\n",
      "Epoch: 3/27... Step: 3930... Loss: 1.2589... Val Loss: 1.4105\n",
      "Epoch: 3/27... Step: 3940... Loss: 1.3389... Val Loss: 1.4099\n",
      "Epoch: 3/27... Step: 3950... Loss: 1.3332... Val Loss: 1.4151\n",
      "Epoch: 3/27... Step: 3960... Loss: 1.4123... Val Loss: 1.4106\n",
      "Epoch: 3/27... Step: 3970... Loss: 1.4040... Val Loss: 1.4194\n",
      "Epoch: 3/27... Step: 3980... Loss: 1.3339... Val Loss: 1.4159\n",
      "Epoch: 3/27... Step: 3990... Loss: 1.3639... Val Loss: 1.4116\n",
      "Epoch: 3/27... Step: 4000... Loss: 1.2867... Val Loss: 1.4143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/27... Step: 4010... Loss: 1.3342... Val Loss: 1.4110\n",
      "Epoch: 3/27... Step: 4020... Loss: 1.3569... Val Loss: 1.4151\n",
      "Epoch: 3/27... Step: 4030... Loss: 1.3089... Val Loss: 1.4111\n",
      "Epoch: 3/27... Step: 4040... Loss: 1.2851... Val Loss: 1.4156\n",
      "Epoch: 3/27... Step: 4050... Loss: 1.3324... Val Loss: 1.4093\n",
      "Epoch: 3/27... Step: 4060... Loss: 1.3426... Val Loss: 1.4071\n",
      "Epoch: 3/27... Step: 4070... Loss: 1.3001... Val Loss: 1.4112\n",
      "Epoch: 3/27... Step: 4080... Loss: 1.3848... Val Loss: 1.4072\n",
      "Epoch: 3/27... Step: 4090... Loss: 1.3807... Val Loss: 1.4072\n",
      "Epoch: 3/27... Step: 4100... Loss: 1.4162... Val Loss: 1.4146\n",
      "Epoch: 3/27... Step: 4110... Loss: 1.3378... Val Loss: 1.4039\n",
      "Epoch: 3/27... Step: 4120... Loss: 1.3104... Val Loss: 1.4179\n",
      "Epoch: 3/27... Step: 4130... Loss: 1.3966... Val Loss: 1.4212\n",
      "Epoch: 3/27... Step: 4140... Loss: 1.3502... Val Loss: 1.4040\n",
      "Epoch: 3/27... Step: 4150... Loss: 1.2844... Val Loss: 1.4090\n",
      "Epoch: 3/27... Step: 4160... Loss: 1.3388... Val Loss: 1.4236\n",
      "Epoch: 3/27... Step: 4170... Loss: 1.4346... Val Loss: 1.4079\n",
      "Epoch: 3/27... Step: 4180... Loss: 1.3447... Val Loss: 1.4059\n",
      "Epoch: 4/27... Step: 4190... Loss: 1.3257... Val Loss: 1.4067\n",
      "Epoch: 4/27... Step: 4200... Loss: 1.3035... Val Loss: 1.4043\n",
      "Epoch: 4/27... Step: 4210... Loss: 1.3693... Val Loss: 1.4031\n",
      "Epoch: 4/27... Step: 4220... Loss: 1.3014... Val Loss: 1.4032\n",
      "Epoch: 4/27... Step: 4230... Loss: 1.3147... Val Loss: 1.3991\n",
      "Epoch: 4/27... Step: 4240... Loss: 1.3323... Val Loss: 1.3996\n",
      "Epoch: 4/27... Step: 4250... Loss: 1.3487... Val Loss: 1.3984\n",
      "Epoch: 4/27... Step: 4260... Loss: 1.2804... Val Loss: 1.4029\n",
      "Epoch: 4/27... Step: 4270... Loss: 1.3521... Val Loss: 1.4076\n",
      "Epoch: 4/27... Step: 4280... Loss: 1.3835... Val Loss: 1.3970\n",
      "Epoch: 4/27... Step: 4290... Loss: 1.3093... Val Loss: 1.3976\n",
      "Epoch: 4/27... Step: 4300... Loss: 1.3134... Val Loss: 1.3994\n",
      "Epoch: 4/27... Step: 4310... Loss: 1.3136... Val Loss: 1.3972\n",
      "Epoch: 4/27... Step: 4320... Loss: 1.2925... Val Loss: 1.4046\n",
      "Epoch: 4/27... Step: 4330... Loss: 1.3553... Val Loss: 1.4035\n",
      "Epoch: 4/27... Step: 4340... Loss: 1.3905... Val Loss: 1.3998\n",
      "Epoch: 4/27... Step: 4350... Loss: 1.3495... Val Loss: 1.3972\n",
      "Epoch: 4/27... Step: 4360... Loss: 1.3126... Val Loss: 1.4036\n",
      "Epoch: 4/27... Step: 4370... Loss: 1.3764... Val Loss: 1.3980\n",
      "Epoch: 4/27... Step: 4380... Loss: 1.3397... Val Loss: 1.3980\n",
      "Epoch: 4/27... Step: 4390... Loss: 1.3067... Val Loss: 1.3985\n",
      "Epoch: 4/27... Step: 4400... Loss: 1.3158... Val Loss: 1.3988\n",
      "Epoch: 4/27... Step: 4410... Loss: 1.3320... Val Loss: 1.3971\n",
      "Epoch: 4/27... Step: 4420... Loss: 1.2366... Val Loss: 1.4018\n",
      "Epoch: 4/27... Step: 4430... Loss: 1.3475... Val Loss: 1.3991\n",
      "Epoch: 4/27... Step: 4440... Loss: 1.3509... Val Loss: 1.3986\n",
      "Epoch: 4/27... Step: 4450... Loss: 1.2958... Val Loss: 1.3956\n",
      "Epoch: 4/27... Step: 4460... Loss: 1.3529... Val Loss: 1.3939\n",
      "Epoch: 4/27... Step: 4470... Loss: 1.3248... Val Loss: 1.3993\n",
      "Epoch: 4/27... Step: 4480... Loss: 1.3754... Val Loss: 1.3954\n",
      "Epoch: 4/27... Step: 4490... Loss: 1.2911... Val Loss: 1.3988\n",
      "Epoch: 4/27... Step: 4500... Loss: 1.3137... Val Loss: 1.3984\n",
      "Epoch: 4/27... Step: 4510... Loss: 1.2982... Val Loss: 1.3962\n",
      "Epoch: 4/27... Step: 4520... Loss: 1.3445... Val Loss: 1.3958\n",
      "Epoch: 4/27... Step: 4530... Loss: 1.4157... Val Loss: 1.3922\n",
      "Epoch: 4/27... Step: 4540... Loss: 1.3444... Val Loss: 1.3923\n",
      "Epoch: 4/27... Step: 4550... Loss: 1.3524... Val Loss: 1.3933\n",
      "Epoch: 4/27... Step: 4560... Loss: 1.4030... Val Loss: 1.3921\n",
      "Epoch: 4/27... Step: 4570... Loss: 1.3447... Val Loss: 1.3912\n",
      "Epoch: 4/27... Step: 4580... Loss: 1.3344... Val Loss: 1.3953\n",
      "Epoch: 4/27... Step: 4590... Loss: 1.3551... Val Loss: 1.3981\n",
      "Epoch: 4/27... Step: 4600... Loss: 1.2930... Val Loss: 1.3970\n",
      "Epoch: 4/27... Step: 4610... Loss: 1.2962... Val Loss: 1.3944\n",
      "Epoch: 4/27... Step: 4620... Loss: 1.3938... Val Loss: 1.3973\n",
      "Epoch: 4/27... Step: 4630... Loss: 1.3084... Val Loss: 1.3971\n",
      "Epoch: 4/27... Step: 4640... Loss: 1.3167... Val Loss: 1.3979\n",
      "Epoch: 4/27... Step: 4650... Loss: 1.3424... Val Loss: 1.3878\n",
      "Epoch: 4/27... Step: 4660... Loss: 1.2539... Val Loss: 1.3982\n",
      "Epoch: 4/27... Step: 4670... Loss: 1.2795... Val Loss: 1.3861\n",
      "Epoch: 4/27... Step: 4680... Loss: 1.3347... Val Loss: 1.3881\n",
      "Epoch: 4/27... Step: 4690... Loss: 1.3150... Val Loss: 1.3860\n",
      "Epoch: 4/27... Step: 4700... Loss: 1.3357... Val Loss: 1.3871\n",
      "Epoch: 4/27... Step: 4710... Loss: 1.3413... Val Loss: 1.3860\n",
      "Epoch: 4/27... Step: 4720... Loss: 1.2851... Val Loss: 1.3879\n",
      "Epoch: 4/27... Step: 4730... Loss: 1.2248... Val Loss: 1.3947\n",
      "Epoch: 4/27... Step: 4740... Loss: 1.2603... Val Loss: 1.3888\n",
      "Epoch: 4/27... Step: 4750... Loss: 1.2413... Val Loss: 1.3912\n",
      "Epoch: 4/27... Step: 4760... Loss: 1.2454... Val Loss: 1.3891\n",
      "Epoch: 4/27... Step: 4770... Loss: 1.2858... Val Loss: 1.3896\n",
      "Epoch: 4/27... Step: 4780... Loss: 1.2799... Val Loss: 1.3899\n",
      "Epoch: 4/27... Step: 4790... Loss: 1.3098... Val Loss: 1.3909\n",
      "Epoch: 4/27... Step: 4800... Loss: 1.2863... Val Loss: 1.3907\n",
      "Epoch: 4/27... Step: 4810... Loss: 1.3588... Val Loss: 1.3916\n",
      "Epoch: 4/27... Step: 4820... Loss: 1.2981... Val Loss: 1.3877\n",
      "Epoch: 4/27... Step: 4830... Loss: 1.3934... Val Loss: 1.3852\n",
      "Epoch: 4/27... Step: 4840... Loss: 1.3066... Val Loss: 1.3898\n",
      "Epoch: 4/27... Step: 4850... Loss: 1.3021... Val Loss: 1.3883\n",
      "Epoch: 4/27... Step: 4860... Loss: 1.3419... Val Loss: 1.3931\n",
      "Epoch: 4/27... Step: 4870... Loss: 1.3266... Val Loss: 1.3883\n",
      "Epoch: 4/27... Step: 4880... Loss: 1.3008... Val Loss: 1.3882\n",
      "Epoch: 4/27... Step: 4890... Loss: 1.3039... Val Loss: 1.3878\n",
      "Epoch: 4/27... Step: 4900... Loss: 1.2861... Val Loss: 1.3852\n",
      "Epoch: 4/27... Step: 4910... Loss: 1.3124... Val Loss: 1.3853\n",
      "Epoch: 4/27... Step: 4920... Loss: 1.2751... Val Loss: 1.3862\n",
      "Epoch: 4/27... Step: 4930... Loss: 1.3578... Val Loss: 1.3861\n",
      "Epoch: 4/27... Step: 4940... Loss: 1.2501... Val Loss: 1.3835\n",
      "Epoch: 4/27... Step: 4950... Loss: 1.3317... Val Loss: 1.3866\n",
      "Epoch: 4/27... Step: 4960... Loss: 1.3301... Val Loss: 1.3887\n",
      "Epoch: 4/27... Step: 4970... Loss: 1.3335... Val Loss: 1.3877\n",
      "Epoch: 4/27... Step: 4980... Loss: 1.2900... Val Loss: 1.3823\n",
      "Epoch: 4/27... Step: 4990... Loss: 1.3057... Val Loss: 1.3813\n",
      "Epoch: 4/27... Step: 5000... Loss: 1.2611... Val Loss: 1.3839\n",
      "Epoch: 4/27... Step: 5010... Loss: 1.3094... Val Loss: 1.3856\n",
      "Epoch: 4/27... Step: 5020... Loss: 1.2865... Val Loss: 1.3808\n",
      "Epoch: 4/27... Step: 5030... Loss: 1.3967... Val Loss: 1.3796\n",
      "Epoch: 4/27... Step: 5040... Loss: 1.2885... Val Loss: 1.3839\n",
      "Epoch: 4/27... Step: 5050... Loss: 1.2983... Val Loss: 1.3873\n",
      "Epoch: 4/27... Step: 5060... Loss: 1.2814... Val Loss: 1.3826\n",
      "Epoch: 4/27... Step: 5070... Loss: 1.3602... Val Loss: 1.3836\n",
      "Epoch: 4/27... Step: 5080... Loss: 1.2728... Val Loss: 1.3825\n",
      "Epoch: 4/27... Step: 5090... Loss: 1.2936... Val Loss: 1.3794\n",
      "Epoch: 4/27... Step: 5100... Loss: 1.2611... Val Loss: 1.3759\n",
      "Epoch: 4/27... Step: 5110... Loss: 1.2880... Val Loss: 1.3797\n",
      "Epoch: 4/27... Step: 5120... Loss: 1.3197... Val Loss: 1.3782\n",
      "Epoch: 4/27... Step: 5130... Loss: 1.3495... Val Loss: 1.3782\n",
      "Epoch: 4/27... Step: 5140... Loss: 1.2967... Val Loss: 1.3806\n",
      "Epoch: 4/27... Step: 5150... Loss: 1.3439... Val Loss: 1.3781\n",
      "Epoch: 4/27... Step: 5160... Loss: 1.3093... Val Loss: 1.3774\n",
      "Epoch: 4/27... Step: 5170... Loss: 1.3582... Val Loss: 1.3824\n",
      "Epoch: 4/27... Step: 5180... Loss: 1.3367... Val Loss: 1.3807\n",
      "Epoch: 4/27... Step: 5190... Loss: 1.2760... Val Loss: 1.3791\n",
      "Epoch: 4/27... Step: 5200... Loss: 1.2706... Val Loss: 1.3812\n",
      "Epoch: 4/27... Step: 5210... Loss: 1.2932... Val Loss: 1.3763\n",
      "Epoch: 4/27... Step: 5220... Loss: 1.3456... Val Loss: 1.3785\n",
      "Epoch: 4/27... Step: 5230... Loss: 1.3371... Val Loss: 1.3779\n",
      "Epoch: 4/27... Step: 5240... Loss: 1.2863... Val Loss: 1.3783\n",
      "Epoch: 4/27... Step: 5250... Loss: 1.3238... Val Loss: 1.3817\n",
      "Epoch: 4/27... Step: 5260... Loss: 1.2909... Val Loss: 1.3790\n",
      "Epoch: 4/27... Step: 5270... Loss: 1.3362... Val Loss: 1.3777\n",
      "Epoch: 4/27... Step: 5280... Loss: 1.2533... Val Loss: 1.3896\n",
      "Epoch: 4/27... Step: 5290... Loss: 1.2346... Val Loss: 1.3770\n",
      "Epoch: 4/27... Step: 5300... Loss: 1.3212... Val Loss: 1.3805\n",
      "Epoch: 4/27... Step: 5310... Loss: 1.2823... Val Loss: 1.3820\n",
      "Epoch: 4/27... Step: 5320... Loss: 1.2696... Val Loss: 1.3832\n",
      "Epoch: 4/27... Step: 5330... Loss: 1.2598... Val Loss: 1.3767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/27... Step: 5340... Loss: 1.2837... Val Loss: 1.3774\n",
      "Epoch: 4/27... Step: 5350... Loss: 1.2760... Val Loss: 1.3812\n",
      "Epoch: 4/27... Step: 5360... Loss: 1.2719... Val Loss: 1.3790\n",
      "Epoch: 4/27... Step: 5370... Loss: 1.2939... Val Loss: 1.3851\n",
      "Epoch: 4/27... Step: 5380... Loss: 1.2993... Val Loss: 1.3768\n",
      "Epoch: 4/27... Step: 5390... Loss: 1.2159... Val Loss: 1.3775\n",
      "Epoch: 4/27... Step: 5400... Loss: 1.2190... Val Loss: 1.3813\n",
      "Epoch: 4/27... Step: 5410... Loss: 1.3668... Val Loss: 1.3774\n",
      "Epoch: 4/27... Step: 5420... Loss: 1.3092... Val Loss: 1.3792\n",
      "Epoch: 4/27... Step: 5430... Loss: 1.3086... Val Loss: 1.3792\n",
      "Epoch: 4/27... Step: 5440... Loss: 1.3275... Val Loss: 1.3774\n",
      "Epoch: 4/27... Step: 5450... Loss: 1.2832... Val Loss: 1.3731\n",
      "Epoch: 4/27... Step: 5460... Loss: 1.3485... Val Loss: 1.3823\n",
      "Epoch: 4/27... Step: 5470... Loss: 1.2736... Val Loss: 1.3728\n",
      "Epoch: 4/27... Step: 5480... Loss: 1.2391... Val Loss: 1.3720\n",
      "Epoch: 4/27... Step: 5490... Loss: 1.2979... Val Loss: 1.3784\n",
      "Epoch: 4/27... Step: 5500... Loss: 1.3632... Val Loss: 1.3754\n",
      "Epoch: 4/27... Step: 5510... Loss: 1.1840... Val Loss: 1.3704\n",
      "Epoch: 4/27... Step: 5520... Loss: 1.3578... Val Loss: 1.3748\n",
      "Epoch: 4/27... Step: 5530... Loss: 1.2589... Val Loss: 1.3800\n",
      "Epoch: 4/27... Step: 5540... Loss: 1.2414... Val Loss: 1.3711\n",
      "Epoch: 4/27... Step: 5550... Loss: 1.1986... Val Loss: 1.3695\n",
      "Epoch: 4/27... Step: 5560... Loss: 1.2295... Val Loss: 1.3742\n",
      "Epoch: 4/27... Step: 5570... Loss: 1.2601... Val Loss: 1.3710\n",
      "Epoch: 4/27... Step: 5580... Loss: 1.8807... Val Loss: 1.3706\n",
      "Epoch: 5/27... Step: 5590... Loss: 1.2874... Val Loss: 1.3787\n",
      "Epoch: 5/27... Step: 5600... Loss: 1.2904... Val Loss: 1.3695\n",
      "Epoch: 5/27... Step: 5610... Loss: 1.3020... Val Loss: 1.3736\n",
      "Epoch: 5/27... Step: 5620... Loss: 1.2914... Val Loss: 1.3717\n",
      "Epoch: 5/27... Step: 5630... Loss: 1.2946... Val Loss: 1.3740\n",
      "Epoch: 5/27... Step: 5640... Loss: 1.3019... Val Loss: 1.3686\n",
      "Epoch: 5/27... Step: 5650... Loss: 1.2353... Val Loss: 1.3680\n",
      "Epoch: 5/27... Step: 5660... Loss: 1.2347... Val Loss: 1.3716\n",
      "Epoch: 5/27... Step: 5670... Loss: 1.2546... Val Loss: 1.3717\n",
      "Epoch: 5/27... Step: 5680... Loss: 1.3190... Val Loss: 1.3688\n",
      "Epoch: 5/27... Step: 5690... Loss: 1.3139... Val Loss: 1.3725\n",
      "Epoch: 5/27... Step: 5700... Loss: 1.2991... Val Loss: 1.3682\n",
      "Epoch: 5/27... Step: 5710... Loss: 1.3480... Val Loss: 1.3697\n",
      "Epoch: 5/27... Step: 5720... Loss: 1.2695... Val Loss: 1.3751\n",
      "Epoch: 5/27... Step: 5730... Loss: 1.2700... Val Loss: 1.3681\n",
      "Epoch: 5/27... Step: 5740... Loss: 1.2053... Val Loss: 1.3665\n",
      "Epoch: 5/27... Step: 5750... Loss: 1.3009... Val Loss: 1.3686\n",
      "Epoch: 5/27... Step: 5760... Loss: 1.3148... Val Loss: 1.3697\n",
      "Epoch: 5/27... Step: 5770... Loss: 1.2629... Val Loss: 1.3693\n",
      "Epoch: 5/27... Step: 5780... Loss: 1.2556... Val Loss: 1.3673\n",
      "Epoch: 5/27... Step: 5790... Loss: 1.2288... Val Loss: 1.3671\n",
      "Epoch: 5/27... Step: 5800... Loss: 1.3369... Val Loss: 1.3718\n",
      "Epoch: 5/27... Step: 5810... Loss: 1.2831... Val Loss: 1.3704\n",
      "Epoch: 5/27... Step: 5820... Loss: 1.3121... Val Loss: 1.3729\n",
      "Epoch: 5/27... Step: 5830... Loss: 1.2643... Val Loss: 1.3719\n",
      "Epoch: 5/27... Step: 5840... Loss: 1.2212... Val Loss: 1.3719\n",
      "Epoch: 5/27... Step: 5850... Loss: 1.1640... Val Loss: 1.3670\n",
      "Epoch: 5/27... Step: 5860... Loss: 1.2739... Val Loss: 1.3706\n",
      "Epoch: 5/27... Step: 5870... Loss: 1.2593... Val Loss: 1.3700\n",
      "Epoch: 5/27... Step: 5880... Loss: 1.3759... Val Loss: 1.3710\n",
      "Epoch: 5/27... Step: 5890... Loss: 1.2499... Val Loss: 1.3697\n",
      "Epoch: 5/27... Step: 5900... Loss: 1.2408... Val Loss: 1.3694\n",
      "Epoch: 5/27... Step: 5910... Loss: 1.3012... Val Loss: 1.3644\n",
      "Epoch: 5/27... Step: 5920... Loss: 1.2954... Val Loss: 1.3655\n",
      "Epoch: 5/27... Step: 5930... Loss: 1.3021... Val Loss: 1.3622\n",
      "Epoch: 5/27... Step: 5940... Loss: 1.3134... Val Loss: 1.3643\n",
      "Epoch: 5/27... Step: 5950... Loss: 1.3657... Val Loss: 1.3694\n",
      "Epoch: 5/27... Step: 5960... Loss: 1.2882... Val Loss: 1.3659\n",
      "Epoch: 5/27... Step: 5970... Loss: 1.2993... Val Loss: 1.3724\n",
      "Epoch: 5/27... Step: 5980... Loss: 1.2612... Val Loss: 1.3679\n",
      "Epoch: 5/27... Step: 5990... Loss: 1.3157... Val Loss: 1.3827\n",
      "Epoch: 5/27... Step: 6000... Loss: 1.3171... Val Loss: 1.3647\n",
      "Epoch: 5/27... Step: 6010... Loss: 1.2631... Val Loss: 1.3649\n",
      "Epoch: 5/27... Step: 6020... Loss: 1.3357... Val Loss: 1.3761\n",
      "Epoch: 5/27... Step: 6030... Loss: 1.2905... Val Loss: 1.3659\n",
      "Epoch: 5/27... Step: 6040... Loss: 1.2942... Val Loss: 1.3737\n",
      "Epoch: 5/27... Step: 6050... Loss: 1.3226... Val Loss: 1.3657\n",
      "Epoch: 5/27... Step: 6060... Loss: 1.2341... Val Loss: 1.3681\n",
      "Epoch: 5/27... Step: 6070... Loss: 1.2121... Val Loss: 1.3630\n",
      "Epoch: 5/27... Step: 6080... Loss: 1.2894... Val Loss: 1.3593\n",
      "Epoch: 5/27... Step: 6090... Loss: 1.2518... Val Loss: 1.3656\n",
      "Epoch: 5/27... Step: 6100... Loss: 1.3232... Val Loss: 1.3624\n",
      "Epoch: 5/27... Step: 6110... Loss: 1.2148... Val Loss: 1.3677\n",
      "Epoch: 5/27... Step: 6120... Loss: 1.2477... Val Loss: 1.3673\n",
      "Epoch: 5/27... Step: 6130... Loss: 1.2716... Val Loss: 1.3687\n",
      "Epoch: 5/27... Step: 6140... Loss: 1.2807... Val Loss: 1.3630\n",
      "Epoch: 5/27... Step: 6150... Loss: 1.3896... Val Loss: 1.3643\n",
      "Epoch: 5/27... Step: 6160... Loss: 1.2601... Val Loss: 1.3636\n",
      "Epoch: 5/27... Step: 6170... Loss: 1.2647... Val Loss: 1.3620\n",
      "Epoch: 5/27... Step: 6180... Loss: 1.2818... Val Loss: 1.3659\n",
      "Epoch: 5/27... Step: 6190... Loss: 1.2773... Val Loss: 1.3634\n",
      "Epoch: 5/27... Step: 6200... Loss: 1.3061... Val Loss: 1.3616\n",
      "Epoch: 5/27... Step: 6210... Loss: 1.3729... Val Loss: 1.3635\n",
      "Epoch: 5/27... Step: 6220... Loss: 1.2589... Val Loss: 1.3620\n",
      "Epoch: 5/27... Step: 6230... Loss: 1.2480... Val Loss: 1.3643\n",
      "Epoch: 5/27... Step: 6240... Loss: 1.2510... Val Loss: 1.3619\n",
      "Epoch: 5/27... Step: 6250... Loss: 1.2474... Val Loss: 1.3641\n",
      "Epoch: 5/27... Step: 6260... Loss: 1.3246... Val Loss: 1.3650\n",
      "Epoch: 5/27... Step: 6270... Loss: 1.3163... Val Loss: 1.3636\n",
      "Epoch: 5/27... Step: 6280... Loss: 1.3417... Val Loss: 1.3654\n",
      "Epoch: 5/27... Step: 6290... Loss: 1.2860... Val Loss: 1.3619\n",
      "Epoch: 5/27... Step: 6300... Loss: 1.3160... Val Loss: 1.3596\n",
      "Epoch: 5/27... Step: 6310... Loss: 1.2434... Val Loss: 1.3616\n",
      "Epoch: 5/27... Step: 6320... Loss: 1.2563... Val Loss: 1.3630\n",
      "Epoch: 5/27... Step: 6330... Loss: 1.3166... Val Loss: 1.3616\n",
      "Epoch: 5/27... Step: 6340... Loss: 1.2576... Val Loss: 1.3589\n",
      "Epoch: 5/27... Step: 6350... Loss: 1.1911... Val Loss: 1.3661\n",
      "Epoch: 5/27... Step: 6360... Loss: 1.3670... Val Loss: 1.3647\n",
      "Epoch: 5/27... Step: 6370... Loss: 1.3133... Val Loss: 1.3647\n",
      "Epoch: 5/27... Step: 6380... Loss: 1.2957... Val Loss: 1.3598\n",
      "Epoch: 5/27... Step: 6390... Loss: 1.2498... Val Loss: 1.3576\n",
      "Epoch: 5/27... Step: 6400... Loss: 1.2859... Val Loss: 1.3643\n",
      "Epoch: 5/27... Step: 6410... Loss: 1.3066... Val Loss: 1.3658\n",
      "Epoch: 5/27... Step: 6420... Loss: 1.2194... Val Loss: 1.3600\n",
      "Epoch: 5/27... Step: 6430... Loss: 1.2795... Val Loss: 1.3577\n",
      "Epoch: 5/27... Step: 6440... Loss: 1.2213... Val Loss: 1.3606\n",
      "Epoch: 5/27... Step: 6450... Loss: 1.2062... Val Loss: 1.3649\n",
      "Epoch: 5/27... Step: 6460... Loss: 1.3421... Val Loss: 1.3604\n",
      "Epoch: 5/27... Step: 6470... Loss: 1.3143... Val Loss: 1.3594\n",
      "Epoch: 5/27... Step: 6480... Loss: 1.2568... Val Loss: 1.3599\n",
      "Epoch: 5/27... Step: 6490... Loss: 1.3083... Val Loss: 1.3593\n",
      "Epoch: 5/27... Step: 6500... Loss: 1.3658... Val Loss: 1.3581\n",
      "Epoch: 5/27... Step: 6510... Loss: 1.2585... Val Loss: 1.3564\n",
      "Epoch: 5/27... Step: 6520... Loss: 1.2746... Val Loss: 1.3565\n",
      "Epoch: 5/27... Step: 6530... Loss: 1.2580... Val Loss: 1.3545\n",
      "Epoch: 5/27... Step: 6540... Loss: 1.3359... Val Loss: 1.3558\n",
      "Epoch: 5/27... Step: 6550... Loss: 1.2103... Val Loss: 1.3564\n",
      "Epoch: 5/27... Step: 6560... Loss: 1.2294... Val Loss: 1.3590\n",
      "Epoch: 5/27... Step: 6570... Loss: 1.3027... Val Loss: 1.3582\n",
      "Epoch: 5/27... Step: 6580... Loss: 1.2810... Val Loss: 1.3600\n",
      "Epoch: 5/27... Step: 6590... Loss: 1.2633... Val Loss: 1.3550\n",
      "Epoch: 5/27... Step: 6600... Loss: 1.3076... Val Loss: 1.3538\n",
      "Epoch: 5/27... Step: 6610... Loss: 1.2692... Val Loss: 1.3521\n",
      "Epoch: 5/27... Step: 6620... Loss: 1.2294... Val Loss: 1.3557\n",
      "Epoch: 5/27... Step: 6630... Loss: 1.1999... Val Loss: 1.3506\n",
      "Epoch: 5/27... Step: 6640... Loss: 1.2469... Val Loss: 1.3566\n",
      "Epoch: 5/27... Step: 6650... Loss: 1.3140... Val Loss: 1.3572\n",
      "Epoch: 5/27... Step: 6660... Loss: 1.2832... Val Loss: 1.3561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/27... Step: 6670... Loss: 1.1656... Val Loss: 1.3561\n",
      "Epoch: 5/27... Step: 6680... Loss: 1.2976... Val Loss: 1.3617\n",
      "Epoch: 5/27... Step: 6690... Loss: 1.2696... Val Loss: 1.3549\n",
      "Epoch: 5/27... Step: 6700... Loss: 1.2609... Val Loss: 1.3581\n",
      "Epoch: 5/27... Step: 6710... Loss: 1.1956... Val Loss: 1.3548\n",
      "Epoch: 5/27... Step: 6720... Loss: 1.1773... Val Loss: 1.3550\n",
      "Epoch: 5/27... Step: 6730... Loss: 1.2680... Val Loss: 1.3532\n",
      "Epoch: 5/27... Step: 6740... Loss: 1.2540... Val Loss: 1.3557\n",
      "Epoch: 5/27... Step: 6750... Loss: 1.3057... Val Loss: 1.3525\n",
      "Epoch: 5/27... Step: 6760... Loss: 1.3061... Val Loss: 1.3524\n",
      "Epoch: 5/27... Step: 6770... Loss: 1.2346... Val Loss: 1.3539\n",
      "Epoch: 5/27... Step: 6780... Loss: 1.2777... Val Loss: 1.3518\n",
      "Epoch: 5/27... Step: 6790... Loss: 1.1873... Val Loss: 1.3523\n",
      "Epoch: 5/27... Step: 6800... Loss: 1.2666... Val Loss: 1.3589\n",
      "Epoch: 5/27... Step: 6810... Loss: 1.2705... Val Loss: 1.3541\n",
      "Epoch: 5/27... Step: 6820... Loss: 1.2446... Val Loss: 1.3559\n",
      "Epoch: 5/27... Step: 6830... Loss: 1.2216... Val Loss: 1.3547\n",
      "Epoch: 5/27... Step: 6840... Loss: 1.2267... Val Loss: 1.3572\n",
      "Epoch: 5/27... Step: 6850... Loss: 1.2500... Val Loss: 1.3551\n",
      "Epoch: 5/27... Step: 6860... Loss: 1.2516... Val Loss: 1.3525\n",
      "Epoch: 5/27... Step: 6870... Loss: 1.2815... Val Loss: 1.3521\n",
      "Epoch: 5/27... Step: 6880... Loss: 1.2899... Val Loss: 1.3518\n",
      "Epoch: 5/27... Step: 6890... Loss: 1.3214... Val Loss: 1.3514\n",
      "Epoch: 5/27... Step: 6900... Loss: 1.2328... Val Loss: 1.3476\n",
      "Epoch: 5/27... Step: 6910... Loss: 1.1939... Val Loss: 1.3504\n",
      "Epoch: 5/27... Step: 6920... Loss: 1.3117... Val Loss: 1.3490\n",
      "Epoch: 5/27... Step: 6930... Loss: 1.2896... Val Loss: 1.3532\n",
      "Epoch: 5/27... Step: 6940... Loss: 1.1973... Val Loss: 1.3465\n",
      "Epoch: 5/27... Step: 6950... Loss: 1.2827... Val Loss: 1.3501\n",
      "Epoch: 5/27... Step: 6960... Loss: 1.3243... Val Loss: 1.3507\n",
      "Epoch: 5/27... Step: 6970... Loss: 1.2829... Val Loss: 1.3567\n",
      "Epoch: 6/27... Step: 6980... Loss: 1.2730... Val Loss: 1.3487\n",
      "Epoch: 6/27... Step: 6990... Loss: 1.2249... Val Loss: 1.3617\n",
      "Epoch: 6/27... Step: 7000... Loss: 1.2793... Val Loss: 1.3501\n",
      "Epoch: 6/27... Step: 7010... Loss: 1.1858... Val Loss: 1.3547\n",
      "Epoch: 6/27... Step: 7020... Loss: 1.2415... Val Loss: 1.3500\n",
      "Epoch: 6/27... Step: 7030... Loss: 1.2704... Val Loss: 1.3612\n",
      "Epoch: 6/27... Step: 7040... Loss: 1.2959... Val Loss: 1.3525\n",
      "Epoch: 6/27... Step: 7050... Loss: 1.2343... Val Loss: 1.3524\n",
      "Epoch: 6/27... Step: 7060... Loss: 1.2515... Val Loss: 1.3605\n",
      "Epoch: 6/27... Step: 7070... Loss: 1.3069... Val Loss: 1.3530\n",
      "Epoch: 6/27... Step: 7080... Loss: 1.2437... Val Loss: 1.3516\n",
      "Epoch: 6/27... Step: 7090... Loss: 1.2290... Val Loss: 1.3556\n",
      "Epoch: 6/27... Step: 7100... Loss: 1.2299... Val Loss: 1.3509\n",
      "Epoch: 6/27... Step: 7110... Loss: 1.2221... Val Loss: 1.3542\n",
      "Epoch: 6/27... Step: 7120... Loss: 1.2643... Val Loss: 1.3546\n",
      "Epoch: 6/27... Step: 7130... Loss: 1.2963... Val Loss: 1.3529\n",
      "Epoch: 6/27... Step: 7140... Loss: 1.2769... Val Loss: 1.3501\n",
      "Epoch: 6/27... Step: 7150... Loss: 1.2549... Val Loss: 1.3497\n",
      "Epoch: 6/27... Step: 7160... Loss: 1.3085... Val Loss: 1.3531\n",
      "Epoch: 6/27... Step: 7170... Loss: 1.2508... Val Loss: 1.3522\n",
      "Epoch: 6/27... Step: 7180... Loss: 1.2374... Val Loss: 1.3513\n",
      "Epoch: 6/27... Step: 7190... Loss: 1.2523... Val Loss: 1.3531\n",
      "Epoch: 6/27... Step: 7200... Loss: 1.2273... Val Loss: 1.3515\n",
      "Epoch: 6/27... Step: 7210... Loss: 1.1648... Val Loss: 1.3535\n",
      "Epoch: 6/27... Step: 7220... Loss: 1.2786... Val Loss: 1.3577\n",
      "Epoch: 6/27... Step: 7230... Loss: 1.2525... Val Loss: 1.3539\n",
      "Epoch: 6/27... Step: 7240... Loss: 1.2439... Val Loss: 1.3514\n",
      "Epoch: 6/27... Step: 7250... Loss: 1.2603... Val Loss: 1.3509\n",
      "Epoch: 6/27... Step: 7260... Loss: 1.2248... Val Loss: 1.3548\n",
      "Epoch: 6/27... Step: 7270... Loss: 1.3176... Val Loss: 1.3521\n",
      "Epoch: 6/27... Step: 7280... Loss: 1.2417... Val Loss: 1.3538\n",
      "Epoch: 6/27... Step: 7290... Loss: 1.2538... Val Loss: 1.3508\n",
      "Epoch: 6/27... Step: 7300... Loss: 1.2018... Val Loss: 1.3528\n",
      "Epoch: 6/27... Step: 7310... Loss: 1.2677... Val Loss: 1.3546\n",
      "Epoch: 6/27... Step: 7320... Loss: 1.3321... Val Loss: 1.3499\n",
      "Epoch: 6/27... Step: 7330... Loss: 1.2494... Val Loss: 1.3492\n",
      "Epoch: 6/27... Step: 7340... Loss: 1.2716... Val Loss: 1.3471\n",
      "Epoch: 6/27... Step: 7350... Loss: 1.3078... Val Loss: 1.3471\n",
      "Epoch: 6/27... Step: 7360... Loss: 1.2610... Val Loss: 1.3465\n",
      "Epoch: 6/27... Step: 7370... Loss: 1.2424... Val Loss: 1.3544\n",
      "Epoch: 6/27... Step: 7380... Loss: 1.2836... Val Loss: 1.3496\n",
      "Epoch: 6/27... Step: 7390... Loss: 1.2269... Val Loss: 1.3552\n",
      "Epoch: 6/27... Step: 7400... Loss: 1.1949... Val Loss: 1.3451\n",
      "Epoch: 6/27... Step: 7410... Loss: 1.3008... Val Loss: 1.3488\n",
      "Epoch: 6/27... Step: 7420... Loss: 1.2119... Val Loss: 1.3501\n",
      "Epoch: 6/27... Step: 7430... Loss: 1.2346... Val Loss: 1.3480\n",
      "Epoch: 6/27... Step: 7440... Loss: 1.2779... Val Loss: 1.3484\n",
      "Epoch: 6/27... Step: 7450... Loss: 1.2241... Val Loss: 1.3508\n",
      "Epoch: 6/27... Step: 7460... Loss: 1.2143... Val Loss: 1.3478\n",
      "Epoch: 6/27... Step: 7470... Loss: 1.2806... Val Loss: 1.3468\n",
      "Epoch: 6/27... Step: 7480... Loss: 1.2275... Val Loss: 1.3418\n",
      "Epoch: 6/27... Step: 7490... Loss: 1.2776... Val Loss: 1.3466\n",
      "Epoch: 6/27... Step: 7500... Loss: 1.2578... Val Loss: 1.3443\n",
      "Epoch: 6/27... Step: 7510... Loss: 1.2038... Val Loss: 1.3472\n",
      "Epoch: 6/27... Step: 7520... Loss: 1.1843... Val Loss: 1.3530\n",
      "Epoch: 6/27... Step: 7530... Loss: 1.2286... Val Loss: 1.3490\n",
      "Epoch: 6/27... Step: 7540... Loss: 1.1608... Val Loss: 1.3516\n",
      "Epoch: 6/27... Step: 7550... Loss: 1.1920... Val Loss: 1.3505\n",
      "Epoch: 6/27... Step: 7560... Loss: 1.2297... Val Loss: 1.3486\n",
      "Epoch: 6/27... Step: 7570... Loss: 1.1880... Val Loss: 1.3496\n",
      "Epoch: 6/27... Step: 7580... Loss: 1.2560... Val Loss: 1.3541\n",
      "Epoch: 6/27... Step: 7590... Loss: 1.2198... Val Loss: 1.3511\n",
      "Epoch: 6/27... Step: 7600... Loss: 1.2762... Val Loss: 1.3507\n",
      "Epoch: 6/27... Step: 7610... Loss: 1.2197... Val Loss: 1.3492\n",
      "Epoch: 6/27... Step: 7620... Loss: 1.2980... Val Loss: 1.3446\n",
      "Epoch: 6/27... Step: 7630... Loss: 1.2388... Val Loss: 1.3442\n",
      "Epoch: 6/27... Step: 7640... Loss: 1.2524... Val Loss: 1.3482\n",
      "Epoch: 6/27... Step: 7650... Loss: 1.2881... Val Loss: 1.3465\n",
      "Epoch: 6/27... Step: 7660... Loss: 1.2553... Val Loss: 1.3499\n",
      "Epoch: 6/27... Step: 7670... Loss: 1.2439... Val Loss: 1.3488\n",
      "Epoch: 6/27... Step: 7680... Loss: 1.2631... Val Loss: 1.3472\n",
      "Epoch: 6/27... Step: 7690... Loss: 1.2306... Val Loss: 1.3468\n",
      "Epoch: 6/27... Step: 7700... Loss: 1.2465... Val Loss: 1.3483\n",
      "Epoch: 6/27... Step: 7710... Loss: 1.2198... Val Loss: 1.3519\n",
      "Epoch: 6/27... Step: 7720... Loss: 1.2712... Val Loss: 1.3502\n",
      "Epoch: 6/27... Step: 7730... Loss: 1.2135... Val Loss: 1.3493\n",
      "Epoch: 6/27... Step: 7740... Loss: 1.2700... Val Loss: 1.3528\n",
      "Epoch: 6/27... Step: 7750... Loss: 1.2336... Val Loss: 1.3529\n",
      "Epoch: 6/27... Step: 7760... Loss: 1.2658... Val Loss: 1.3494\n",
      "Epoch: 6/27... Step: 7770... Loss: 1.2463... Val Loss: 1.3498\n",
      "Epoch: 6/27... Step: 7780... Loss: 1.2362... Val Loss: 1.3440\n",
      "Epoch: 6/27... Step: 7790... Loss: 1.2279... Val Loss: 1.3420\n",
      "Epoch: 6/27... Step: 7800... Loss: 1.2187... Val Loss: 1.3460\n",
      "Epoch: 6/27... Step: 7810... Loss: 1.2163... Val Loss: 1.3447\n",
      "Epoch: 6/27... Step: 7820... Loss: 1.3182... Val Loss: 1.3424\n",
      "Epoch: 6/27... Step: 7830... Loss: 1.2350... Val Loss: 1.3417\n",
      "Epoch: 6/27... Step: 7840... Loss: 1.2185... Val Loss: 1.3539\n",
      "Epoch: 6/27... Step: 7850... Loss: 1.2455... Val Loss: 1.3496\n",
      "Epoch: 6/27... Step: 7860... Loss: 1.2757... Val Loss: 1.3454\n",
      "Epoch: 6/27... Step: 7870... Loss: 1.2170... Val Loss: 1.3462\n",
      "Epoch: 6/27... Step: 7880... Loss: 1.2413... Val Loss: 1.3449\n",
      "Epoch: 6/27... Step: 7890... Loss: 1.2169... Val Loss: 1.3420\n",
      "Epoch: 6/27... Step: 7900... Loss: 1.2417... Val Loss: 1.3435\n",
      "Epoch: 6/27... Step: 7910... Loss: 1.2443... Val Loss: 1.3502\n",
      "Epoch: 6/27... Step: 7920... Loss: 1.2958... Val Loss: 1.3427\n",
      "Epoch: 6/27... Step: 7930... Loss: 1.2341... Val Loss: 1.3450\n",
      "Epoch: 6/27... Step: 7940... Loss: 1.2694... Val Loss: 1.3454\n",
      "Epoch: 6/27... Step: 7950... Loss: 1.2781... Val Loss: 1.3463\n",
      "Epoch: 6/27... Step: 7960... Loss: 1.2886... Val Loss: 1.3484\n",
      "Epoch: 6/27... Step: 7970... Loss: 1.2733... Val Loss: 1.3470\n",
      "Epoch: 6/27... Step: 7980... Loss: 1.2329... Val Loss: 1.3453\n",
      "Epoch: 6/27... Step: 7990... Loss: 1.1987... Val Loss: 1.3477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/27... Step: 8000... Loss: 1.2494... Val Loss: 1.3429\n",
      "Epoch: 6/27... Step: 8010... Loss: 1.2932... Val Loss: 1.3460\n",
      "Epoch: 6/27... Step: 8020... Loss: 1.2616... Val Loss: 1.3444\n",
      "Epoch: 6/27... Step: 8030... Loss: 1.2567... Val Loss: 1.3423\n",
      "Epoch: 6/27... Step: 8040... Loss: 1.2767... Val Loss: 1.3462\n",
      "Epoch: 6/27... Step: 8050... Loss: 1.2271... Val Loss: 1.3444\n",
      "Epoch: 6/27... Step: 8060... Loss: 1.2574... Val Loss: 1.3458\n",
      "Epoch: 6/27... Step: 8070... Loss: 1.2043... Val Loss: 1.3492\n",
      "Epoch: 6/27... Step: 8080... Loss: 1.1493... Val Loss: 1.3438\n",
      "Epoch: 6/27... Step: 8090... Loss: 1.2609... Val Loss: 1.3454\n",
      "Epoch: 6/27... Step: 8100... Loss: 1.2148... Val Loss: 1.3431\n",
      "Epoch: 6/27... Step: 8110... Loss: 1.2189... Val Loss: 1.3430\n",
      "Epoch: 6/27... Step: 8120... Loss: 1.2013... Val Loss: 1.3457\n",
      "Epoch: 6/27... Step: 8130... Loss: 1.2370... Val Loss: 1.3415\n",
      "Epoch: 6/27... Step: 8140... Loss: 1.2137... Val Loss: 1.3447\n",
      "Epoch: 6/27... Step: 8150... Loss: 1.1970... Val Loss: 1.3452\n",
      "Epoch: 6/27... Step: 8160... Loss: 1.2340... Val Loss: 1.3478\n",
      "Epoch: 6/27... Step: 8170... Loss: 1.2223... Val Loss: 1.3431\n",
      "Epoch: 6/27... Step: 8180... Loss: 1.1811... Val Loss: 1.3422\n",
      "Epoch: 6/27... Step: 8190... Loss: 1.1572... Val Loss: 1.3483\n",
      "Epoch: 6/27... Step: 8200... Loss: 1.3041... Val Loss: 1.3436\n",
      "Epoch: 6/27... Step: 8210... Loss: 1.2683... Val Loss: 1.3454\n",
      "Epoch: 6/27... Step: 8220... Loss: 1.2527... Val Loss: 1.3469\n",
      "Epoch: 6/27... Step: 8230... Loss: 1.2471... Val Loss: 1.3454\n",
      "Epoch: 6/27... Step: 8240... Loss: 1.2632... Val Loss: 1.3547\n",
      "Epoch: 6/27... Step: 8250... Loss: 1.3382... Val Loss: 1.3469\n",
      "Epoch: 6/27... Step: 8260... Loss: 1.2244... Val Loss: 1.3561\n",
      "Epoch: 6/27... Step: 8270... Loss: 1.1821... Val Loss: 1.3432\n",
      "Epoch: 6/27... Step: 8280... Loss: 1.2437... Val Loss: 1.3419\n",
      "Epoch: 6/27... Step: 8290... Loss: 1.2981... Val Loss: 1.3472\n",
      "Epoch: 6/27... Step: 8300... Loss: 1.1252... Val Loss: 1.3394\n",
      "Epoch: 6/27... Step: 8310... Loss: 1.3021... Val Loss: 1.3415\n",
      "Epoch: 6/27... Step: 8320... Loss: 1.2304... Val Loss: 1.3392\n",
      "Epoch: 6/27... Step: 8330... Loss: 1.1750... Val Loss: 1.3371\n",
      "Epoch: 6/27... Step: 8340... Loss: 1.1689... Val Loss: 1.3377\n",
      "Epoch: 6/27... Step: 8350... Loss: 1.2007... Val Loss: 1.3386\n",
      "Epoch: 6/27... Step: 8360... Loss: 1.1984... Val Loss: 1.3406\n",
      "Epoch: 6/27... Step: 8370... Loss: 1.8581... Val Loss: 1.3462\n",
      "Epoch: 7/27... Step: 8380... Loss: 1.2466... Val Loss: 1.3387\n",
      "Epoch: 7/27... Step: 8390... Loss: 1.2137... Val Loss: 1.3433\n",
      "Epoch: 7/27... Step: 8400... Loss: 1.2376... Val Loss: 1.3434\n",
      "Epoch: 7/27... Step: 8410... Loss: 1.2188... Val Loss: 1.3391\n",
      "Epoch: 7/27... Step: 8420... Loss: 1.2671... Val Loss: 1.3340\n",
      "Epoch: 7/27... Step: 8430... Loss: 1.2526... Val Loss: 1.3392\n",
      "Epoch: 7/27... Step: 8440... Loss: 1.1977... Val Loss: 1.3453\n",
      "Epoch: 7/27... Step: 8450... Loss: 1.1588... Val Loss: 1.3430\n",
      "Epoch: 7/27... Step: 8460... Loss: 1.1925... Val Loss: 1.3432\n",
      "Epoch: 7/27... Step: 8470... Loss: 1.2889... Val Loss: 1.3388\n",
      "Epoch: 7/27... Step: 8480... Loss: 1.2410... Val Loss: 1.3446\n",
      "Epoch: 7/27... Step: 8490... Loss: 1.2536... Val Loss: 1.3411\n",
      "Epoch: 7/27... Step: 8500... Loss: 1.2846... Val Loss: 1.3387\n",
      "Epoch: 7/27... Step: 8510... Loss: 1.2333... Val Loss: 1.3492\n",
      "Epoch: 7/27... Step: 8520... Loss: 1.2278... Val Loss: 1.3493\n",
      "Epoch: 7/27... Step: 8530... Loss: 1.1524... Val Loss: 1.3425\n",
      "Epoch: 7/27... Step: 8540... Loss: 1.2385... Val Loss: 1.3416\n",
      "Epoch: 7/27... Step: 8550... Loss: 1.2729... Val Loss: 1.3458\n",
      "Epoch: 7/27... Step: 8560... Loss: 1.2046... Val Loss: 1.3402\n",
      "Epoch: 7/27... Step: 8570... Loss: 1.2167... Val Loss: 1.3387\n",
      "Epoch: 7/27... Step: 8580... Loss: 1.1955... Val Loss: 1.3404\n",
      "Epoch: 7/27... Step: 8590... Loss: 1.3021... Val Loss: 1.3443\n",
      "Epoch: 7/27... Step: 8600... Loss: 1.1994... Val Loss: 1.3405\n",
      "Epoch: 7/27... Step: 8610... Loss: 1.2303... Val Loss: 1.3442\n",
      "Epoch: 7/27... Step: 8620... Loss: 1.2170... Val Loss: 1.3472\n",
      "Epoch: 7/27... Step: 8630... Loss: 1.1727... Val Loss: 1.3431\n",
      "Epoch: 7/27... Step: 8640... Loss: 1.1131... Val Loss: 1.3418\n",
      "Epoch: 7/27... Step: 8650... Loss: 1.2432... Val Loss: 1.3426\n",
      "Epoch: 7/27... Step: 8660... Loss: 1.2153... Val Loss: 1.3455\n",
      "Epoch: 7/27... Step: 8670... Loss: 1.3078... Val Loss: 1.3426\n",
      "Epoch: 7/27... Step: 8680... Loss: 1.2047... Val Loss: 1.3387\n",
      "Epoch: 7/27... Step: 8690... Loss: 1.2127... Val Loss: 1.3445\n",
      "Epoch: 7/27... Step: 8700... Loss: 1.2473... Val Loss: 1.3465\n",
      "Epoch: 7/27... Step: 8710... Loss: 1.2360... Val Loss: 1.3368\n",
      "Epoch: 7/27... Step: 8720... Loss: 1.2413... Val Loss: 1.3365\n",
      "Epoch: 7/27... Step: 8730... Loss: 1.2454... Val Loss: 1.3344\n",
      "Epoch: 7/27... Step: 8740... Loss: 1.3217... Val Loss: 1.3418\n",
      "Epoch: 7/27... Step: 8750... Loss: 1.2632... Val Loss: 1.3373\n",
      "Epoch: 7/27... Step: 8760... Loss: 1.2311... Val Loss: 1.3380\n",
      "Epoch: 7/27... Step: 8770... Loss: 1.1669... Val Loss: 1.3401\n",
      "Epoch: 7/27... Step: 8780... Loss: 1.2510... Val Loss: 1.3463\n",
      "Epoch: 7/27... Step: 8790... Loss: 1.2726... Val Loss: 1.3363\n",
      "Epoch: 7/27... Step: 8800... Loss: 1.2231... Val Loss: 1.3343\n",
      "Epoch: 7/27... Step: 8810... Loss: 1.2916... Val Loss: 1.3339\n",
      "Epoch: 7/27... Step: 8820... Loss: 1.2298... Val Loss: 1.3321\n",
      "Epoch: 7/27... Step: 8830... Loss: 1.2578... Val Loss: 1.3323\n",
      "Epoch: 7/27... Step: 8840... Loss: 1.2798... Val Loss: 1.3322\n",
      "Epoch: 7/27... Step: 8850... Loss: 1.2077... Val Loss: 1.3346\n",
      "Epoch: 7/27... Step: 8860... Loss: 1.1511... Val Loss: 1.3373\n",
      "Epoch: 7/27... Step: 8870... Loss: 1.2293... Val Loss: 1.3352\n",
      "Epoch: 7/27... Step: 8880... Loss: 1.1991... Val Loss: 1.3347\n",
      "Epoch: 7/27... Step: 8890... Loss: 1.2673... Val Loss: 1.3352\n",
      "Epoch: 7/27... Step: 8900... Loss: 1.1709... Val Loss: 1.3356\n",
      "Epoch: 7/27... Step: 8910... Loss: 1.1803... Val Loss: 1.3394\n",
      "Epoch: 7/27... Step: 8920... Loss: 1.2127... Val Loss: 1.3385\n",
      "Epoch: 7/27... Step: 8930... Loss: 1.2366... Val Loss: 1.3371\n",
      "Epoch: 7/27... Step: 8940... Loss: 1.3246... Val Loss: 1.3412\n",
      "Epoch: 7/27... Step: 8950... Loss: 1.2194... Val Loss: 1.3356\n",
      "Epoch: 7/27... Step: 8960... Loss: 1.1876... Val Loss: 1.3364\n",
      "Epoch: 7/27... Step: 8970... Loss: 1.2409... Val Loss: 1.3414\n",
      "Epoch: 7/27... Step: 8980... Loss: 1.2334... Val Loss: 1.3377\n",
      "Epoch: 7/27... Step: 8990... Loss: 1.2405... Val Loss: 1.3383\n",
      "Epoch: 7/27... Step: 9000... Loss: 1.2899... Val Loss: 1.3421\n",
      "Epoch: 7/27... Step: 9010... Loss: 1.2118... Val Loss: 1.3362\n",
      "Epoch: 7/27... Step: 9020... Loss: 1.2017... Val Loss: 1.3353\n",
      "Epoch: 7/27... Step: 9030... Loss: 1.2344... Val Loss: 1.3358\n",
      "Epoch: 7/27... Step: 9040... Loss: 1.2112... Val Loss: 1.3388\n",
      "Epoch: 7/27... Step: 9050... Loss: 1.2760... Val Loss: 1.3387\n",
      "Epoch: 7/27... Step: 9060... Loss: 1.2382... Val Loss: 1.3360\n",
      "Epoch: 7/27... Step: 9070... Loss: 1.2817... Val Loss: 1.3332\n",
      "Epoch: 7/27... Step: 9080... Loss: 1.2184... Val Loss: 1.3359\n",
      "Epoch: 7/27... Step: 9090... Loss: 1.2667... Val Loss: 1.3376\n",
      "Epoch: 7/27... Step: 9100... Loss: 1.2047... Val Loss: 1.3347\n",
      "Epoch: 7/27... Step: 9110... Loss: 1.2227... Val Loss: 1.3370\n",
      "Epoch: 7/27... Step: 9120... Loss: 1.2619... Val Loss: 1.3347\n",
      "Epoch: 7/27... Step: 9130... Loss: 1.2062... Val Loss: 1.3337\n",
      "Epoch: 7/27... Step: 9140... Loss: 1.1507... Val Loss: 1.3394\n",
      "Epoch: 7/27... Step: 9150... Loss: 1.3132... Val Loss: 1.3392\n",
      "Epoch: 7/27... Step: 9160... Loss: 1.2573... Val Loss: 1.3378\n",
      "Epoch: 7/27... Step: 9170... Loss: 1.2423... Val Loss: 1.3360\n",
      "Epoch: 7/27... Step: 9180... Loss: 1.2526... Val Loss: 1.3325\n",
      "Epoch: 7/27... Step: 9190... Loss: 1.2327... Val Loss: 1.3371\n",
      "Epoch: 7/27... Step: 9200... Loss: 1.2534... Val Loss: 1.3378\n",
      "Epoch: 7/27... Step: 9210... Loss: 1.2178... Val Loss: 1.3329\n",
      "Epoch: 7/27... Step: 9220... Loss: 1.2210... Val Loss: 1.3359\n",
      "Epoch: 7/27... Step: 9230... Loss: 1.2181... Val Loss: 1.3332\n",
      "Epoch: 7/27... Step: 9240... Loss: 1.1884... Val Loss: 1.3463\n",
      "Epoch: 7/27... Step: 9250... Loss: 1.3099... Val Loss: 1.3352\n",
      "Epoch: 7/27... Step: 9260... Loss: 1.2634... Val Loss: 1.3352\n",
      "Epoch: 7/27... Step: 9270... Loss: 1.2571... Val Loss: 1.3344\n",
      "Epoch: 7/27... Step: 9280... Loss: 1.2543... Val Loss: 1.3320\n",
      "Epoch: 7/27... Step: 9290... Loss: 1.3184... Val Loss: 1.3316\n",
      "Epoch: 7/27... Step: 9300... Loss: 1.1980... Val Loss: 1.3313\n",
      "Epoch: 7/27... Step: 9310... Loss: 1.2194... Val Loss: 1.3315\n",
      "Epoch: 7/27... Step: 9320... Loss: 1.2126... Val Loss: 1.3294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/27... Step: 9330... Loss: 1.2877... Val Loss: 1.3329\n",
      "Epoch: 7/27... Step: 9340... Loss: 1.1471... Val Loss: 1.3305\n",
      "Epoch: 7/27... Step: 9350... Loss: 1.2130... Val Loss: 1.3346\n",
      "Epoch: 7/27... Step: 9360... Loss: 1.2493... Val Loss: 1.3374\n",
      "Epoch: 7/27... Step: 9370... Loss: 1.2219... Val Loss: 1.3391\n",
      "Epoch: 7/27... Step: 9380... Loss: 1.2327... Val Loss: 1.3345\n",
      "Epoch: 7/27... Step: 9390... Loss: 1.2497... Val Loss: 1.3347\n",
      "Epoch: 7/27... Step: 9400... Loss: 1.2269... Val Loss: 1.3326\n",
      "Epoch: 7/27... Step: 9410... Loss: 1.1733... Val Loss: 1.3347\n",
      "Epoch: 7/27... Step: 9420... Loss: 1.1815... Val Loss: 1.3299\n",
      "Epoch: 7/27... Step: 9430... Loss: 1.2313... Val Loss: 1.3339\n",
      "Epoch: 7/27... Step: 9440... Loss: 1.2682... Val Loss: 1.3353\n",
      "Epoch: 7/27... Step: 9450... Loss: 1.2238... Val Loss: 1.3324\n",
      "Epoch: 7/27... Step: 9460... Loss: 1.1280... Val Loss: 1.3379\n",
      "Epoch: 7/27... Step: 9470... Loss: 1.2304... Val Loss: 1.3396\n",
      "Epoch: 7/27... Step: 9480... Loss: 1.2446... Val Loss: 1.3360\n",
      "Epoch: 7/27... Step: 9490... Loss: 1.1946... Val Loss: 1.3389\n",
      "Epoch: 7/27... Step: 9500... Loss: 1.1734... Val Loss: 1.3381\n",
      "Epoch: 7/27... Step: 9510... Loss: 1.1810... Val Loss: 1.3348\n",
      "Epoch: 7/27... Step: 9520... Loss: 1.2013... Val Loss: 1.3343\n",
      "Epoch: 7/27... Step: 9530... Loss: 1.1974... Val Loss: 1.3327\n",
      "Epoch: 7/27... Step: 9540... Loss: 1.2461... Val Loss: 1.3333\n",
      "Epoch: 7/27... Step: 9550... Loss: 1.2893... Val Loss: 1.3388\n",
      "Epoch: 7/27... Step: 9560... Loss: 1.2210... Val Loss: 1.3425\n",
      "Epoch: 7/27... Step: 9570... Loss: 1.2311... Val Loss: 1.3354\n",
      "Epoch: 7/27... Step: 9580... Loss: 1.1575... Val Loss: 1.3345\n",
      "Epoch: 7/27... Step: 9590... Loss: 1.2191... Val Loss: 1.3414\n",
      "Epoch: 7/27... Step: 9600... Loss: 1.1968... Val Loss: 1.3352\n",
      "Epoch: 7/27... Step: 9610... Loss: 1.1968... Val Loss: 1.3347\n",
      "Epoch: 7/27... Step: 9620... Loss: 1.1694... Val Loss: 1.3355\n",
      "Epoch: 7/27... Step: 9630... Loss: 1.1771... Val Loss: 1.3354\n",
      "Epoch: 7/27... Step: 9640... Loss: 1.2176... Val Loss: 1.3350\n",
      "Epoch: 7/27... Step: 9650... Loss: 1.1958... Val Loss: 1.3366\n",
      "Epoch: 7/27... Step: 9660... Loss: 1.2143... Val Loss: 1.3351\n",
      "Epoch: 7/27... Step: 9670... Loss: 1.2302... Val Loss: 1.3337\n",
      "Epoch: 7/27... Step: 9680... Loss: 1.2691... Val Loss: 1.3364\n",
      "Epoch: 7/27... Step: 9690... Loss: 1.2144... Val Loss: 1.3339\n",
      "Epoch: 7/27... Step: 9700... Loss: 1.1771... Val Loss: 1.3328\n",
      "Epoch: 7/27... Step: 9710... Loss: 1.2773... Val Loss: 1.3310\n",
      "Epoch: 7/27... Step: 9720... Loss: 1.2344... Val Loss: 1.3283\n",
      "Epoch: 7/27... Step: 9730... Loss: 1.1818... Val Loss: 1.3274\n",
      "Epoch: 7/27... Step: 9740... Loss: 1.2326... Val Loss: 1.3289\n",
      "Epoch: 7/27... Step: 9750... Loss: 1.2773... Val Loss: 1.3248\n",
      "Epoch: 7/27... Step: 9760... Loss: 1.2571... Val Loss: 1.3285\n",
      "Epoch: 8/27... Step: 9770... Loss: 1.2252... Val Loss: 1.3306\n",
      "Epoch: 8/27... Step: 9780... Loss: 1.2047... Val Loss: 1.3364\n",
      "Epoch: 8/27... Step: 9790... Loss: 1.2375... Val Loss: 1.3261\n",
      "Epoch: 8/27... Step: 9800... Loss: 1.1760... Val Loss: 1.3280\n",
      "Epoch: 8/27... Step: 9810... Loss: 1.2111... Val Loss: 1.3266\n",
      "Epoch: 8/27... Step: 9820... Loss: 1.2142... Val Loss: 1.3242\n",
      "Epoch: 8/27... Step: 9830... Loss: 1.2136... Val Loss: 1.3296\n",
      "Epoch: 8/27... Step: 9840... Loss: 1.1824... Val Loss: 1.3286\n",
      "Epoch: 8/27... Step: 9850... Loss: 1.1905... Val Loss: 1.3342\n",
      "Epoch: 8/27... Step: 9860... Loss: 1.2543... Val Loss: 1.3335\n",
      "Epoch: 8/27... Step: 9870... Loss: 1.1845... Val Loss: 1.3293\n",
      "Epoch: 8/27... Step: 9880... Loss: 1.1917... Val Loss: 1.3351\n",
      "Epoch: 8/27... Step: 9890... Loss: 1.2146... Val Loss: 1.3288\n",
      "Epoch: 8/27... Step: 9900... Loss: 1.1854... Val Loss: 1.3311\n",
      "Epoch: 8/27... Step: 9910... Loss: 1.2337... Val Loss: 1.3351\n",
      "Epoch: 8/27... Step: 9920... Loss: 1.2794... Val Loss: 1.3307\n",
      "Epoch: 8/27... Step: 9930... Loss: 1.2536... Val Loss: 1.3271\n",
      "Epoch: 8/27... Step: 9940... Loss: 1.1971... Val Loss: 1.3300\n",
      "Epoch: 8/27... Step: 9950... Loss: 1.2655... Val Loss: 1.3313\n",
      "Epoch: 8/27... Step: 9960... Loss: 1.2102... Val Loss: 1.3308\n",
      "Epoch: 8/27... Step: 9970... Loss: 1.1732... Val Loss: 1.3302\n",
      "Epoch: 8/27... Step: 9980... Loss: 1.2017... Val Loss: 1.3312\n",
      "Epoch: 8/27... Step: 9990... Loss: 1.2049... Val Loss: 1.3337\n",
      "Epoch: 8/27... Step: 10000... Loss: 1.1333... Val Loss: 1.3333\n",
      "Epoch: 8/27... Step: 10010... Loss: 1.1905... Val Loss: 1.3344\n",
      "Epoch: 8/27... Step: 10020... Loss: 1.2137... Val Loss: 1.3349\n",
      "Epoch: 8/27... Step: 10030... Loss: 1.2117... Val Loss: 1.3314\n",
      "Epoch: 8/27... Step: 10040... Loss: 1.2323... Val Loss: 1.3292\n",
      "Epoch: 8/27... Step: 10050... Loss: 1.1994... Val Loss: 1.3295\n",
      "Epoch: 8/27... Step: 10060... Loss: 1.2789... Val Loss: 1.3329\n",
      "Epoch: 8/27... Step: 10070... Loss: 1.1938... Val Loss: 1.3350\n",
      "Epoch: 8/27... Step: 10080... Loss: 1.2154... Val Loss: 1.3297\n",
      "Epoch: 8/27... Step: 10090... Loss: 1.1827... Val Loss: 1.3292\n",
      "Epoch: 8/27... Step: 10100... Loss: 1.2219... Val Loss: 1.3320\n",
      "Epoch: 8/27... Step: 10110... Loss: 1.2736... Val Loss: 1.3329\n",
      "Epoch: 8/27... Step: 10120... Loss: 1.2322... Val Loss: 1.3301\n",
      "Epoch: 8/27... Step: 10130... Loss: 1.2481... Val Loss: 1.3270\n",
      "Epoch: 8/27... Step: 10140... Loss: 1.2521... Val Loss: 1.3299\n",
      "Epoch: 8/27... Step: 10150... Loss: 1.2211... Val Loss: 1.3319\n",
      "Epoch: 8/27... Step: 10160... Loss: 1.1974... Val Loss: 1.3299\n",
      "Epoch: 8/27... Step: 10170... Loss: 1.2523... Val Loss: 1.3261\n",
      "Epoch: 8/27... Step: 10180... Loss: 1.1704... Val Loss: 1.3277\n",
      "Epoch: 8/27... Step: 10190... Loss: 1.1416... Val Loss: 1.3261\n",
      "Epoch: 8/27... Step: 10200... Loss: 1.2694... Val Loss: 1.3280\n",
      "Epoch: 8/27... Step: 10210... Loss: 1.1580... Val Loss: 1.3231\n",
      "Epoch: 8/27... Step: 10220... Loss: 1.2075... Val Loss: 1.3253\n",
      "Epoch: 8/27... Step: 10230... Loss: 1.2542... Val Loss: 1.3209\n",
      "Epoch: 8/27... Step: 10240... Loss: 1.1512... Val Loss: 1.3229\n",
      "Epoch: 8/27... Step: 10250... Loss: 1.2015... Val Loss: 1.3223\n",
      "Epoch: 8/27... Step: 10260... Loss: 1.2350... Val Loss: 1.3278\n",
      "Epoch: 8/27... Step: 10270... Loss: 1.1838... Val Loss: 1.3206\n",
      "Epoch: 8/27... Step: 10280... Loss: 1.1942... Val Loss: 1.3211\n",
      "Epoch: 8/27... Step: 10290... Loss: 1.2141... Val Loss: 1.3232\n",
      "Epoch: 8/27... Step: 10300... Loss: 1.1762... Val Loss: 1.3242\n",
      "Epoch: 8/27... Step: 10310... Loss: 1.1392... Val Loss: 1.3297\n",
      "Epoch: 8/27... Step: 10320... Loss: 1.1608... Val Loss: 1.3274\n",
      "Epoch: 8/27... Step: 10330... Loss: 1.1298... Val Loss: 1.3272\n",
      "Epoch: 8/27... Step: 10340... Loss: 1.1547... Val Loss: 1.3276\n",
      "Epoch: 8/27... Step: 10350... Loss: 1.1796... Val Loss: 1.3245\n",
      "Epoch: 8/27... Step: 10360... Loss: 1.1544... Val Loss: 1.3302\n",
      "Epoch: 8/27... Step: 10370... Loss: 1.2217... Val Loss: 1.3297\n",
      "Epoch: 8/27... Step: 10380... Loss: 1.1747... Val Loss: 1.3272\n",
      "Epoch: 8/27... Step: 10390... Loss: 1.2716... Val Loss: 1.3305\n",
      "Epoch: 8/27... Step: 10400... Loss: 1.1835... Val Loss: 1.3283\n",
      "Epoch: 8/27... Step: 10410... Loss: 1.2767... Val Loss: 1.3273\n",
      "Epoch: 8/27... Step: 10420... Loss: 1.2033... Val Loss: 1.3295\n",
      "Epoch: 8/27... Step: 10430... Loss: 1.2210... Val Loss: 1.3251\n",
      "Epoch: 8/27... Step: 10440... Loss: 1.2648... Val Loss: 1.3272\n",
      "Epoch: 8/27... Step: 10450... Loss: 1.2000... Val Loss: 1.3277\n",
      "Epoch: 8/27... Step: 10460... Loss: 1.2001... Val Loss: 1.3322\n",
      "Epoch: 8/27... Step: 10470... Loss: 1.2278... Val Loss: 1.3264\n",
      "Epoch: 8/27... Step: 10480... Loss: 1.2218... Val Loss: 1.3282\n",
      "Epoch: 8/27... Step: 10490... Loss: 1.2262... Val Loss: 1.3250\n",
      "Epoch: 8/27... Step: 10500... Loss: 1.1949... Val Loss: 1.3340\n",
      "Epoch: 8/27... Step: 10510... Loss: 1.2557... Val Loss: 1.3300\n",
      "Epoch: 8/27... Step: 10520... Loss: 1.1681... Val Loss: 1.3256\n",
      "Epoch: 8/27... Step: 10530... Loss: 1.2175... Val Loss: 1.3259\n",
      "Epoch: 8/27... Step: 10540... Loss: 1.1999... Val Loss: 1.3325\n",
      "Epoch: 8/27... Step: 10550... Loss: 1.2586... Val Loss: 1.3291\n",
      "Epoch: 8/27... Step: 10560... Loss: 1.2130... Val Loss: 1.3282\n",
      "Epoch: 8/27... Step: 10570... Loss: 1.2096... Val Loss: 1.3267\n",
      "Epoch: 8/27... Step: 10580... Loss: 1.1918... Val Loss: 1.3245\n",
      "Epoch: 8/27... Step: 10590... Loss: 1.1787... Val Loss: 1.3296\n",
      "Epoch: 8/27... Step: 10600... Loss: 1.1596... Val Loss: 1.3311\n",
      "Epoch: 8/27... Step: 10610... Loss: 1.2993... Val Loss: 1.3285\n",
      "Epoch: 8/27... Step: 10620... Loss: 1.1795... Val Loss: 1.3265\n",
      "Epoch: 8/27... Step: 10630... Loss: 1.1965... Val Loss: 1.3331\n",
      "Epoch: 8/27... Step: 10640... Loss: 1.2451... Val Loss: 1.3304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/27... Step: 10650... Loss: 1.2568... Val Loss: 1.3248\n",
      "Epoch: 8/27... Step: 10660... Loss: 1.1866... Val Loss: 1.3271\n",
      "Epoch: 8/27... Step: 10670... Loss: 1.2164... Val Loss: 1.3274\n",
      "Epoch: 8/27... Step: 10680... Loss: 1.1772... Val Loss: 1.3251\n",
      "Epoch: 8/27... Step: 10690... Loss: 1.2122... Val Loss: 1.3238\n",
      "Epoch: 8/27... Step: 10700... Loss: 1.2152... Val Loss: 1.3255\n",
      "Epoch: 8/27... Step: 10710... Loss: 1.2832... Val Loss: 1.3228\n",
      "Epoch: 8/27... Step: 10720... Loss: 1.1985... Val Loss: 1.3227\n",
      "Epoch: 8/27... Step: 10730... Loss: 1.2293... Val Loss: 1.3238\n",
      "Epoch: 8/27... Step: 10740... Loss: 1.2527... Val Loss: 1.3252\n",
      "Epoch: 8/27... Step: 10750... Loss: 1.2395... Val Loss: 1.3298\n",
      "Epoch: 8/27... Step: 10760... Loss: 1.2069... Val Loss: 1.3298\n",
      "Epoch: 8/27... Step: 10770... Loss: 1.1952... Val Loss: 1.3275\n",
      "Epoch: 8/27... Step: 10780... Loss: 1.1686... Val Loss: 1.3278\n",
      "Epoch: 8/27... Step: 10790... Loss: 1.2057... Val Loss: 1.3257\n",
      "Epoch: 8/27... Step: 10800... Loss: 1.2510... Val Loss: 1.3259\n",
      "Epoch: 8/27... Step: 10810... Loss: 1.2393... Val Loss: 1.3278\n",
      "Epoch: 8/27... Step: 10820... Loss: 1.2080... Val Loss: 1.3319\n",
      "Epoch: 8/27... Step: 10830... Loss: 1.2099... Val Loss: 1.3345\n",
      "Epoch: 8/27... Step: 10840... Loss: 1.1911... Val Loss: 1.3317\n",
      "Epoch: 8/27... Step: 10850... Loss: 1.2257... Val Loss: 1.3301\n",
      "Epoch: 8/27... Step: 10860... Loss: 1.1618... Val Loss: 1.3361\n",
      "Epoch: 8/27... Step: 10870... Loss: 1.1344... Val Loss: 1.3298\n",
      "Epoch: 8/27... Step: 10880... Loss: 1.1864... Val Loss: 1.3251\n",
      "Epoch: 8/27... Step: 10890... Loss: 1.1871... Val Loss: 1.3290\n",
      "Epoch: 8/27... Step: 10900... Loss: 1.1703... Val Loss: 1.3299\n",
      "Epoch: 8/27... Step: 10910... Loss: 1.1999... Val Loss: 1.3305\n",
      "Epoch: 8/27... Step: 10920... Loss: 1.1850... Val Loss: 1.3278\n",
      "Epoch: 8/27... Step: 10930... Loss: 1.1778... Val Loss: 1.3301\n",
      "Epoch: 8/27... Step: 10940... Loss: 1.1651... Val Loss: 1.3273\n",
      "Epoch: 8/27... Step: 10950... Loss: 1.1926... Val Loss: 1.3332\n",
      "Epoch: 8/27... Step: 10960... Loss: 1.1932... Val Loss: 1.3305\n",
      "Epoch: 8/27... Step: 10970... Loss: 1.1414... Val Loss: 1.3256\n",
      "Epoch: 8/27... Step: 10980... Loss: 1.1357... Val Loss: 1.3319\n",
      "Epoch: 8/27... Step: 10990... Loss: 1.2588... Val Loss: 1.3329\n",
      "Epoch: 8/27... Step: 11000... Loss: 1.2353... Val Loss: 1.3307\n",
      "Epoch: 8/27... Step: 11010... Loss: 1.2263... Val Loss: 1.3294\n",
      "Epoch: 8/27... Step: 11020... Loss: 1.2292... Val Loss: 1.3351\n",
      "Epoch: 8/27... Step: 11030... Loss: 1.2080... Val Loss: 1.3282\n",
      "Epoch: 8/27... Step: 11040... Loss: 1.2944... Val Loss: 1.3269\n",
      "Epoch: 8/27... Step: 11050... Loss: 1.1808... Val Loss: 1.3315\n",
      "Epoch: 8/27... Step: 11060... Loss: 1.1241... Val Loss: 1.3280\n",
      "Epoch: 8/27... Step: 11070... Loss: 1.2235... Val Loss: 1.3271\n",
      "Epoch: 8/27... Step: 11080... Loss: 1.2393... Val Loss: 1.3304\n",
      "Epoch: 8/27... Step: 11090... Loss: 1.0973... Val Loss: 1.3248\n",
      "Epoch: 8/27... Step: 11100... Loss: 1.2644... Val Loss: 1.3248\n",
      "Epoch: 8/27... Step: 11110... Loss: 1.1659... Val Loss: 1.3237\n",
      "Epoch: 8/27... Step: 11120... Loss: 1.1411... Val Loss: 1.3218\n",
      "Epoch: 8/27... Step: 11130... Loss: 1.1343... Val Loss: 1.3217\n",
      "Epoch: 8/27... Step: 11140... Loss: 1.1692... Val Loss: 1.3260\n",
      "Epoch: 8/27... Step: 11150... Loss: 1.1506... Val Loss: 1.3247\n",
      "Epoch: 8/27... Step: 11160... Loss: 1.8406... Val Loss: 1.3274\n",
      "Epoch: 9/27... Step: 11170... Loss: 1.2263... Val Loss: 1.3274\n",
      "Epoch: 9/27... Step: 11180... Loss: 1.1906... Val Loss: 1.3255\n",
      "Epoch: 9/27... Step: 11190... Loss: 1.2154... Val Loss: 1.3209\n",
      "Epoch: 9/27... Step: 11200... Loss: 1.1986... Val Loss: 1.3201\n",
      "Epoch: 9/27... Step: 11210... Loss: 1.2132... Val Loss: 1.3207\n",
      "Epoch: 9/27... Step: 11220... Loss: 1.2266... Val Loss: 1.3223\n",
      "Epoch: 9/27... Step: 11230... Loss: 1.1756... Val Loss: 1.3264\n",
      "Epoch: 9/27... Step: 11240... Loss: 1.1461... Val Loss: 1.3248\n",
      "Epoch: 9/27... Step: 11250... Loss: 1.1668... Val Loss: 1.3290\n",
      "Epoch: 9/27... Step: 11260... Loss: 1.2406... Val Loss: 1.3236\n",
      "Epoch: 9/27... Step: 11270... Loss: 1.2329... Val Loss: 1.3239\n",
      "Epoch: 9/27... Step: 11280... Loss: 1.2154... Val Loss: 1.3243\n",
      "Epoch: 9/27... Step: 11290... Loss: 1.2335... Val Loss: 1.3226\n",
      "Epoch: 9/27... Step: 11300... Loss: 1.1858... Val Loss: 1.3272\n",
      "Epoch: 9/27... Step: 11310... Loss: 1.1717... Val Loss: 1.3263\n",
      "Epoch: 9/27... Step: 11320... Loss: 1.1146... Val Loss: 1.3208\n",
      "Epoch: 9/27... Step: 11330... Loss: 1.1841... Val Loss: 1.3203\n",
      "Epoch: 9/27... Step: 11340... Loss: 1.2396... Val Loss: 1.3233\n",
      "Epoch: 9/27... Step: 11350... Loss: 1.1845... Val Loss: 1.3212\n",
      "Epoch: 9/27... Step: 11360... Loss: 1.1630... Val Loss: 1.3212\n",
      "Epoch: 9/27... Step: 11370... Loss: 1.1243... Val Loss: 1.3194\n",
      "Epoch: 9/27... Step: 11380... Loss: 1.2230... Val Loss: 1.3213\n",
      "Epoch: 9/27... Step: 11390... Loss: 1.1771... Val Loss: 1.3212\n",
      "Epoch: 9/27... Step: 11400... Loss: 1.2061... Val Loss: 1.3260\n",
      "Epoch: 9/27... Step: 11410... Loss: 1.1575... Val Loss: 1.3235\n",
      "Epoch: 9/27... Step: 11420... Loss: 1.1444... Val Loss: 1.3244\n",
      "Epoch: 9/27... Step: 11430... Loss: 1.0608... Val Loss: 1.3224\n",
      "Epoch: 9/27... Step: 11440... Loss: 1.2060... Val Loss: 1.3229\n",
      "Epoch: 9/27... Step: 11450... Loss: 1.2047... Val Loss: 1.3242\n",
      "Epoch: 9/27... Step: 11460... Loss: 1.2625... Val Loss: 1.3296\n",
      "Epoch: 9/27... Step: 11470... Loss: 1.1939... Val Loss: 1.3256\n",
      "Epoch: 9/27... Step: 11480... Loss: 1.1659... Val Loss: 1.3255\n",
      "Epoch: 9/27... Step: 11490... Loss: 1.2170... Val Loss: 1.3258\n",
      "Epoch: 9/27... Step: 11500... Loss: 1.2136... Val Loss: 1.3231\n",
      "Epoch: 9/27... Step: 11510... Loss: 1.2145... Val Loss: 1.3219\n",
      "Epoch: 9/27... Step: 11520... Loss: 1.2278... Val Loss: 1.3206\n",
      "Epoch: 9/27... Step: 11530... Loss: 1.2725... Val Loss: 1.3218\n",
      "Epoch: 9/27... Step: 11540... Loss: 1.2240... Val Loss: 1.3226\n",
      "Epoch: 9/27... Step: 11550... Loss: 1.2083... Val Loss: 1.3218\n",
      "Epoch: 9/27... Step: 11560... Loss: 1.1408... Val Loss: 1.3252\n",
      "Epoch: 9/27... Step: 11570... Loss: 1.2212... Val Loss: 1.3229\n",
      "Epoch: 9/27... Step: 11580... Loss: 1.2479... Val Loss: 1.3247\n",
      "Epoch: 9/27... Step: 11590... Loss: 1.1820... Val Loss: 1.3276\n",
      "Epoch: 9/27... Step: 11600... Loss: 1.2343... Val Loss: 1.3252\n",
      "Epoch: 9/27... Step: 11610... Loss: 1.2187... Val Loss: 1.3177\n",
      "Epoch: 9/27... Step: 11620... Loss: 1.2243... Val Loss: 1.3205\n",
      "Epoch: 9/27... Step: 11630... Loss: 1.2536... Val Loss: 1.3186\n",
      "Epoch: 9/27... Step: 11640... Loss: 1.1399... Val Loss: 1.3193\n",
      "Epoch: 9/27... Step: 11650... Loss: 1.1251... Val Loss: 1.3245\n",
      "Epoch: 9/27... Step: 11660... Loss: 1.2277... Val Loss: 1.3251\n",
      "Epoch: 9/27... Step: 11670... Loss: 1.1803... Val Loss: 1.3220\n",
      "Epoch: 9/27... Step: 11680... Loss: 1.2663... Val Loss: 1.3180\n",
      "Epoch: 9/27... Step: 11690... Loss: 1.1381... Val Loss: 1.3215\n",
      "Epoch: 9/27... Step: 11700... Loss: 1.1602... Val Loss: 1.3238\n",
      "Epoch: 9/27... Step: 11710... Loss: 1.1681... Val Loss: 1.3240\n",
      "Epoch: 9/27... Step: 11720... Loss: 1.1930... Val Loss: 1.3209\n",
      "Epoch: 9/27... Step: 11730... Loss: 1.2984... Val Loss: 1.3239\n",
      "Epoch: 9/27... Step: 11740... Loss: 1.1965... Val Loss: 1.3210\n",
      "Epoch: 9/27... Step: 11750... Loss: 1.1863... Val Loss: 1.3210\n",
      "Epoch: 9/27... Step: 11760... Loss: 1.1997... Val Loss: 1.3234\n",
      "Epoch: 9/27... Step: 11770... Loss: 1.2083... Val Loss: 1.3221\n",
      "Epoch: 9/27... Step: 11780... Loss: 1.2461... Val Loss: 1.3198\n",
      "Epoch: 9/27... Step: 11790... Loss: 1.2578... Val Loss: 1.3256\n",
      "Epoch: 9/27... Step: 11800... Loss: 1.1708... Val Loss: 1.3237\n",
      "Epoch: 9/27... Step: 11810... Loss: 1.1609... Val Loss: 1.3239\n",
      "Epoch: 9/27... Step: 11820... Loss: 1.1866... Val Loss: 1.3181\n",
      "Epoch: 9/27... Step: 11830... Loss: 1.1745... Val Loss: 1.3190\n",
      "Epoch: 9/27... Step: 11840... Loss: 1.2411... Val Loss: 1.3222\n",
      "Epoch: 9/27... Step: 11850... Loss: 1.2279... Val Loss: 1.3195\n",
      "Epoch: 9/27... Step: 11860... Loss: 1.2849... Val Loss: 1.3202\n",
      "Epoch: 9/27... Step: 11870... Loss: 1.1742... Val Loss: 1.3202\n",
      "Epoch: 9/27... Step: 11880... Loss: 1.1998... Val Loss: 1.3217\n",
      "Epoch: 9/27... Step: 11890... Loss: 1.1677... Val Loss: 1.3224\n",
      "Epoch: 9/27... Step: 11900... Loss: 1.1992... Val Loss: 1.3254\n",
      "Epoch: 9/27... Step: 11910... Loss: 1.2402... Val Loss: 1.3246\n",
      "Epoch: 9/27... Step: 11920... Loss: 1.1535... Val Loss: 1.3198\n",
      "Epoch: 9/27... Step: 11930... Loss: 1.1292... Val Loss: 1.3277\n",
      "Epoch: 9/27... Step: 11940... Loss: 1.3060... Val Loss: 1.3274\n",
      "Epoch: 9/27... Step: 11950... Loss: 1.2205... Val Loss: 1.3194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/27... Step: 11960... Loss: 1.1965... Val Loss: 1.3203\n",
      "Epoch: 9/27... Step: 11970... Loss: 1.1917... Val Loss: 1.3185\n",
      "Epoch: 9/27... Step: 11980... Loss: 1.2299... Val Loss: 1.3205\n",
      "Epoch: 9/27... Step: 11990... Loss: 1.2070... Val Loss: 1.3280\n",
      "Epoch: 9/27... Step: 12000... Loss: 1.1641... Val Loss: 1.3238\n",
      "Epoch: 9/27... Step: 12010... Loss: 1.2023... Val Loss: 1.3154\n",
      "Epoch: 9/27... Step: 12020... Loss: 1.1676... Val Loss: 1.3208\n",
      "Epoch: 9/27... Step: 12030... Loss: 1.1681... Val Loss: 1.3241\n",
      "Epoch: 9/27... Step: 12040... Loss: 1.2856... Val Loss: 1.3198\n",
      "Epoch: 9/27... Step: 12050... Loss: 1.2456... Val Loss: 1.3197\n",
      "Epoch: 9/27... Step: 12060... Loss: 1.2069... Val Loss: 1.3238\n",
      "Epoch: 9/27... Step: 12070... Loss: 1.2224... Val Loss: 1.3232\n",
      "Epoch: 9/27... Step: 12080... Loss: 1.2916... Val Loss: 1.3216\n",
      "Epoch: 9/27... Step: 12090... Loss: 1.1665... Val Loss: 1.3228\n",
      "Epoch: 9/27... Step: 12100... Loss: 1.1673... Val Loss: 1.3189\n",
      "Epoch: 9/27... Step: 12110... Loss: 1.1921... Val Loss: 1.3190\n",
      "Epoch: 9/27... Step: 12120... Loss: 1.2437... Val Loss: 1.3217\n",
      "Epoch: 9/27... Step: 12130... Loss: 1.1236... Val Loss: 1.3186\n",
      "Epoch: 9/27... Step: 12140... Loss: 1.1617... Val Loss: 1.3218\n",
      "Epoch: 9/27... Step: 12150... Loss: 1.2182... Val Loss: 1.3252\n",
      "Epoch: 9/27... Step: 12160... Loss: 1.2126... Val Loss: 1.3251\n",
      "Epoch: 9/27... Step: 12170... Loss: 1.2035... Val Loss: 1.3211\n",
      "Epoch: 9/27... Step: 12180... Loss: 1.2208... Val Loss: 1.3205\n",
      "Epoch: 9/27... Step: 12190... Loss: 1.1916... Val Loss: 1.3204\n",
      "Epoch: 9/27... Step: 12200... Loss: 1.1431... Val Loss: 1.3234\n",
      "Epoch: 9/27... Step: 12210... Loss: 1.1476... Val Loss: 1.3181\n",
      "Epoch: 9/27... Step: 12220... Loss: 1.1917... Val Loss: 1.3240\n",
      "Epoch: 9/27... Step: 12230... Loss: 1.2311... Val Loss: 1.3271\n",
      "Epoch: 9/27... Step: 12240... Loss: 1.2071... Val Loss: 1.3218\n",
      "Epoch: 9/27... Step: 12250... Loss: 1.0904... Val Loss: 1.3219\n",
      "Epoch: 9/27... Step: 12260... Loss: 1.2355... Val Loss: 1.3291\n",
      "Epoch: 9/27... Step: 12270... Loss: 1.2179... Val Loss: 1.3205\n",
      "Epoch: 9/27... Step: 12280... Loss: 1.1993... Val Loss: 1.3212\n",
      "Epoch: 9/27... Step: 12290... Loss: 1.1148... Val Loss: 1.3253\n",
      "Epoch: 9/27... Step: 12300... Loss: 1.1092... Val Loss: 1.3290\n",
      "Epoch: 9/27... Step: 12310... Loss: 1.1602... Val Loss: 1.3217\n",
      "Epoch: 9/27... Step: 12320... Loss: 1.1764... Val Loss: 1.3259\n",
      "Epoch: 9/27... Step: 12330... Loss: 1.2236... Val Loss: 1.3237\n",
      "Epoch: 9/27... Step: 12340... Loss: 1.2431... Val Loss: 1.3274\n",
      "Epoch: 9/27... Step: 12350... Loss: 1.1860... Val Loss: 1.3216\n",
      "Epoch: 9/27... Step: 12360... Loss: 1.1793... Val Loss: 1.3202\n",
      "Epoch: 9/27... Step: 12370... Loss: 1.1142... Val Loss: 1.3212\n",
      "Epoch: 9/27... Step: 12380... Loss: 1.1836... Val Loss: 1.3282\n",
      "Epoch: 9/27... Step: 12390... Loss: 1.1584... Val Loss: 1.3228\n",
      "Epoch: 9/27... Step: 12400... Loss: 1.1752... Val Loss: 1.3212\n",
      "Epoch: 9/27... Step: 12410... Loss: 1.1564... Val Loss: 1.3264\n",
      "Epoch: 9/27... Step: 12420... Loss: 1.1620... Val Loss: 1.3189\n",
      "Epoch: 9/27... Step: 12430... Loss: 1.1899... Val Loss: 1.3196\n",
      "Epoch: 9/27... Step: 12440... Loss: 1.1692... Val Loss: 1.3238\n",
      "Epoch: 9/27... Step: 12450... Loss: 1.2191... Val Loss: 1.3253\n",
      "Epoch: 9/27... Step: 12460... Loss: 1.2384... Val Loss: 1.3208\n",
      "Epoch: 9/27... Step: 12470... Loss: 1.2588... Val Loss: 1.3202\n",
      "Epoch: 9/27... Step: 12480... Loss: 1.1983... Val Loss: 1.3251\n",
      "Epoch: 9/27... Step: 12490... Loss: 1.1506... Val Loss: 1.3202\n",
      "Epoch: 9/27... Step: 12500... Loss: 1.2485... Val Loss: 1.3185\n",
      "Epoch: 9/27... Step: 12510... Loss: 1.2372... Val Loss: 1.3157\n",
      "Epoch: 9/27... Step: 12520... Loss: 1.1799... Val Loss: 1.3172\n",
      "Epoch: 9/27... Step: 12530... Loss: 1.2245... Val Loss: 1.3179\n",
      "Epoch: 9/27... Step: 12540... Loss: 1.2403... Val Loss: 1.3147\n",
      "Epoch: 9/27... Step: 12550... Loss: 1.2392... Val Loss: 1.3170\n",
      "Epoch: 10/27... Step: 12560... Loss: 1.2006... Val Loss: 1.3183\n",
      "Epoch: 10/27... Step: 12570... Loss: 1.1557... Val Loss: 1.3167\n",
      "Epoch: 10/27... Step: 12580... Loss: 1.2263... Val Loss: 1.3155\n",
      "Epoch: 10/27... Step: 12590... Loss: 1.1519... Val Loss: 1.3177\n",
      "Epoch: 10/27... Step: 12600... Loss: 1.1683... Val Loss: 1.3173\n",
      "Epoch: 10/27... Step: 12610... Loss: 1.1815... Val Loss: 1.3198\n",
      "Epoch: 10/27... Step: 12620... Loss: 1.1782... Val Loss: 1.3191\n",
      "Epoch: 10/27... Step: 12630... Loss: 1.1511... Val Loss: 1.3195\n",
      "Epoch: 10/27... Step: 12640... Loss: 1.1489... Val Loss: 1.3198\n",
      "Epoch: 10/27... Step: 12650... Loss: 1.2165... Val Loss: 1.3201\n",
      "Epoch: 10/27... Step: 12660... Loss: 1.1846... Val Loss: 1.3209\n",
      "Epoch: 10/27... Step: 12670... Loss: 1.1856... Val Loss: 1.3204\n",
      "Epoch: 10/27... Step: 12680... Loss: 1.1699... Val Loss: 1.3146\n",
      "Epoch: 10/27... Step: 12690... Loss: 1.1836... Val Loss: 1.3195\n",
      "Epoch: 10/27... Step: 12700... Loss: 1.2037... Val Loss: 1.3234\n",
      "Epoch: 10/27... Step: 12710... Loss: 1.2288... Val Loss: 1.3202\n",
      "Epoch: 10/27... Step: 12720... Loss: 1.2032... Val Loss: 1.3152\n",
      "Epoch: 10/27... Step: 12730... Loss: 1.1757... Val Loss: 1.3170\n",
      "Epoch: 10/27... Step: 12740... Loss: 1.2410... Val Loss: 1.3194\n",
      "Epoch: 10/27... Step: 12750... Loss: 1.1874... Val Loss: 1.3160\n",
      "Epoch: 10/27... Step: 12760... Loss: 1.1566... Val Loss: 1.3151\n",
      "Epoch: 10/27... Step: 12770... Loss: 1.1663... Val Loss: 1.3165\n",
      "Epoch: 10/27... Step: 12780... Loss: 1.1684... Val Loss: 1.3222\n",
      "Epoch: 10/27... Step: 12790... Loss: 1.0997... Val Loss: 1.3206\n",
      "Epoch: 10/27... Step: 12800... Loss: 1.1745... Val Loss: 1.3191\n",
      "Epoch: 10/27... Step: 12810... Loss: 1.1999... Val Loss: 1.3195\n",
      "Epoch: 10/27... Step: 12820... Loss: 1.1833... Val Loss: 1.3186\n",
      "Epoch: 10/27... Step: 12830... Loss: 1.2012... Val Loss: 1.3170\n",
      "Epoch: 10/27... Step: 12840... Loss: 1.1998... Val Loss: 1.3165\n",
      "Epoch: 10/27... Step: 12850... Loss: 1.2596... Val Loss: 1.3240\n",
      "Epoch: 10/27... Step: 12860... Loss: 1.1535... Val Loss: 1.3295\n",
      "Epoch: 10/27... Step: 12870... Loss: 1.1808... Val Loss: 1.3170\n",
      "Epoch: 10/27... Step: 12880... Loss: 1.1493... Val Loss: 1.3157\n",
      "Epoch: 10/27... Step: 12890... Loss: 1.1616... Val Loss: 1.3170\n",
      "Epoch: 10/27... Step: 12900... Loss: 1.2199... Val Loss: 1.3190\n",
      "Epoch: 10/27... Step: 12910... Loss: 1.1796... Val Loss: 1.3142\n",
      "Epoch: 10/27... Step: 12920... Loss: 1.2154... Val Loss: 1.3130\n",
      "Epoch: 10/27... Step: 12930... Loss: 1.2449... Val Loss: 1.3137\n",
      "Epoch: 10/27... Step: 12940... Loss: 1.1785... Val Loss: 1.3188\n",
      "Epoch: 10/27... Step: 12950... Loss: 1.1858... Val Loss: 1.3195\n",
      "Epoch: 10/27... Step: 12960... Loss: 1.2154... Val Loss: 1.3198\n",
      "Epoch: 10/27... Step: 12970... Loss: 1.1439... Val Loss: 1.3193\n",
      "Epoch: 10/27... Step: 12980... Loss: 1.1552... Val Loss: 1.3158\n",
      "Epoch: 10/27... Step: 12990... Loss: 1.2159... Val Loss: 1.3171\n",
      "Epoch: 10/27... Step: 13000... Loss: 1.1392... Val Loss: 1.3158\n",
      "Epoch: 10/27... Step: 13010... Loss: 1.1803... Val Loss: 1.3160\n",
      "Epoch: 10/27... Step: 13020... Loss: 1.2041... Val Loss: 1.3160\n",
      "Epoch: 10/27... Step: 13030... Loss: 1.1394... Val Loss: 1.3176\n",
      "Epoch: 10/27... Step: 13040... Loss: 1.1693... Val Loss: 1.3155\n",
      "Epoch: 10/27... Step: 13050... Loss: 1.1775... Val Loss: 1.3209\n",
      "Epoch: 10/27... Step: 13060... Loss: 1.1444... Val Loss: 1.3189\n",
      "Epoch: 10/27... Step: 13070... Loss: 1.1787... Val Loss: 1.3152\n",
      "Epoch: 10/27... Step: 13080... Loss: 1.2073... Val Loss: 1.3123\n",
      "Epoch: 10/27... Step: 13090... Loss: 1.1359... Val Loss: 1.3164\n",
      "Epoch: 10/27... Step: 13100... Loss: 1.1550... Val Loss: 1.3222\n",
      "Epoch: 10/27... Step: 13110... Loss: 1.1329... Val Loss: 1.3182\n",
      "Epoch: 10/27... Step: 13120... Loss: 1.0995... Val Loss: 1.3179\n",
      "Epoch: 10/27... Step: 13130... Loss: 1.1148... Val Loss: 1.3189\n",
      "Epoch: 10/27... Step: 13140... Loss: 1.1601... Val Loss: 1.3167\n",
      "Epoch: 10/27... Step: 13150... Loss: 1.1524... Val Loss: 1.3179\n",
      "Epoch: 10/27... Step: 13160... Loss: 1.1928... Val Loss: 1.3219\n",
      "Epoch: 10/27... Step: 13170... Loss: 1.1379... Val Loss: 1.3164\n",
      "Epoch: 10/27... Step: 13180... Loss: 1.2058... Val Loss: 1.3191\n",
      "Epoch: 10/27... Step: 13190... Loss: 1.1755... Val Loss: 1.3196\n",
      "Epoch: 10/27... Step: 13200... Loss: 1.2160... Val Loss: 1.3153\n",
      "Epoch: 10/27... Step: 13210... Loss: 1.1810... Val Loss: 1.3131\n",
      "Epoch: 10/27... Step: 13220... Loss: 1.1820... Val Loss: 1.3159\n",
      "Epoch: 10/27... Step: 13230... Loss: 1.2047... Val Loss: 1.3172\n",
      "Epoch: 10/27... Step: 13240... Loss: 1.1788... Val Loss: 1.3170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/27... Step: 13250... Loss: 1.2035... Val Loss: 1.3194\n",
      "Epoch: 10/27... Step: 13260... Loss: 1.1993... Val Loss: 1.3196\n",
      "Epoch: 10/27... Step: 13270... Loss: 1.1849... Val Loss: 1.3184\n",
      "Epoch: 10/27... Step: 13280... Loss: 1.1760... Val Loss: 1.3185\n",
      "Epoch: 10/27... Step: 13290... Loss: 1.1578... Val Loss: 1.3206\n",
      "Epoch: 10/27... Step: 13300... Loss: 1.2159... Val Loss: 1.3205\n",
      "Epoch: 10/27... Step: 13310... Loss: 1.1435... Val Loss: 1.3189\n",
      "Epoch: 10/27... Step: 13320... Loss: 1.1950... Val Loss: 1.3192\n",
      "Epoch: 10/27... Step: 13330... Loss: 1.1687... Val Loss: 1.3235\n",
      "Epoch: 10/27... Step: 13340... Loss: 1.2324... Val Loss: 1.3184\n",
      "Epoch: 10/27... Step: 13350... Loss: 1.2056... Val Loss: 1.3162\n",
      "Epoch: 10/27... Step: 13360... Loss: 1.1619... Val Loss: 1.3166\n",
      "Epoch: 10/27... Step: 13370... Loss: 1.1783... Val Loss: 1.3134\n",
      "Epoch: 10/27... Step: 13380... Loss: 1.1468... Val Loss: 1.3168\n",
      "Epoch: 10/27... Step: 13390... Loss: 1.1472... Val Loss: 1.3188\n",
      "Epoch: 10/27... Step: 13400... Loss: 1.2527... Val Loss: 1.3140\n",
      "Epoch: 10/27... Step: 13410... Loss: 1.1556... Val Loss: 1.3130\n",
      "Epoch: 10/27... Step: 13420... Loss: 1.1902... Val Loss: 1.3211\n",
      "Epoch: 10/27... Step: 13430... Loss: 1.1836... Val Loss: 1.3154\n",
      "Epoch: 10/27... Step: 13440... Loss: 1.2088... Val Loss: 1.3127\n",
      "Epoch: 10/27... Step: 13450... Loss: 1.1551... Val Loss: 1.3170\n",
      "Epoch: 10/27... Step: 13460... Loss: 1.1876... Val Loss: 1.3176\n",
      "Epoch: 10/27... Step: 13470... Loss: 1.1368... Val Loss: 1.3177\n",
      "Epoch: 10/27... Step: 13480... Loss: 1.1798... Val Loss: 1.3163\n",
      "Epoch: 10/27... Step: 13490... Loss: 1.1950... Val Loss: 1.3181\n",
      "Epoch: 10/27... Step: 13500... Loss: 1.2406... Val Loss: 1.3162\n",
      "Epoch: 10/27... Step: 13510... Loss: 1.2040... Val Loss: 1.3131\n",
      "Epoch: 10/27... Step: 13520... Loss: 1.2068... Val Loss: 1.3142\n",
      "Epoch: 10/27... Step: 13530... Loss: 1.1857... Val Loss: 1.3148\n",
      "Epoch: 10/27... Step: 13540... Loss: 1.2184... Val Loss: 1.3179\n",
      "Epoch: 10/27... Step: 13550... Loss: 1.1985... Val Loss: 1.3187\n",
      "Epoch: 10/27... Step: 13560... Loss: 1.1498... Val Loss: 1.3156\n",
      "Epoch: 10/27... Step: 13570... Loss: 1.1503... Val Loss: 1.3136\n",
      "Epoch: 10/27... Step: 13580... Loss: 1.1894... Val Loss: 1.3134\n",
      "Epoch: 10/27... Step: 13590... Loss: 1.2247... Val Loss: 1.3193\n",
      "Epoch: 10/27... Step: 13600... Loss: 1.2120... Val Loss: 1.3156\n",
      "Epoch: 10/27... Step: 13610... Loss: 1.1896... Val Loss: 1.3117\n",
      "Epoch: 10/27... Step: 13620... Loss: 1.2077... Val Loss: 1.3165\n",
      "Epoch: 10/27... Step: 13630... Loss: 1.1517... Val Loss: 1.3223\n",
      "Epoch: 10/27... Step: 13640... Loss: 1.2030... Val Loss: 1.3164\n",
      "Epoch: 10/27... Step: 13650... Loss: 1.1374... Val Loss: 1.3173\n",
      "Epoch: 10/27... Step: 13660... Loss: 1.1018... Val Loss: 1.3195\n",
      "Epoch: 10/27... Step: 13670... Loss: 1.1794... Val Loss: 1.3150\n",
      "Epoch: 10/27... Step: 13680... Loss: 1.1537... Val Loss: 1.3132\n",
      "Epoch: 10/27... Step: 13690... Loss: 1.1378... Val Loss: 1.3159\n",
      "Epoch: 10/27... Step: 13700... Loss: 1.1527... Val Loss: 1.3187\n",
      "Epoch: 10/27... Step: 13710... Loss: 1.1458... Val Loss: 1.3194\n",
      "Epoch: 10/27... Step: 13720... Loss: 1.1512... Val Loss: 1.3168\n",
      "Epoch: 10/27... Step: 13730... Loss: 1.1641... Val Loss: 1.3156\n",
      "Epoch: 10/27... Step: 13740... Loss: 1.1674... Val Loss: 1.3162\n",
      "Epoch: 10/27... Step: 13750... Loss: 1.1557... Val Loss: 1.3213\n",
      "Epoch: 10/27... Step: 13760... Loss: 1.1197... Val Loss: 1.3154\n",
      "Epoch: 10/27... Step: 13770... Loss: 1.0953... Val Loss: 1.3191\n",
      "Epoch: 10/27... Step: 13780... Loss: 1.2520... Val Loss: 1.3223\n",
      "Epoch: 10/27... Step: 13790... Loss: 1.1896... Val Loss: 1.3181\n",
      "Epoch: 10/27... Step: 13800... Loss: 1.2113... Val Loss: 1.3138\n",
      "Epoch: 10/27... Step: 13810... Loss: 1.1992... Val Loss: 1.3183\n",
      "Epoch: 10/27... Step: 13820... Loss: 1.2168... Val Loss: 1.3182\n",
      "Epoch: 10/27... Step: 13830... Loss: 1.2653... Val Loss: 1.3163\n",
      "Epoch: 10/27... Step: 13840... Loss: 1.1564... Val Loss: 1.3157\n",
      "Epoch: 10/27... Step: 13850... Loss: 1.1200... Val Loss: 1.3172\n",
      "Epoch: 10/27... Step: 13860... Loss: 1.1912... Val Loss: 1.3169\n",
      "Epoch: 10/27... Step: 13870... Loss: 1.2368... Val Loss: 1.3129\n",
      "Epoch: 10/27... Step: 13880... Loss: 1.0904... Val Loss: 1.3153\n",
      "Epoch: 10/27... Step: 13890... Loss: 1.2314... Val Loss: 1.3171\n",
      "Epoch: 10/27... Step: 13900... Loss: 1.1776... Val Loss: 1.3161\n",
      "Epoch: 10/27... Step: 13910... Loss: 1.1223... Val Loss: 1.3142\n",
      "Epoch: 10/27... Step: 13920... Loss: 1.1148... Val Loss: 1.3157\n",
      "Epoch: 10/27... Step: 13930... Loss: 1.1417... Val Loss: 1.3122\n",
      "Epoch: 10/27... Step: 13940... Loss: 1.1632... Val Loss: 1.3122\n",
      "Epoch: 10/27... Step: 13950... Loss: 1.8000... Val Loss: 1.3188\n",
      "Epoch: 11/27... Step: 13960... Loss: 1.1815... Val Loss: 1.3168\n",
      "Epoch: 11/27... Step: 13970... Loss: 1.1592... Val Loss: 1.3166\n",
      "Epoch: 11/27... Step: 13980... Loss: 1.1960... Val Loss: 1.3158\n",
      "Epoch: 11/27... Step: 13990... Loss: 1.1653... Val Loss: 1.3154\n",
      "Epoch: 11/27... Step: 14000... Loss: 1.2055... Val Loss: 1.3140\n",
      "Epoch: 11/27... Step: 14010... Loss: 1.2222... Val Loss: 1.3124\n",
      "Epoch: 11/27... Step: 14020... Loss: 1.1393... Val Loss: 1.3160\n",
      "Epoch: 11/27... Step: 14030... Loss: 1.1262... Val Loss: 1.3175\n",
      "Epoch: 11/27... Step: 14040... Loss: 1.1472... Val Loss: 1.3146\n",
      "Epoch: 11/27... Step: 14050... Loss: 1.1945... Val Loss: 1.3138\n",
      "Epoch: 11/27... Step: 14060... Loss: 1.2148... Val Loss: 1.3211\n",
      "Epoch: 11/27... Step: 14070... Loss: 1.2329... Val Loss: 1.3154\n",
      "Epoch: 11/27... Step: 14080... Loss: 1.2043... Val Loss: 1.3107\n",
      "Epoch: 11/27... Step: 14090... Loss: 1.1560... Val Loss: 1.3171\n",
      "Epoch: 11/27... Step: 14100... Loss: 1.1799... Val Loss: 1.3267\n",
      "Epoch: 11/27... Step: 14110... Loss: 1.1217... Val Loss: 1.3132\n",
      "Epoch: 11/27... Step: 14120... Loss: 1.1646... Val Loss: 1.3101\n",
      "Epoch: 11/27... Step: 14130... Loss: 1.1959... Val Loss: 1.3131\n",
      "Epoch: 11/27... Step: 14140... Loss: 1.1632... Val Loss: 1.3142\n",
      "Epoch: 11/27... Step: 14150... Loss: 1.1519... Val Loss: 1.3126\n",
      "Epoch: 11/27... Step: 14160... Loss: 1.1435... Val Loss: 1.3110\n",
      "Epoch: 11/27... Step: 14170... Loss: 1.2004... Val Loss: 1.3127\n",
      "Epoch: 11/27... Step: 14180... Loss: 1.1455... Val Loss: 1.3134\n",
      "Epoch: 11/27... Step: 14190... Loss: 1.1725... Val Loss: 1.3142\n",
      "Epoch: 11/27... Step: 14200... Loss: 1.1247... Val Loss: 1.3146\n",
      "Epoch: 11/27... Step: 14210... Loss: 1.1100... Val Loss: 1.3189\n",
      "Epoch: 11/27... Step: 14220... Loss: 1.0600... Val Loss: 1.3169\n",
      "Epoch: 11/27... Step: 14230... Loss: 1.1740... Val Loss: 1.3149\n",
      "Epoch: 11/27... Step: 14240... Loss: 1.1549... Val Loss: 1.3135\n",
      "Epoch: 11/27... Step: 14250... Loss: 1.2577... Val Loss: 1.3149\n",
      "Epoch: 11/27... Step: 14260... Loss: 1.1702... Val Loss: 1.3166\n",
      "Epoch: 11/27... Step: 14270... Loss: 1.1525... Val Loss: 1.3136\n",
      "Epoch: 11/27... Step: 14280... Loss: 1.1900... Val Loss: 1.3115\n",
      "Epoch: 11/27... Step: 14290... Loss: 1.1801... Val Loss: 1.3118\n",
      "Epoch: 11/27... Step: 14300... Loss: 1.1805... Val Loss: 1.3139\n",
      "Epoch: 11/27... Step: 14310... Loss: 1.1784... Val Loss: 1.3123\n",
      "Epoch: 11/27... Step: 14320... Loss: 1.2619... Val Loss: 1.3123\n",
      "Epoch: 11/27... Step: 14330... Loss: 1.2099... Val Loss: 1.3086\n",
      "Epoch: 11/27... Step: 14340... Loss: 1.1624... Val Loss: 1.3133\n",
      "Epoch: 11/27... Step: 14350... Loss: 1.1241... Val Loss: 1.3176\n",
      "Epoch: 11/27... Step: 14360... Loss: 1.2129... Val Loss: 1.3180\n",
      "Epoch: 11/27... Step: 14370... Loss: 1.2050... Val Loss: 1.3130\n",
      "Epoch: 11/27... Step: 14380... Loss: 1.1833... Val Loss: 1.3129\n",
      "Epoch: 11/27... Step: 14390... Loss: 1.2178... Val Loss: 1.3137\n",
      "Epoch: 11/27... Step: 14400... Loss: 1.2098... Val Loss: 1.3124\n",
      "Epoch: 11/27... Step: 14410... Loss: 1.2277... Val Loss: 1.3126\n",
      "Epoch: 11/27... Step: 14420... Loss: 1.2425... Val Loss: 1.3136\n",
      "Epoch: 11/27... Step: 14430... Loss: 1.1382... Val Loss: 1.3121\n",
      "Epoch: 11/27... Step: 14440... Loss: 1.1268... Val Loss: 1.3119\n",
      "Epoch: 11/27... Step: 14450... Loss: 1.1928... Val Loss: 1.3118\n",
      "Epoch: 11/27... Step: 14460... Loss: 1.1744... Val Loss: 1.3135\n",
      "Epoch: 11/27... Step: 14470... Loss: 1.2327... Val Loss: 1.3132\n",
      "Epoch: 11/27... Step: 14480... Loss: 1.1120... Val Loss: 1.3115\n",
      "Epoch: 11/27... Step: 14490... Loss: 1.1400... Val Loss: 1.3205\n",
      "Epoch: 11/27... Step: 14500... Loss: 1.1662... Val Loss: 1.3224\n",
      "Epoch: 11/27... Step: 14510... Loss: 1.1818... Val Loss: 1.3215\n",
      "Epoch: 11/27... Step: 14520... Loss: 1.2462... Val Loss: 1.3201\n",
      "Epoch: 11/27... Step: 14530... Loss: 1.1774... Val Loss: 1.3118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/27... Step: 14540... Loss: 1.1691... Val Loss: 1.3129\n",
      "Epoch: 11/27... Step: 14550... Loss: 1.1615... Val Loss: 1.3183\n",
      "Epoch: 11/27... Step: 14560... Loss: 1.1408... Val Loss: 1.3217\n",
      "Epoch: 11/27... Step: 14570... Loss: 1.1867... Val Loss: 1.3202\n",
      "Epoch: 11/27... Step: 14580... Loss: 1.2213... Val Loss: 1.3187\n",
      "Epoch: 11/27... Step: 14590... Loss: 1.1504... Val Loss: 1.3176\n",
      "Epoch: 11/27... Step: 14600... Loss: 1.1664... Val Loss: 1.3168\n",
      "Epoch: 11/27... Step: 14610... Loss: 1.1795... Val Loss: 1.3180\n",
      "Epoch: 11/27... Step: 14620... Loss: 1.1563... Val Loss: 1.3129\n",
      "Epoch: 11/27... Step: 14630... Loss: 1.2488... Val Loss: 1.3136\n",
      "Epoch: 11/27... Step: 14640... Loss: 1.2074... Val Loss: 1.3138\n",
      "Epoch: 11/27... Step: 14650... Loss: 1.2544... Val Loss: 1.3145\n",
      "Epoch: 11/27... Step: 14660... Loss: 1.1409... Val Loss: 1.3151\n",
      "Epoch: 11/27... Step: 14670... Loss: 1.2102... Val Loss: 1.3163\n",
      "Epoch: 11/27... Step: 14680... Loss: 1.1362... Val Loss: 1.3147\n",
      "Epoch: 11/27... Step: 14690... Loss: 1.1785... Val Loss: 1.3178\n",
      "Epoch: 11/27... Step: 14700... Loss: 1.1991... Val Loss: 1.3183\n",
      "Epoch: 11/27... Step: 14710... Loss: 1.1587... Val Loss: 1.3124\n",
      "Epoch: 11/27... Step: 14720... Loss: 1.0958... Val Loss: 1.3161\n",
      "Epoch: 11/27... Step: 14730... Loss: 1.2657... Val Loss: 1.3178\n",
      "Epoch: 11/27... Step: 14740... Loss: 1.2175... Val Loss: 1.3144\n",
      "Epoch: 11/27... Step: 14750... Loss: 1.1993... Val Loss: 1.3133\n",
      "Epoch: 11/27... Step: 14760... Loss: 1.1691... Val Loss: 1.3128\n",
      "Epoch: 11/27... Step: 14770... Loss: 1.1863... Val Loss: 1.3143\n",
      "Epoch: 11/27... Step: 14780... Loss: 1.1907... Val Loss: 1.3138\n",
      "Epoch: 11/27... Step: 14790... Loss: 1.1267... Val Loss: 1.3120\n",
      "Epoch: 11/27... Step: 14800... Loss: 1.1621... Val Loss: 1.3100\n",
      "Epoch: 11/27... Step: 14810... Loss: 1.1273... Val Loss: 1.3091\n",
      "Epoch: 11/27... Step: 14820... Loss: 1.1393... Val Loss: 1.3205\n",
      "Epoch: 11/27... Step: 14830... Loss: 1.2332... Val Loss: 1.3130\n",
      "Epoch: 11/27... Step: 14840... Loss: 1.2138... Val Loss: 1.3113\n",
      "Epoch: 11/27... Step: 14850... Loss: 1.1863... Val Loss: 1.3145\n",
      "Epoch: 11/27... Step: 14860... Loss: 1.1967... Val Loss: 1.3119\n",
      "Epoch: 11/27... Step: 14870... Loss: 1.2373... Val Loss: 1.3102\n",
      "Epoch: 11/27... Step: 14880... Loss: 1.1593... Val Loss: 1.3119\n",
      "Epoch: 11/27... Step: 14890... Loss: 1.1733... Val Loss: 1.3140\n",
      "Epoch: 11/27... Step: 14900... Loss: 1.1685... Val Loss: 1.3093\n",
      "Epoch: 11/27... Step: 14910... Loss: 1.2546... Val Loss: 1.3091\n",
      "Epoch: 11/27... Step: 14920... Loss: 1.1196... Val Loss: 1.3061\n",
      "Epoch: 11/27... Step: 14930... Loss: 1.1572... Val Loss: 1.3071\n",
      "Epoch: 11/27... Step: 14940... Loss: 1.1878... Val Loss: 1.3151\n",
      "Epoch: 11/27... Step: 14950... Loss: 1.1588... Val Loss: 1.3165\n",
      "Epoch: 11/27... Step: 14960... Loss: 1.1883... Val Loss: 1.3105\n",
      "Epoch: 11/27... Step: 14970... Loss: 1.1915... Val Loss: 1.3107\n",
      "Epoch: 11/27... Step: 14980... Loss: 1.1613... Val Loss: 1.3142\n",
      "Epoch: 11/27... Step: 14990... Loss: 1.1402... Val Loss: 1.3139\n",
      "Epoch: 11/27... Step: 15000... Loss: 1.1422... Val Loss: 1.3096\n",
      "Epoch: 11/27... Step: 15010... Loss: 1.1449... Val Loss: 1.3099\n",
      "Epoch: 11/27... Step: 15020... Loss: 1.2195... Val Loss: 1.3119\n",
      "Epoch: 11/27... Step: 15030... Loss: 1.1725... Val Loss: 1.3120\n",
      "Epoch: 11/27... Step: 15040... Loss: 1.0747... Val Loss: 1.3137\n",
      "Epoch: 11/27... Step: 15050... Loss: 1.2134... Val Loss: 1.3175\n",
      "Epoch: 11/27... Step: 15060... Loss: 1.1993... Val Loss: 1.3145\n",
      "Epoch: 11/27... Step: 15070... Loss: 1.1676... Val Loss: 1.3136\n",
      "Epoch: 11/27... Step: 15080... Loss: 1.0995... Val Loss: 1.3151\n",
      "Epoch: 11/27... Step: 15090... Loss: 1.0995... Val Loss: 1.3155\n",
      "Epoch: 11/27... Step: 15100... Loss: 1.1430... Val Loss: 1.3136\n",
      "Epoch: 11/27... Step: 15110... Loss: 1.1689... Val Loss: 1.3137\n",
      "Epoch: 11/27... Step: 15120... Loss: 1.1872... Val Loss: 1.3158\n",
      "Epoch: 11/27... Step: 15130... Loss: 1.2202... Val Loss: 1.3153\n",
      "Epoch: 11/27... Step: 15140... Loss: 1.1354... Val Loss: 1.3175\n",
      "Epoch: 11/27... Step: 15150... Loss: 1.1783... Val Loss: 1.3144\n",
      "Epoch: 11/27... Step: 15160... Loss: 1.1242... Val Loss: 1.3099\n",
      "Epoch: 11/27... Step: 15170... Loss: 1.1712... Val Loss: 1.3168\n",
      "Epoch: 11/27... Step: 15180... Loss: 1.1780... Val Loss: 1.3163\n",
      "Epoch: 11/27... Step: 15190... Loss: 1.1580... Val Loss: 1.3164\n",
      "Epoch: 11/27... Step: 15200... Loss: 1.1371... Val Loss: 1.3120\n",
      "Epoch: 11/27... Step: 15210... Loss: 1.1533... Val Loss: 1.3137\n",
      "Epoch: 11/27... Step: 15220... Loss: 1.1656... Val Loss: 1.3192\n",
      "Epoch: 11/27... Step: 15230... Loss: 1.1473... Val Loss: 1.3172\n",
      "Epoch: 11/27... Step: 15240... Loss: 1.1797... Val Loss: 1.3158\n",
      "Epoch: 11/27... Step: 15250... Loss: 1.1969... Val Loss: 1.3149\n",
      "Epoch: 11/27... Step: 15260... Loss: 1.2191... Val Loss: 1.3144\n",
      "Epoch: 11/27... Step: 15270... Loss: 1.1480... Val Loss: 1.3122\n",
      "Epoch: 11/27... Step: 15280... Loss: 1.1172... Val Loss: 1.3130\n",
      "Epoch: 11/27... Step: 15290... Loss: 1.2279... Val Loss: 1.3141\n",
      "Epoch: 11/27... Step: 15300... Loss: 1.2219... Val Loss: 1.3109\n",
      "Epoch: 11/27... Step: 15310... Loss: 1.1129... Val Loss: 1.3113\n",
      "Epoch: 11/27... Step: 15320... Loss: 1.1857... Val Loss: 1.3147\n",
      "Epoch: 11/27... Step: 15330... Loss: 1.2457... Val Loss: 1.3127\n",
      "Epoch: 11/27... Step: 15340... Loss: 1.2202... Val Loss: 1.3118\n",
      "Epoch: 12/27... Step: 15350... Loss: 1.1908... Val Loss: 1.3113\n",
      "Epoch: 12/27... Step: 15360... Loss: 1.1349... Val Loss: 1.3185\n",
      "Epoch: 12/27... Step: 15370... Loss: 1.2122... Val Loss: 1.3088\n",
      "Epoch: 12/27... Step: 15380... Loss: 1.1298... Val Loss: 1.3077\n",
      "Epoch: 12/27... Step: 15390... Loss: 1.1564... Val Loss: 1.3089\n",
      "Epoch: 12/27... Step: 15400... Loss: 1.1541... Val Loss: 1.3094\n",
      "Epoch: 12/27... Step: 15410... Loss: 1.1895... Val Loss: 1.3104\n",
      "Epoch: 12/27... Step: 15420... Loss: 1.1295... Val Loss: 1.3138\n",
      "Epoch: 12/27... Step: 15430... Loss: 1.1590... Val Loss: 1.3131\n",
      "Epoch: 12/27... Step: 15440... Loss: 1.2102... Val Loss: 1.3118\n",
      "Epoch: 12/27... Step: 15450... Loss: 1.1280... Val Loss: 1.3109\n",
      "Epoch: 12/27... Step: 15460... Loss: 1.1621... Val Loss: 1.3150\n",
      "Epoch: 12/27... Step: 15470... Loss: 1.1432... Val Loss: 1.3128\n",
      "Epoch: 12/27... Step: 15480... Loss: 1.1639... Val Loss: 1.3153\n",
      "Epoch: 12/27... Step: 15490... Loss: 1.1612... Val Loss: 1.3196\n",
      "Epoch: 12/27... Step: 15500... Loss: 1.2350... Val Loss: 1.3144\n",
      "Epoch: 12/27... Step: 15510... Loss: 1.1486... Val Loss: 1.3111\n",
      "Epoch: 12/27... Step: 15520... Loss: 1.1755... Val Loss: 1.3118\n",
      "Epoch: 12/27... Step: 15530... Loss: 1.2086... Val Loss: 1.3148\n",
      "Epoch: 12/27... Step: 15540... Loss: 1.1729... Val Loss: 1.3108\n",
      "Epoch: 12/27... Step: 15550... Loss: 1.1370... Val Loss: 1.3093\n",
      "Epoch: 12/27... Step: 15560... Loss: 1.1575... Val Loss: 1.3134\n",
      "Epoch: 12/27... Step: 15570... Loss: 1.1621... Val Loss: 1.3107\n",
      "Epoch: 12/27... Step: 15580... Loss: 1.1150... Val Loss: 1.3131\n",
      "Epoch: 12/27... Step: 15590... Loss: 1.1497... Val Loss: 1.3131\n",
      "Epoch: 12/27... Step: 15600... Loss: 1.1545... Val Loss: 1.3113\n",
      "Epoch: 12/27... Step: 15610... Loss: 1.1594... Val Loss: 1.3106\n",
      "Epoch: 12/27... Step: 15620... Loss: 1.1840... Val Loss: 1.3117\n",
      "Epoch: 12/27... Step: 15630... Loss: 1.1605... Val Loss: 1.3131\n",
      "Epoch: 12/27... Step: 15640... Loss: 1.1925... Val Loss: 1.3153\n",
      "Epoch: 12/27... Step: 15650... Loss: 1.1335... Val Loss: 1.3221\n",
      "Epoch: 12/27... Step: 15660... Loss: 1.1645... Val Loss: 1.3173\n",
      "Epoch: 12/27... Step: 15670... Loss: 1.1419... Val Loss: 1.3182\n",
      "Epoch: 12/27... Step: 15680... Loss: 1.1479... Val Loss: 1.3179\n",
      "Epoch: 12/27... Step: 15690... Loss: 1.2096... Val Loss: 1.3151\n",
      "Epoch: 12/27... Step: 15700... Loss: 1.1788... Val Loss: 1.3137\n",
      "Epoch: 12/27... Step: 15710... Loss: 1.2070... Val Loss: 1.3102\n",
      "Epoch: 12/27... Step: 15720... Loss: 1.2182... Val Loss: 1.3089\n",
      "Epoch: 12/27... Step: 15730... Loss: 1.1686... Val Loss: 1.3134\n",
      "Epoch: 12/27... Step: 15740... Loss: 1.1693... Val Loss: 1.3171\n",
      "Epoch: 12/27... Step: 15750... Loss: 1.2001... Val Loss: 1.3142\n",
      "Epoch: 12/27... Step: 15760... Loss: 1.1344... Val Loss: 1.3130\n",
      "Epoch: 12/27... Step: 15770... Loss: 1.1158... Val Loss: 1.3139\n",
      "Epoch: 12/27... Step: 15780... Loss: 1.1723... Val Loss: 1.3157\n",
      "Epoch: 12/27... Step: 15790... Loss: 1.0952... Val Loss: 1.3111\n",
      "Epoch: 12/27... Step: 15800... Loss: 1.1649... Val Loss: 1.3075\n",
      "Epoch: 12/27... Step: 15810... Loss: 1.1876... Val Loss: 1.3057\n",
      "Epoch: 12/27... Step: 15820... Loss: 1.1235... Val Loss: 1.3097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/27... Step: 15830... Loss: 1.1450... Val Loss: 1.3075\n",
      "Epoch: 12/27... Step: 15840... Loss: 1.2123... Val Loss: 1.3114\n",
      "Epoch: 12/27... Step: 15850... Loss: 1.1273... Val Loss: 1.3099\n",
      "Epoch: 12/27... Step: 15860... Loss: 1.1556... Val Loss: 1.3100\n",
      "Epoch: 12/27... Step: 15870... Loss: 1.1718... Val Loss: 1.3078\n",
      "Epoch: 12/27... Step: 15880... Loss: 1.1391... Val Loss: 1.3120\n",
      "Epoch: 12/27... Step: 15890... Loss: 1.0849... Val Loss: 1.3172\n",
      "Epoch: 12/27... Step: 15900... Loss: 1.1443... Val Loss: 1.3155\n",
      "Epoch: 12/27... Step: 15910... Loss: 1.0690... Val Loss: 1.3156\n",
      "Epoch: 12/27... Step: 15920... Loss: 1.1238... Val Loss: 1.3152\n",
      "Epoch: 12/27... Step: 15930... Loss: 1.1499... Val Loss: 1.3133\n",
      "Epoch: 12/27... Step: 15940... Loss: 1.1123... Val Loss: 1.3166\n",
      "Epoch: 12/27... Step: 15950... Loss: 1.1675... Val Loss: 1.3190\n",
      "Epoch: 12/27... Step: 15960... Loss: 1.1277... Val Loss: 1.3137\n",
      "Epoch: 12/27... Step: 15970... Loss: 1.1826... Val Loss: 1.3139\n",
      "Epoch: 12/27... Step: 15980... Loss: 1.1483... Val Loss: 1.3172\n",
      "Epoch: 12/27... Step: 15990... Loss: 1.2213... Val Loss: 1.3155\n",
      "Epoch: 12/27... Step: 16000... Loss: 1.1609... Val Loss: 1.3154\n",
      "Epoch: 12/27... Step: 16010... Loss: 1.1716... Val Loss: 1.3120\n",
      "Epoch: 12/27... Step: 16020... Loss: 1.1994... Val Loss: 1.3117\n",
      "Epoch: 12/27... Step: 16030... Loss: 1.1691... Val Loss: 1.3141\n",
      "Epoch: 12/27... Step: 16040... Loss: 1.1577... Val Loss: 1.3117\n",
      "Epoch: 12/27... Step: 16050... Loss: 1.1671... Val Loss: 1.3092\n",
      "Epoch: 12/27... Step: 16060... Loss: 1.1789... Val Loss: 1.3101\n",
      "Epoch: 12/27... Step: 16070... Loss: 1.1404... Val Loss: 1.3106\n",
      "Epoch: 12/27... Step: 16080... Loss: 1.1664... Val Loss: 1.3139\n",
      "Epoch: 12/27... Step: 16090... Loss: 1.2139... Val Loss: 1.3146\n",
      "Epoch: 12/27... Step: 16100... Loss: 1.1132... Val Loss: 1.3138\n",
      "Epoch: 12/27... Step: 16110... Loss: 1.1715... Val Loss: 1.3117\n",
      "Epoch: 12/27... Step: 16120... Loss: 1.1471... Val Loss: 1.3130\n",
      "Epoch: 12/27... Step: 16130... Loss: 1.1821... Val Loss: 1.3088\n",
      "Epoch: 12/27... Step: 16140... Loss: 1.1612... Val Loss: 1.3084\n",
      "Epoch: 12/27... Step: 16150... Loss: 1.1549... Val Loss: 1.3114\n",
      "Epoch: 12/27... Step: 16160... Loss: 1.1576... Val Loss: 1.3063\n",
      "Epoch: 12/27... Step: 16170... Loss: 1.1449... Val Loss: 1.3136\n",
      "Epoch: 12/27... Step: 16180... Loss: 1.1425... Val Loss: 1.3177\n",
      "Epoch: 12/27... Step: 16190... Loss: 1.2324... Val Loss: 1.3062\n",
      "Epoch: 12/27... Step: 16200... Loss: 1.1461... Val Loss: 1.3058\n",
      "Epoch: 12/27... Step: 16210... Loss: 1.1777... Val Loss: 1.3170\n",
      "Epoch: 12/27... Step: 16220... Loss: 1.1557... Val Loss: 1.3168\n",
      "Epoch: 12/27... Step: 16230... Loss: 1.2300... Val Loss: 1.3082\n",
      "Epoch: 12/27... Step: 16240... Loss: 1.1403... Val Loss: 1.3095\n",
      "Epoch: 12/27... Step: 16250... Loss: 1.1794... Val Loss: 1.3123\n",
      "Epoch: 12/27... Step: 16260... Loss: 1.1424... Val Loss: 1.3130\n",
      "Epoch: 12/27... Step: 16270... Loss: 1.1746... Val Loss: 1.3111\n",
      "Epoch: 12/27... Step: 16280... Loss: 1.1448... Val Loss: 1.3119\n",
      "Epoch: 12/27... Step: 16290... Loss: 1.2364... Val Loss: 1.3090\n",
      "Epoch: 12/27... Step: 16300... Loss: 1.1453... Val Loss: 1.3089\n",
      "Epoch: 12/27... Step: 16310... Loss: 1.1963... Val Loss: 1.3112\n",
      "Epoch: 12/27... Step: 16320... Loss: 1.1870... Val Loss: 1.3127\n",
      "Epoch: 12/27... Step: 16330... Loss: 1.2056... Val Loss: 1.3160\n",
      "Epoch: 12/27... Step: 16340... Loss: 1.1808... Val Loss: 1.3139\n",
      "Epoch: 12/27... Step: 16350... Loss: 1.1534... Val Loss: 1.3154\n",
      "Epoch: 12/27... Step: 16360... Loss: 1.1373... Val Loss: 1.3151\n",
      "Epoch: 12/27... Step: 16370... Loss: 1.1624... Val Loss: 1.3110\n",
      "Epoch: 12/27... Step: 16380... Loss: 1.1977... Val Loss: 1.3158\n",
      "Epoch: 12/27... Step: 16390... Loss: 1.2036... Val Loss: 1.3137\n",
      "Epoch: 12/27... Step: 16400... Loss: 1.1344... Val Loss: 1.3101\n",
      "Epoch: 12/27... Step: 16410... Loss: 1.1818... Val Loss: 1.3149\n",
      "Epoch: 12/27... Step: 16420... Loss: 1.1402... Val Loss: 1.3169\n",
      "Epoch: 12/27... Step: 16430... Loss: 1.1798... Val Loss: 1.3130\n",
      "Epoch: 12/27... Step: 16440... Loss: 1.1170... Val Loss: 1.3143\n",
      "Epoch: 12/27... Step: 16450... Loss: 1.0916... Val Loss: 1.3150\n",
      "Epoch: 12/27... Step: 16460... Loss: 1.1445... Val Loss: 1.3122\n",
      "Epoch: 12/27... Step: 16470... Loss: 1.1411... Val Loss: 1.3122\n",
      "Epoch: 12/27... Step: 16480... Loss: 1.1172... Val Loss: 1.3170\n",
      "Epoch: 12/27... Step: 16490... Loss: 1.1493... Val Loss: 1.3142\n",
      "Epoch: 12/27... Step: 16500... Loss: 1.1470... Val Loss: 1.3137\n",
      "Epoch: 12/27... Step: 16510... Loss: 1.1375... Val Loss: 1.3140\n",
      "Epoch: 12/27... Step: 16520... Loss: 1.1070... Val Loss: 1.3098\n",
      "Epoch: 12/27... Step: 16530... Loss: 1.1700... Val Loss: 1.3164\n",
      "Epoch: 12/27... Step: 16540... Loss: 1.1454... Val Loss: 1.3142\n",
      "Epoch: 12/27... Step: 16550... Loss: 1.1100... Val Loss: 1.3103\n",
      "Epoch: 12/27... Step: 16560... Loss: 1.0811... Val Loss: 1.3121\n",
      "Epoch: 12/27... Step: 16570... Loss: 1.2272... Val Loss: 1.3142\n",
      "Epoch: 12/27... Step: 16580... Loss: 1.1967... Val Loss: 1.3119\n",
      "Epoch: 12/27... Step: 16590... Loss: 1.1943... Val Loss: 1.3139\n",
      "Epoch: 12/27... Step: 16600... Loss: 1.1981... Val Loss: 1.3094\n",
      "Epoch: 12/27... Step: 16610... Loss: 1.1758... Val Loss: 1.3087\n",
      "Epoch: 12/27... Step: 16620... Loss: 1.2367... Val Loss: 1.3080\n",
      "Epoch: 12/27... Step: 16630... Loss: 1.1350... Val Loss: 1.3158\n",
      "Epoch: 12/27... Step: 16640... Loss: 1.1235... Val Loss: 1.3127\n",
      "Epoch: 12/27... Step: 16650... Loss: 1.1775... Val Loss: 1.3093\n",
      "Epoch: 12/27... Step: 16660... Loss: 1.2250... Val Loss: 1.3088\n",
      "Epoch: 12/27... Step: 16670... Loss: 1.0620... Val Loss: 1.3119\n",
      "Epoch: 12/27... Step: 16680... Loss: 1.2360... Val Loss: 1.3116\n",
      "Epoch: 12/27... Step: 16690... Loss: 1.1382... Val Loss: 1.3064\n",
      "Epoch: 12/27... Step: 16700... Loss: 1.1059... Val Loss: 1.3058\n",
      "Epoch: 12/27... Step: 16710... Loss: 1.1068... Val Loss: 1.3081\n",
      "Epoch: 12/27... Step: 16720... Loss: 1.1168... Val Loss: 1.3095\n",
      "Epoch: 12/27... Step: 16730... Loss: 1.1398... Val Loss: 1.3048\n",
      "Epoch: 12/27... Step: 16740... Loss: 1.8028... Val Loss: 1.3104\n",
      "Epoch: 13/27... Step: 16750... Loss: 1.1881... Val Loss: 1.3142\n",
      "Epoch: 13/27... Step: 16760... Loss: 1.1341... Val Loss: 1.3114\n",
      "Epoch: 13/27... Step: 16770... Loss: 1.1639... Val Loss: 1.3064\n",
      "Epoch: 13/27... Step: 16780... Loss: 1.1440... Val Loss: 1.3126\n",
      "Epoch: 13/27... Step: 16790... Loss: 1.1823... Val Loss: 1.3129\n",
      "Epoch: 13/27... Step: 16800... Loss: 1.1764... Val Loss: 1.3079\n",
      "Epoch: 13/27... Step: 16810... Loss: 1.1301... Val Loss: 1.3130\n",
      "Epoch: 13/27... Step: 16820... Loss: 1.1102... Val Loss: 1.3153\n",
      "Epoch: 13/27... Step: 16830... Loss: 1.1316... Val Loss: 1.3131\n",
      "Epoch: 13/27... Step: 16840... Loss: 1.1818... Val Loss: 1.3091\n",
      "Epoch: 13/27... Step: 16850... Loss: 1.2055... Val Loss: 1.3111\n",
      "Epoch: 13/27... Step: 16860... Loss: 1.2018... Val Loss: 1.3087\n",
      "Epoch: 13/27... Step: 16870... Loss: 1.1963... Val Loss: 1.3060\n",
      "Epoch: 13/27... Step: 16880... Loss: 1.1355... Val Loss: 1.3138\n",
      "Epoch: 13/27... Step: 16890... Loss: 1.1724... Val Loss: 1.3185\n",
      "Epoch: 13/27... Step: 16900... Loss: 1.0721... Val Loss: 1.3106\n",
      "Epoch: 13/27... Step: 16910... Loss: 1.1710... Val Loss: 1.3094\n",
      "Epoch: 13/27... Step: 16920... Loss: 1.1716... Val Loss: 1.3118\n",
      "Epoch: 13/27... Step: 16930... Loss: 1.1559... Val Loss: 1.3116\n",
      "Epoch: 13/27... Step: 16940... Loss: 1.1164... Val Loss: 1.3102\n",
      "Epoch: 13/27... Step: 16950... Loss: 1.1345... Val Loss: 1.3084\n",
      "Epoch: 13/27... Step: 16960... Loss: 1.1813... Val Loss: 1.3128\n",
      "Epoch: 13/27... Step: 16970... Loss: 1.1325... Val Loss: 1.3122\n",
      "Epoch: 13/27... Step: 16980... Loss: 1.1272... Val Loss: 1.3104\n",
      "Epoch: 13/27... Step: 16990... Loss: 1.1263... Val Loss: 1.3097\n",
      "Epoch: 13/27... Step: 17000... Loss: 1.1017... Val Loss: 1.3165\n",
      "Epoch: 13/27... Step: 17010... Loss: 1.0324... Val Loss: 1.3129\n",
      "Epoch: 13/27... Step: 17020... Loss: 1.1664... Val Loss: 1.3097\n",
      "Epoch: 13/27... Step: 17030... Loss: 1.1581... Val Loss: 1.3086\n",
      "Epoch: 13/27... Step: 17040... Loss: 1.2217... Val Loss: 1.3154\n",
      "Epoch: 13/27... Step: 17050... Loss: 1.1643... Val Loss: 1.3193\n",
      "Epoch: 13/27... Step: 17060... Loss: 1.1311... Val Loss: 1.3135\n",
      "Epoch: 13/27... Step: 17070... Loss: 1.1654... Val Loss: 1.3106\n",
      "Epoch: 13/27... Step: 17080... Loss: 1.1755... Val Loss: 1.3110\n",
      "Epoch: 13/27... Step: 17090... Loss: 1.1949... Val Loss: 1.3113\n",
      "Epoch: 13/27... Step: 17100... Loss: 1.1722... Val Loss: 1.3070\n",
      "Epoch: 13/27... Step: 17110... Loss: 1.2294... Val Loss: 1.3055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/27... Step: 17120... Loss: 1.2040... Val Loss: 1.3061\n",
      "Epoch: 13/27... Step: 17130... Loss: 1.1483... Val Loss: 1.3127\n",
      "Epoch: 13/27... Step: 17140... Loss: 1.1191... Val Loss: 1.3146\n",
      "Epoch: 13/27... Step: 17150... Loss: 1.1539... Val Loss: 1.3117\n",
      "Epoch: 13/27... Step: 17160... Loss: 1.1884... Val Loss: 1.3089\n",
      "Epoch: 13/27... Step: 17170... Loss: 1.1619... Val Loss: 1.3106\n",
      "Epoch: 13/27... Step: 17180... Loss: 1.2109... Val Loss: 1.3130\n",
      "Epoch: 13/27... Step: 17190... Loss: 1.1496... Val Loss: 1.3083\n",
      "Epoch: 13/27... Step: 17200... Loss: 1.2076... Val Loss: 1.3099\n",
      "Epoch: 13/27... Step: 17210... Loss: 1.2189... Val Loss: 1.3121\n",
      "Epoch: 13/27... Step: 17220... Loss: 1.1445... Val Loss: 1.3109\n",
      "Epoch: 13/27... Step: 17230... Loss: 1.1144... Val Loss: 1.3124\n",
      "Epoch: 13/27... Step: 17240... Loss: 1.1654... Val Loss: 1.3144\n",
      "Epoch: 13/27... Step: 17250... Loss: 1.1526... Val Loss: 1.3076\n",
      "Epoch: 13/27... Step: 17260... Loss: 1.2257... Val Loss: 1.3076\n",
      "Epoch: 13/27... Step: 17270... Loss: 1.1205... Val Loss: 1.3071\n",
      "Epoch: 13/27... Step: 17280... Loss: 1.1438... Val Loss: 1.3122\n",
      "Epoch: 13/27... Step: 17290... Loss: 1.1456... Val Loss: 1.3138\n",
      "Epoch: 13/27... Step: 17300... Loss: 1.1688... Val Loss: 1.3109\n",
      "Epoch: 13/27... Step: 17310... Loss: 1.2274... Val Loss: 1.3100\n",
      "Epoch: 13/27... Step: 17320... Loss: 1.1391... Val Loss: 1.3114\n",
      "Epoch: 13/27... Step: 17330... Loss: 1.1385... Val Loss: 1.3119\n",
      "Epoch: 13/27... Step: 17340... Loss: 1.1356... Val Loss: 1.3110\n",
      "Epoch: 13/27... Step: 17350... Loss: 1.1284... Val Loss: 1.3137\n",
      "Epoch: 13/27... Step: 17360... Loss: 1.1627... Val Loss: 1.3133\n",
      "Epoch: 13/27... Step: 17370... Loss: 1.2019... Val Loss: 1.3127\n",
      "Epoch: 13/27... Step: 17380... Loss: 1.1527... Val Loss: 1.3137\n",
      "Epoch: 13/27... Step: 17390... Loss: 1.1418... Val Loss: 1.3179\n",
      "Epoch: 13/27... Step: 17400... Loss: 1.1502... Val Loss: 1.3157\n",
      "Epoch: 13/27... Step: 17410... Loss: 1.1440... Val Loss: 1.3113\n",
      "Epoch: 13/27... Step: 17420... Loss: 1.2106... Val Loss: 1.3125\n",
      "Epoch: 13/27... Step: 17430... Loss: 1.1807... Val Loss: 1.3138\n",
      "Epoch: 13/27... Step: 17440... Loss: 1.2273... Val Loss: 1.3126\n",
      "Epoch: 13/27... Step: 17450... Loss: 1.1448... Val Loss: 1.3118\n",
      "Epoch: 13/27... Step: 17460... Loss: 1.1892... Val Loss: 1.3106\n",
      "Epoch: 13/27... Step: 17470... Loss: 1.1220... Val Loss: 1.3147\n",
      "Epoch: 13/27... Step: 17480... Loss: 1.1387... Val Loss: 1.3183\n",
      "Epoch: 13/27... Step: 17490... Loss: 1.1896... Val Loss: 1.3122\n",
      "Epoch: 13/27... Step: 17500... Loss: 1.1416... Val Loss: 1.3100\n",
      "Epoch: 13/27... Step: 17510... Loss: 1.0778... Val Loss: 1.3166\n",
      "Epoch: 13/27... Step: 17520... Loss: 1.2493... Val Loss: 1.3165\n",
      "Epoch: 13/27... Step: 17530... Loss: 1.1809... Val Loss: 1.3146\n",
      "Epoch: 13/27... Step: 17540... Loss: 1.1551... Val Loss: 1.3098\n",
      "Epoch: 13/27... Step: 17550... Loss: 1.1663... Val Loss: 1.3096\n",
      "Epoch: 13/27... Step: 17560... Loss: 1.1526... Val Loss: 1.3109\n",
      "Epoch: 13/27... Step: 17570... Loss: 1.1894... Val Loss: 1.3149\n",
      "Epoch: 13/27... Step: 17580... Loss: 1.1311... Val Loss: 1.3107\n",
      "Epoch: 13/27... Step: 17590... Loss: 1.1650... Val Loss: 1.3082\n",
      "Epoch: 13/27... Step: 17600... Loss: 1.1232... Val Loss: 1.3081\n",
      "Epoch: 13/27... Step: 17610... Loss: 1.1227... Val Loss: 1.3207\n",
      "Epoch: 13/27... Step: 17620... Loss: 1.2351... Val Loss: 1.3107\n",
      "Epoch: 13/27... Step: 17630... Loss: 1.2315... Val Loss: 1.3094\n",
      "Epoch: 13/27... Step: 17640... Loss: 1.1660... Val Loss: 1.3124\n",
      "Epoch: 13/27... Step: 17650... Loss: 1.1606... Val Loss: 1.3150\n",
      "Epoch: 13/27... Step: 17660... Loss: 1.2242... Val Loss: 1.3136\n",
      "Epoch: 13/27... Step: 17670... Loss: 1.1208... Val Loss: 1.3110\n",
      "Epoch: 13/27... Step: 17680... Loss: 1.1533... Val Loss: 1.3117\n",
      "Epoch: 13/27... Step: 17690... Loss: 1.1462... Val Loss: 1.3107\n",
      "Epoch: 13/27... Step: 17700... Loss: 1.2374... Val Loss: 1.3122\n",
      "Epoch: 13/27... Step: 17710... Loss: 1.1101... Val Loss: 1.3097\n",
      "Epoch: 13/27... Step: 17720... Loss: 1.1496... Val Loss: 1.3108\n",
      "Epoch: 13/27... Step: 17730... Loss: 1.1662... Val Loss: 1.3154\n",
      "Epoch: 13/27... Step: 17740... Loss: 1.1510... Val Loss: 1.3166\n",
      "Epoch: 13/27... Step: 17750... Loss: 1.1612... Val Loss: 1.3138\n",
      "Epoch: 13/27... Step: 17760... Loss: 1.1669... Val Loss: 1.3125\n",
      "Epoch: 13/27... Step: 17770... Loss: 1.1600... Val Loss: 1.3129\n",
      "Epoch: 13/27... Step: 17780... Loss: 1.1156... Val Loss: 1.3142\n",
      "Epoch: 13/27... Step: 17790... Loss: 1.1107... Val Loss: 1.3111\n",
      "Epoch: 13/27... Step: 17800... Loss: 1.1393... Val Loss: 1.3116\n",
      "Epoch: 13/27... Step: 17810... Loss: 1.2036... Val Loss: 1.3158\n",
      "Epoch: 13/27... Step: 17820... Loss: 1.1464... Val Loss: 1.3129\n",
      "Epoch: 13/27... Step: 17830... Loss: 1.0512... Val Loss: 1.3131\n",
      "Epoch: 13/27... Step: 17840... Loss: 1.1594... Val Loss: 1.3149\n",
      "Epoch: 13/27... Step: 17850... Loss: 1.1744... Val Loss: 1.3141\n",
      "Epoch: 13/27... Step: 17860... Loss: 1.1256... Val Loss: 1.3125\n",
      "Epoch: 13/27... Step: 17870... Loss: 1.0885... Val Loss: 1.3143\n",
      "Epoch: 13/27... Step: 17880... Loss: 1.1099... Val Loss: 1.3133\n",
      "Epoch: 13/27... Step: 17890... Loss: 1.1405... Val Loss: 1.3146\n",
      "Epoch: 13/27... Step: 17900... Loss: 1.1846... Val Loss: 1.3132\n",
      "Epoch: 13/27... Step: 17910... Loss: 1.2021... Val Loss: 1.3140\n",
      "Epoch: 13/27... Step: 17920... Loss: 1.1923... Val Loss: 1.3153\n",
      "Epoch: 13/27... Step: 17930... Loss: 1.1430... Val Loss: 1.3141\n",
      "Epoch: 13/27... Step: 17940... Loss: 1.1840... Val Loss: 1.3093\n",
      "Epoch: 13/27... Step: 17950... Loss: 1.0896... Val Loss: 1.3102\n",
      "Epoch: 13/27... Step: 17960... Loss: 1.1521... Val Loss: 1.3173\n",
      "Epoch: 13/27... Step: 17970... Loss: 1.1333... Val Loss: 1.3167\n",
      "Epoch: 13/27... Step: 17980... Loss: 1.1263... Val Loss: 1.3120\n",
      "Epoch: 13/27... Step: 17990... Loss: 1.1351... Val Loss: 1.3135\n",
      "Epoch: 13/27... Step: 18000... Loss: 1.1386... Val Loss: 1.3191\n",
      "Epoch: 13/27... Step: 18010... Loss: 1.1422... Val Loss: 1.3144\n",
      "Epoch: 13/27... Step: 18020... Loss: 1.1455... Val Loss: 1.3111\n",
      "Epoch: 13/27... Step: 18030... Loss: 1.1675... Val Loss: 1.3161\n",
      "Epoch: 13/27... Step: 18040... Loss: 1.1942... Val Loss: 1.3161\n",
      "Epoch: 13/27... Step: 18050... Loss: 1.2239... Val Loss: 1.3111\n",
      "Epoch: 13/27... Step: 18060... Loss: 1.1538... Val Loss: 1.3072\n",
      "Epoch: 13/27... Step: 18070... Loss: 1.1305... Val Loss: 1.3152\n",
      "Epoch: 13/27... Step: 18080... Loss: 1.1963... Val Loss: 1.3156\n",
      "Epoch: 13/27... Step: 18090... Loss: 1.2168... Val Loss: 1.3083\n",
      "Epoch: 13/27... Step: 18100... Loss: 1.1230... Val Loss: 1.3069\n",
      "Epoch: 13/27... Step: 18110... Loss: 1.1656... Val Loss: 1.3099\n",
      "Epoch: 13/27... Step: 18120... Loss: 1.1882... Val Loss: 1.3144\n",
      "Epoch: 13/27... Step: 18130... Loss: 1.2036... Val Loss: 1.3088\n",
      "Epoch: 14/27... Step: 18140... Loss: 1.1548... Val Loss: 1.3048\n",
      "Epoch: 14/27... Step: 18150... Loss: 1.1208... Val Loss: 1.3121\n",
      "Epoch: 14/27... Step: 18160... Loss: 1.1497... Val Loss: 1.3121\n",
      "Epoch: 14/27... Step: 18170... Loss: 1.1047... Val Loss: 1.3055\n",
      "Epoch: 14/27... Step: 18180... Loss: 1.1381... Val Loss: 1.3067\n",
      "Epoch: 14/27... Step: 18190... Loss: 1.1467... Val Loss: 1.3101\n",
      "Epoch: 14/27... Step: 18200... Loss: 1.1473... Val Loss: 1.3123\n",
      "Epoch: 14/27... Step: 18210... Loss: 1.1199... Val Loss: 1.3131\n",
      "Epoch: 14/27... Step: 18220... Loss: 1.0892... Val Loss: 1.3132\n",
      "Epoch: 14/27... Step: 18230... Loss: 1.2017... Val Loss: 1.3114\n",
      "Epoch: 14/27... Step: 18240... Loss: 1.1262... Val Loss: 1.3107\n",
      "Epoch: 14/27... Step: 18250... Loss: 1.1459... Val Loss: 1.3145\n",
      "Epoch: 14/27... Step: 18260... Loss: 1.1389... Val Loss: 1.3101\n",
      "Epoch: 14/27... Step: 18270... Loss: 1.1313... Val Loss: 1.3092\n",
      "Epoch: 14/27... Step: 18280... Loss: 1.1552... Val Loss: 1.3152\n",
      "Epoch: 14/27... Step: 18290... Loss: 1.1989... Val Loss: 1.3152\n",
      "Epoch: 14/27... Step: 18300... Loss: 1.1842... Val Loss: 1.3094\n",
      "Epoch: 14/27... Step: 18310... Loss: 1.1384... Val Loss: 1.3075\n",
      "Epoch: 14/27... Step: 18320... Loss: 1.2085... Val Loss: 1.3089\n",
      "Epoch: 14/27... Step: 18330... Loss: 1.1783... Val Loss: 1.3095\n",
      "Epoch: 14/27... Step: 18340... Loss: 1.1264... Val Loss: 1.3090\n",
      "Epoch: 14/27... Step: 18350... Loss: 1.1358... Val Loss: 1.3102\n",
      "Epoch: 14/27... Step: 18360... Loss: 1.1452... Val Loss: 1.3089\n",
      "Epoch: 14/27... Step: 18370... Loss: 1.0945... Val Loss: 1.3093\n",
      "Epoch: 14/27... Step: 18380... Loss: 1.1523... Val Loss: 1.3093\n",
      "Epoch: 14/27... Step: 18390... Loss: 1.1589... Val Loss: 1.3099\n",
      "Epoch: 14/27... Step: 18400... Loss: 1.1289... Val Loss: 1.3132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/27... Step: 18410... Loss: 1.1567... Val Loss: 1.3100\n",
      "Epoch: 14/27... Step: 18420... Loss: 1.1474... Val Loss: 1.3079\n",
      "Epoch: 14/27... Step: 18430... Loss: 1.1977... Val Loss: 1.3094\n",
      "Epoch: 14/27... Step: 18440... Loss: 1.1475... Val Loss: 1.3169\n",
      "Epoch: 14/27... Step: 18450... Loss: 1.1499... Val Loss: 1.3158\n",
      "Epoch: 14/27... Step: 18460... Loss: 1.1384... Val Loss: 1.3098\n",
      "Epoch: 14/27... Step: 18470... Loss: 1.1422... Val Loss: 1.3094\n",
      "Epoch: 14/27... Step: 18480... Loss: 1.1580... Val Loss: 1.3113\n",
      "Epoch: 14/27... Step: 18490... Loss: 1.1798... Val Loss: 1.3113\n",
      "Epoch: 14/27... Step: 18500... Loss: 1.1954... Val Loss: 1.3091\n",
      "Epoch: 14/27... Step: 18510... Loss: 1.1993... Val Loss: 1.3084\n",
      "Epoch: 14/27... Step: 18520... Loss: 1.1453... Val Loss: 1.3098\n",
      "Epoch: 14/27... Step: 18530... Loss: 1.1406... Val Loss: 1.3146\n",
      "Epoch: 14/27... Step: 18540... Loss: 1.1723... Val Loss: 1.3141\n",
      "Epoch: 14/27... Step: 18550... Loss: 1.1435... Val Loss: 1.3126\n",
      "Epoch: 14/27... Step: 18560... Loss: 1.0996... Val Loss: 1.3088\n",
      "Epoch: 14/27... Step: 18570... Loss: 1.2134... Val Loss: 1.3129\n",
      "Epoch: 14/27... Step: 18580... Loss: 1.0920... Val Loss: 1.3113\n",
      "Epoch: 14/27... Step: 18590... Loss: 1.1533... Val Loss: 1.3065\n",
      "Epoch: 14/27... Step: 18600... Loss: 1.1721... Val Loss: 1.3059\n",
      "Epoch: 14/27... Step: 18610... Loss: 1.1133... Val Loss: 1.3086\n",
      "Epoch: 14/27... Step: 18620... Loss: 1.1063... Val Loss: 1.3114\n",
      "Epoch: 14/27... Step: 18630... Loss: 1.1741... Val Loss: 1.3179\n",
      "Epoch: 14/27... Step: 18640... Loss: 1.1283... Val Loss: 1.3102\n",
      "Epoch: 14/27... Step: 18650... Loss: 1.1418... Val Loss: 1.3061\n",
      "Epoch: 14/27... Step: 18660... Loss: 1.1600... Val Loss: 1.3046\n",
      "Epoch: 14/27... Step: 18670... Loss: 1.1091... Val Loss: 1.3084\n",
      "Epoch: 14/27... Step: 18680... Loss: 1.0826... Val Loss: 1.3151\n",
      "Epoch: 14/27... Step: 18690... Loss: 1.1326... Val Loss: 1.3111\n",
      "Epoch: 14/27... Step: 18700... Loss: 1.0710... Val Loss: 1.3078\n",
      "Epoch: 14/27... Step: 18710... Loss: 1.1025... Val Loss: 1.3152\n",
      "Epoch: 14/27... Step: 18720... Loss: 1.1615... Val Loss: 1.3148\n",
      "Epoch: 14/27... Step: 18730... Loss: 1.0967... Val Loss: 1.3114\n",
      "Epoch: 14/27... Step: 18740... Loss: 1.1593... Val Loss: 1.3120\n",
      "Epoch: 14/27... Step: 18750... Loss: 1.1108... Val Loss: 1.3128\n",
      "Epoch: 14/27... Step: 18760... Loss: 1.2073... Val Loss: 1.3170\n",
      "Epoch: 14/27... Step: 18770... Loss: 1.1236... Val Loss: 1.3168\n",
      "Epoch: 14/27... Step: 18780... Loss: 1.2219... Val Loss: 1.3152\n",
      "Epoch: 14/27... Step: 18790... Loss: 1.1682... Val Loss: 1.3127\n",
      "Epoch: 14/27... Step: 18800... Loss: 1.1669... Val Loss: 1.3138\n",
      "Epoch: 14/27... Step: 18810... Loss: 1.1677... Val Loss: 1.3142\n",
      "Epoch: 14/27... Step: 18820... Loss: 1.1353... Val Loss: 1.3119\n",
      "Epoch: 14/27... Step: 18830... Loss: 1.1511... Val Loss: 1.3102\n",
      "Epoch: 14/27... Step: 18840... Loss: 1.1783... Val Loss: 1.3075\n",
      "Epoch: 14/27... Step: 18850... Loss: 1.1548... Val Loss: 1.3082\n",
      "Epoch: 14/27... Step: 18860... Loss: 1.1320... Val Loss: 1.3078\n",
      "Epoch: 14/27... Step: 18870... Loss: 1.1200... Val Loss: 1.3134\n",
      "Epoch: 14/27... Step: 18880... Loss: 1.2132... Val Loss: 1.3139\n",
      "Epoch: 14/27... Step: 18890... Loss: 1.1009... Val Loss: 1.3090\n",
      "Epoch: 14/27... Step: 18900... Loss: 1.1833... Val Loss: 1.3085\n",
      "Epoch: 14/27... Step: 18910... Loss: 1.1420... Val Loss: 1.3144\n",
      "Epoch: 14/27... Step: 18920... Loss: 1.2177... Val Loss: 1.3119\n",
      "Epoch: 14/27... Step: 18930... Loss: 1.1683... Val Loss: 1.3106\n",
      "Epoch: 14/27... Step: 18940... Loss: 1.1497... Val Loss: 1.3110\n",
      "Epoch: 14/27... Step: 18950... Loss: 1.1629... Val Loss: 1.3066\n",
      "Epoch: 14/27... Step: 18960... Loss: 1.1167... Val Loss: 1.3098\n",
      "Epoch: 14/27... Step: 18970... Loss: 1.1066... Val Loss: 1.3147\n",
      "Epoch: 14/27... Step: 18980... Loss: 1.2184... Val Loss: 1.3065\n",
      "Epoch: 14/27... Step: 18990... Loss: 1.1082... Val Loss: 1.3054\n",
      "Epoch: 14/27... Step: 19000... Loss: 1.1462... Val Loss: 1.3088\n",
      "Epoch: 14/27... Step: 19010... Loss: 1.1587... Val Loss: 1.3109\n",
      "Epoch: 14/27... Step: 19020... Loss: 1.1839... Val Loss: 1.3083\n",
      "Epoch: 14/27... Step: 19030... Loss: 1.1011... Val Loss: 1.3056\n",
      "Epoch: 14/27... Step: 19040... Loss: 1.1454... Val Loss: 1.3085\n",
      "Epoch: 14/27... Step: 19050... Loss: 1.1002... Val Loss: 1.3082\n",
      "Epoch: 14/27... Step: 19060... Loss: 1.1570... Val Loss: 1.3098\n",
      "Epoch: 14/27... Step: 19070... Loss: 1.1787... Val Loss: 1.3091\n",
      "Epoch: 14/27... Step: 19080... Loss: 1.1920... Val Loss: 1.3105\n",
      "Epoch: 14/27... Step: 19090... Loss: 1.1471... Val Loss: 1.3079\n",
      "Epoch: 14/27... Step: 19100... Loss: 1.1586... Val Loss: 1.3036\n",
      "Epoch: 14/27... Step: 19110... Loss: 1.1752... Val Loss: 1.3101\n",
      "Epoch: 14/27... Step: 19120... Loss: 1.1897... Val Loss: 1.3111\n",
      "Epoch: 14/27... Step: 19130... Loss: 1.1637... Val Loss: 1.3088\n",
      "Epoch: 14/27... Step: 19140... Loss: 1.1293... Val Loss: 1.3083\n",
      "Epoch: 14/27... Step: 19150... Loss: 1.1372... Val Loss: 1.3093\n",
      "Epoch: 14/27... Step: 19160... Loss: 1.1331... Val Loss: 1.3073\n",
      "Epoch: 14/27... Step: 19170... Loss: 1.1826... Val Loss: 1.3090\n",
      "Epoch: 14/27... Step: 19180... Loss: 1.1860... Val Loss: 1.3110\n",
      "Epoch: 14/27... Step: 19190... Loss: 1.1356... Val Loss: 1.3074\n",
      "Epoch: 14/27... Step: 19200... Loss: 1.1796... Val Loss: 1.3123\n",
      "Epoch: 14/27... Step: 19210... Loss: 1.1349... Val Loss: 1.3161\n",
      "Epoch: 14/27... Step: 19220... Loss: 1.1714... Val Loss: 1.3136\n",
      "Epoch: 14/27... Step: 19230... Loss: 1.0904... Val Loss: 1.3123\n",
      "Epoch: 14/27... Step: 19240... Loss: 1.1048... Val Loss: 1.3125\n",
      "Epoch: 14/27... Step: 19250... Loss: 1.1495... Val Loss: 1.3092\n",
      "Epoch: 14/27... Step: 19260... Loss: 1.1266... Val Loss: 1.3106\n",
      "Epoch: 14/27... Step: 19270... Loss: 1.1076... Val Loss: 1.3112\n",
      "Epoch: 14/27... Step: 19280... Loss: 1.1488... Val Loss: 1.3095\n",
      "Epoch: 14/27... Step: 19290... Loss: 1.0961... Val Loss: 1.3137\n",
      "Epoch: 14/27... Step: 19300... Loss: 1.1190... Val Loss: 1.3149\n",
      "Epoch: 14/27... Step: 19310... Loss: 1.1003... Val Loss: 1.3158\n",
      "Epoch: 14/27... Step: 19320... Loss: 1.1478... Val Loss: 1.3126\n",
      "Epoch: 14/27... Step: 19330... Loss: 1.1436... Val Loss: 1.3074\n",
      "Epoch: 14/27... Step: 19340... Loss: 1.1089... Val Loss: 1.3073\n",
      "Epoch: 14/27... Step: 19350... Loss: 1.0630... Val Loss: 1.3099\n",
      "Epoch: 14/27... Step: 19360... Loss: 1.2101... Val Loss: 1.3134\n",
      "Epoch: 14/27... Step: 19370... Loss: 1.1704... Val Loss: 1.3143\n",
      "Epoch: 14/27... Step: 19380... Loss: 1.1793... Val Loss: 1.3151\n",
      "Epoch: 14/27... Step: 19390... Loss: 1.1842... Val Loss: 1.3130\n",
      "Epoch: 14/27... Step: 19400... Loss: 1.1547... Val Loss: 1.3114\n",
      "Epoch: 14/27... Step: 19410... Loss: 1.2339... Val Loss: 1.3102\n",
      "Epoch: 14/27... Step: 19420... Loss: 1.1330... Val Loss: 1.3094\n",
      "Epoch: 14/27... Step: 19430... Loss: 1.1048... Val Loss: 1.3128\n",
      "Epoch: 14/27... Step: 19440... Loss: 1.1423... Val Loss: 1.3092\n",
      "Epoch: 14/27... Step: 19450... Loss: 1.2180... Val Loss: 1.3053\n",
      "Epoch: 14/27... Step: 19460... Loss: 1.0331... Val Loss: 1.3093\n",
      "Epoch: 14/27... Step: 19470... Loss: 1.1877... Val Loss: 1.3113\n",
      "Epoch: 14/27... Step: 19480... Loss: 1.1065... Val Loss: 1.3052\n",
      "Epoch: 14/27... Step: 19490... Loss: 1.0903... Val Loss: 1.3037\n",
      "Epoch: 14/27... Step: 19500... Loss: 1.0809... Val Loss: 1.3061\n",
      "Epoch: 14/27... Step: 19510... Loss: 1.1146... Val Loss: 1.3065\n",
      "Epoch: 14/27... Step: 19520... Loss: 1.1176... Val Loss: 1.3037\n",
      "Epoch: 14/27... Step: 19530... Loss: 1.7975... Val Loss: 1.3095\n",
      "Epoch: 15/27... Step: 19540... Loss: 1.1592... Val Loss: 1.3092\n",
      "Epoch: 15/27... Step: 19550... Loss: 1.0964... Val Loss: 1.3086\n",
      "Epoch: 15/27... Step: 19560... Loss: 1.1496... Val Loss: 1.3060\n",
      "Epoch: 15/27... Step: 19570... Loss: 1.1631... Val Loss: 1.3066\n",
      "Epoch: 15/27... Step: 19580... Loss: 1.1469... Val Loss: 1.3075\n",
      "Epoch: 15/27... Step: 19590... Loss: 1.1534... Val Loss: 1.3072\n",
      "Epoch: 15/27... Step: 19600... Loss: 1.1203... Val Loss: 1.3083\n",
      "Epoch: 15/27... Step: 19610... Loss: 1.0998... Val Loss: 1.3110\n",
      "Epoch: 15/27... Step: 19620... Loss: 1.1175... Val Loss: 1.3165\n",
      "Epoch: 15/27... Step: 19630... Loss: 1.1461... Val Loss: 1.3115\n",
      "Epoch: 15/27... Step: 19640... Loss: 1.1761... Val Loss: 1.3078\n",
      "Epoch: 15/27... Step: 19650... Loss: 1.1781... Val Loss: 1.3088\n",
      "Epoch: 15/27... Step: 19660... Loss: 1.1796... Val Loss: 1.3058\n",
      "Epoch: 15/27... Step: 19670... Loss: 1.1337... Val Loss: 1.3123\n",
      "Epoch: 15/27... Step: 19680... Loss: 1.1278... Val Loss: 1.3123\n",
      "Epoch: 15/27... Step: 19690... Loss: 1.0874... Val Loss: 1.3073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/27... Step: 19700... Loss: 1.1405... Val Loss: 1.3056\n",
      "Epoch: 15/27... Step: 19710... Loss: 1.1748... Val Loss: 1.3077\n",
      "Epoch: 15/27... Step: 19720... Loss: 1.1202... Val Loss: 1.3081\n",
      "Epoch: 15/27... Step: 19730... Loss: 1.0991... Val Loss: 1.3099\n",
      "Epoch: 15/27... Step: 19740... Loss: 1.1152... Val Loss: 1.3105\n",
      "Epoch: 15/27... Step: 19750... Loss: 1.1923... Val Loss: 1.3112\n",
      "Epoch: 15/27... Step: 19760... Loss: 1.1355... Val Loss: 1.3084\n",
      "Epoch: 15/27... Step: 19770... Loss: 1.1489... Val Loss: 1.3080\n",
      "Epoch: 15/27... Step: 19780... Loss: 1.0867... Val Loss: 1.3088\n",
      "Epoch: 15/27... Step: 19790... Loss: 1.0887... Val Loss: 1.3069\n",
      "Epoch: 15/27... Step: 19800... Loss: 0.9865... Val Loss: 1.3089\n",
      "Epoch: 15/27... Step: 19810... Loss: 1.1466... Val Loss: 1.3097\n",
      "Epoch: 15/27... Step: 19820... Loss: 1.1284... Val Loss: 1.3059\n",
      "Epoch: 15/27... Step: 19830... Loss: 1.2037... Val Loss: 1.3088\n",
      "Epoch: 15/27... Step: 19840... Loss: 1.1167... Val Loss: 1.3161\n",
      "Epoch: 15/27... Step: 19850... Loss: 1.1173... Val Loss: 1.3153\n",
      "Epoch: 15/27... Step: 19860... Loss: 1.1311... Val Loss: 1.3118\n",
      "Epoch: 15/27... Step: 19870... Loss: 1.1440... Val Loss: 1.3105\n",
      "Epoch: 15/27... Step: 19880... Loss: 1.1843... Val Loss: 1.3072\n",
      "Epoch: 15/27... Step: 19890... Loss: 1.1795... Val Loss: 1.3043\n",
      "Epoch: 15/27... Step: 19900... Loss: 1.2185... Val Loss: 1.3061\n",
      "Epoch: 15/27... Step: 19910... Loss: 1.1608... Val Loss: 1.3069\n",
      "Epoch: 15/27... Step: 19920... Loss: 1.1201... Val Loss: 1.3101\n",
      "Epoch: 15/27... Step: 19930... Loss: 1.0770... Val Loss: 1.3120\n",
      "Epoch: 15/27... Step: 19940... Loss: 1.1489... Val Loss: 1.3111\n",
      "Epoch: 15/27... Step: 19950... Loss: 1.1693... Val Loss: 1.3095\n",
      "Epoch: 15/27... Step: 19960... Loss: 1.1604... Val Loss: 1.3085\n",
      "Epoch: 15/27... Step: 19970... Loss: 1.1944... Val Loss: 1.3074\n",
      "Epoch: 15/27... Step: 19980... Loss: 1.1565... Val Loss: 1.3087\n",
      "Epoch: 15/27... Step: 19990... Loss: 1.1729... Val Loss: 1.3082\n",
      "Epoch: 15/27... Step: 20000... Loss: 1.2300... Val Loss: 1.3082\n",
      "Epoch: 15/27... Step: 20010... Loss: 1.1274... Val Loss: 1.3065\n",
      "Epoch: 15/27... Step: 20020... Loss: 1.1025... Val Loss: 1.3106\n",
      "Epoch: 15/27... Step: 20030... Loss: 1.1983... Val Loss: 1.3132\n",
      "Epoch: 15/27... Step: 20040... Loss: 1.1298... Val Loss: 1.3054\n",
      "Epoch: 15/27... Step: 20050... Loss: 1.2057... Val Loss: 1.3057\n",
      "Epoch: 15/27... Step: 20060... Loss: 1.1107... Val Loss: 1.3075\n",
      "Epoch: 15/27... Step: 20070... Loss: 1.1192... Val Loss: 1.3097\n",
      "Epoch: 15/27... Step: 20080... Loss: 1.1123... Val Loss: 1.3151\n",
      "Epoch: 15/27... Step: 20090... Loss: 1.1343... Val Loss: 1.3110\n",
      "Epoch: 15/27... Step: 20100... Loss: 1.2190... Val Loss: 1.3108\n",
      "Epoch: 15/27... Step: 20110... Loss: 1.1484... Val Loss: 1.3107\n",
      "Epoch: 15/27... Step: 20120... Loss: 1.1330... Val Loss: 1.3085\n",
      "Epoch: 15/27... Step: 20130... Loss: 1.1222... Val Loss: 1.3105\n",
      "Epoch: 15/27... Step: 20140... Loss: 1.1309... Val Loss: 1.3114\n",
      "Epoch: 15/27... Step: 20150... Loss: 1.1904... Val Loss: 1.3101\n",
      "Epoch: 15/27... Step: 20160... Loss: 1.1998... Val Loss: 1.3109\n",
      "Epoch: 15/27... Step: 20170... Loss: 1.1413... Val Loss: 1.3087\n",
      "Epoch: 15/27... Step: 20180... Loss: 1.1121... Val Loss: 1.3086\n",
      "Epoch: 15/27... Step: 20190... Loss: 1.1390... Val Loss: 1.3101\n",
      "Epoch: 15/27... Step: 20200... Loss: 1.1307... Val Loss: 1.3078\n",
      "Epoch: 15/27... Step: 20210... Loss: 1.1772... Val Loss: 1.3074\n",
      "Epoch: 15/27... Step: 20220... Loss: 1.1792... Val Loss: 1.3096\n",
      "Epoch: 15/27... Step: 20230... Loss: 1.2173... Val Loss: 1.3076\n",
      "Epoch: 15/27... Step: 20240... Loss: 1.1127... Val Loss: 1.3051\n",
      "Epoch: 15/27... Step: 20250... Loss: 1.1689... Val Loss: 1.3050\n",
      "Epoch: 15/27... Step: 20260... Loss: 1.1090... Val Loss: 1.3095\n",
      "Epoch: 15/27... Step: 20270... Loss: 1.1436... Val Loss: 1.3158\n",
      "Epoch: 15/27... Step: 20280... Loss: 1.1559... Val Loss: 1.3147\n",
      "Epoch: 15/27... Step: 20290... Loss: 1.1536... Val Loss: 1.3069\n",
      "Epoch: 15/27... Step: 20300... Loss: 1.0683... Val Loss: 1.3077\n",
      "Epoch: 15/27... Step: 20310... Loss: 1.2241... Val Loss: 1.3092\n",
      "Epoch: 15/27... Step: 20320... Loss: 1.1990... Val Loss: 1.3088\n",
      "Epoch: 15/27... Step: 20330... Loss: 1.1231... Val Loss: 1.3103\n",
      "Epoch: 15/27... Step: 20340... Loss: 1.1364... Val Loss: 1.3093\n",
      "Epoch: 15/27... Step: 20350... Loss: 1.1644... Val Loss: 1.3106\n",
      "Epoch: 15/27... Step: 20360... Loss: 1.1547... Val Loss: 1.3161\n",
      "Epoch: 15/27... Step: 20370... Loss: 1.1198... Val Loss: 1.3133\n",
      "Epoch: 15/27... Step: 20380... Loss: 1.1585... Val Loss: 1.3063\n",
      "Epoch: 15/27... Step: 20390... Loss: 1.1053... Val Loss: 1.3090\n",
      "Epoch: 15/27... Step: 20400... Loss: 1.1260... Val Loss: 1.3136\n",
      "Epoch: 15/27... Step: 20410... Loss: 1.2239... Val Loss: 1.3148\n",
      "Epoch: 15/27... Step: 20420... Loss: 1.1830... Val Loss: 1.3109\n",
      "Epoch: 15/27... Step: 20430... Loss: 1.1460... Val Loss: 1.3097\n",
      "Epoch: 15/27... Step: 20440... Loss: 1.1756... Val Loss: 1.3076\n",
      "Epoch: 15/27... Step: 20450... Loss: 1.2153... Val Loss: 1.3086\n",
      "Epoch: 15/27... Step: 20460... Loss: 1.1222... Val Loss: 1.3078\n",
      "Epoch: 15/27... Step: 20470... Loss: 1.1279... Val Loss: 1.3071\n",
      "Epoch: 15/27... Step: 20480... Loss: 1.1533... Val Loss: 1.3067\n",
      "Epoch: 15/27... Step: 20490... Loss: 1.2024... Val Loss: 1.3079\n",
      "Epoch: 15/27... Step: 20500... Loss: 1.0830... Val Loss: 1.3079\n",
      "Epoch: 15/27... Step: 20510... Loss: 1.1345... Val Loss: 1.3102\n",
      "Epoch: 15/27... Step: 20520... Loss: 1.1689... Val Loss: 1.3124\n",
      "Epoch: 15/27... Step: 20530... Loss: 1.1473... Val Loss: 1.3140\n",
      "Epoch: 15/27... Step: 20540... Loss: 1.1663... Val Loss: 1.3137\n",
      "Epoch: 15/27... Step: 20550... Loss: 1.1532... Val Loss: 1.3082\n",
      "Epoch: 15/27... Step: 20560... Loss: 1.1450... Val Loss: 1.3046\n",
      "Epoch: 15/27... Step: 20570... Loss: 1.1042... Val Loss: 1.3109\n",
      "Epoch: 15/27... Step: 20580... Loss: 1.0974... Val Loss: 1.3102\n",
      "Epoch: 15/27... Step: 20590... Loss: 1.1424... Val Loss: 1.3106\n",
      "Epoch: 15/27... Step: 20600... Loss: 1.2038... Val Loss: 1.3111\n",
      "Epoch: 15/27... Step: 20610... Loss: 1.1650... Val Loss: 1.3099\n",
      "Epoch: 15/27... Step: 20620... Loss: 1.0463... Val Loss: 1.3098\n",
      "Epoch: 15/27... Step: 20630... Loss: 1.1597... Val Loss: 1.3097\n",
      "Epoch: 15/27... Step: 20640... Loss: 1.1552... Val Loss: 1.3069\n",
      "Epoch: 15/27... Step: 20650... Loss: 1.1241... Val Loss: 1.3077\n",
      "Epoch: 15/27... Step: 20660... Loss: 1.0963... Val Loss: 1.3057\n",
      "Epoch: 15/27... Step: 20670... Loss: 1.0668... Val Loss: 1.3094\n",
      "Epoch: 15/27... Step: 20680... Loss: 1.1188... Val Loss: 1.3090\n",
      "Epoch: 15/27... Step: 20690... Loss: 1.1401... Val Loss: 1.3089\n",
      "Epoch: 15/27... Step: 20700... Loss: 1.1469... Val Loss: 1.3093\n",
      "Epoch: 15/27... Step: 20710... Loss: 1.1813... Val Loss: 1.3084\n",
      "Epoch: 15/27... Step: 20720... Loss: 1.1259... Val Loss: 1.3114\n",
      "Epoch: 15/27... Step: 20730... Loss: 1.1555... Val Loss: 1.3097\n",
      "Epoch: 15/27... Step: 20740... Loss: 1.0860... Val Loss: 1.3068\n",
      "Epoch: 15/27... Step: 20750... Loss: 1.1232... Val Loss: 1.3112\n",
      "Epoch: 15/27... Step: 20760... Loss: 1.1059... Val Loss: 1.3091\n",
      "Epoch: 15/27... Step: 20770... Loss: 1.1324... Val Loss: 1.3082\n",
      "Epoch: 15/27... Step: 20780... Loss: 1.1378... Val Loss: 1.3091\n",
      "Epoch: 15/27... Step: 20790... Loss: 1.1116... Val Loss: 1.3109\n",
      "Epoch: 15/27... Step: 20800... Loss: 1.1532... Val Loss: 1.3083\n",
      "Epoch: 15/27... Step: 20810... Loss: 1.1369... Val Loss: 1.3048\n",
      "Epoch: 15/27... Step: 20820... Loss: 1.1453... Val Loss: 1.3070\n",
      "Epoch: 15/27... Step: 20830... Loss: 1.1805... Val Loss: 1.3087\n",
      "Epoch: 15/27... Step: 20840... Loss: 1.1960... Val Loss: 1.3100\n",
      "Epoch: 15/27... Step: 20850... Loss: 1.1298... Val Loss: 1.3052\n",
      "Epoch: 15/27... Step: 20860... Loss: 1.1006... Val Loss: 1.3046\n",
      "Epoch: 15/27... Step: 20870... Loss: 1.1675... Val Loss: 1.3086\n",
      "Epoch: 15/27... Step: 20880... Loss: 1.1750... Val Loss: 1.3062\n",
      "Epoch: 15/27... Step: 20890... Loss: 1.0944... Val Loss: 1.3028\n",
      "Epoch: 15/27... Step: 20900... Loss: 1.1494... Val Loss: 1.3061\n",
      "Epoch: 15/27... Step: 20910... Loss: 1.1809... Val Loss: 1.3077\n",
      "Epoch: 15/27... Step: 20920... Loss: 1.1949... Val Loss: 1.3111\n",
      "Epoch: 16/27... Step: 20930... Loss: 1.1604... Val Loss: 1.3093\n",
      "Epoch: 16/27... Step: 20940... Loss: 1.1291... Val Loss: 1.3081\n",
      "Epoch: 16/27... Step: 20950... Loss: 1.1396... Val Loss: 1.3097\n",
      "Epoch: 16/27... Step: 20960... Loss: 1.0931... Val Loss: 1.3081\n",
      "Epoch: 16/27... Step: 20970... Loss: 1.1348... Val Loss: 1.3094\n",
      "Epoch: 16/27... Step: 20980... Loss: 1.1371... Val Loss: 1.3077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/27... Step: 20990... Loss: 1.1393... Val Loss: 1.3067\n",
      "Epoch: 16/27... Step: 21000... Loss: 1.1331... Val Loss: 1.3124\n",
      "Epoch: 16/27... Step: 21010... Loss: 1.1155... Val Loss: 1.3180\n",
      "Epoch: 16/27... Step: 21020... Loss: 1.1694... Val Loss: 1.3122\n",
      "Epoch: 16/27... Step: 21030... Loss: 1.1076... Val Loss: 1.3108\n",
      "Epoch: 16/27... Step: 21040... Loss: 1.1651... Val Loss: 1.3106\n",
      "Epoch: 16/27... Step: 21050... Loss: 1.1283... Val Loss: 1.3061\n",
      "Epoch: 16/27... Step: 21060... Loss: 1.1068... Val Loss: 1.3091\n",
      "Epoch: 16/27... Step: 21070... Loss: 1.1492... Val Loss: 1.3139\n",
      "Epoch: 16/27... Step: 21080... Loss: 1.1801... Val Loss: 1.3103\n",
      "Epoch: 16/27... Step: 21090... Loss: 1.1276... Val Loss: 1.3081\n",
      "Epoch: 16/27... Step: 21100... Loss: 1.1231... Val Loss: 1.3092\n",
      "Epoch: 16/27... Step: 21110... Loss: 1.2160... Val Loss: 1.3114\n",
      "Epoch: 16/27... Step: 21120... Loss: 1.1732... Val Loss: 1.3080\n",
      "Epoch: 16/27... Step: 21130... Loss: 1.1211... Val Loss: 1.3077\n",
      "Epoch: 16/27... Step: 21140... Loss: 1.1492... Val Loss: 1.3112\n",
      "Epoch: 16/27... Step: 21150... Loss: 1.1297... Val Loss: 1.3077\n",
      "Epoch: 16/27... Step: 21160... Loss: 1.0871... Val Loss: 1.3073\n",
      "Epoch: 16/27... Step: 21170... Loss: 1.1082... Val Loss: 1.3067\n",
      "Epoch: 16/27... Step: 21180... Loss: 1.1638... Val Loss: 1.3065\n",
      "Epoch: 16/27... Step: 21190... Loss: 1.1266... Val Loss: 1.3100\n",
      "Epoch: 16/27... Step: 21200... Loss: 1.1436... Val Loss: 1.3091\n",
      "Epoch: 16/27... Step: 21210... Loss: 1.1015... Val Loss: 1.3082\n",
      "Epoch: 16/27... Step: 21220... Loss: 1.2054... Val Loss: 1.3073\n",
      "Epoch: 16/27... Step: 21230... Loss: 1.1139... Val Loss: 1.3109\n",
      "Epoch: 16/27... Step: 21240... Loss: 1.1127... Val Loss: 1.3105\n",
      "Epoch: 16/27... Step: 21250... Loss: 1.1009... Val Loss: 1.3099\n",
      "Epoch: 16/27... Step: 21260... Loss: 1.1288... Val Loss: 1.3103\n",
      "Epoch: 16/27... Step: 21270... Loss: 1.1748... Val Loss: 1.3104\n",
      "Epoch: 16/27... Step: 21280... Loss: 1.1265... Val Loss: 1.3060\n",
      "Epoch: 16/27... Step: 21290... Loss: 1.1550... Val Loss: 1.3051\n",
      "Epoch: 16/27... Step: 21300... Loss: 1.1594... Val Loss: 1.3063\n",
      "Epoch: 16/27... Step: 21310... Loss: 1.1295... Val Loss: 1.3102\n",
      "Epoch: 16/27... Step: 21320... Loss: 1.1474... Val Loss: 1.3128\n",
      "Epoch: 16/27... Step: 21330... Loss: 1.1905... Val Loss: 1.3124\n",
      "Epoch: 16/27... Step: 21340... Loss: 1.1129... Val Loss: 1.3110\n",
      "Epoch: 16/27... Step: 21350... Loss: 1.0884... Val Loss: 1.3081\n",
      "Epoch: 16/27... Step: 21360... Loss: 1.1606... Val Loss: 1.3095\n",
      "Epoch: 16/27... Step: 21370... Loss: 1.0850... Val Loss: 1.3086\n",
      "Epoch: 16/27... Step: 21380... Loss: 1.1466... Val Loss: 1.3072\n",
      "Epoch: 16/27... Step: 21390... Loss: 1.1712... Val Loss: 1.3063\n",
      "Epoch: 16/27... Step: 21400... Loss: 1.0990... Val Loss: 1.3069\n",
      "Epoch: 16/27... Step: 21410... Loss: 1.1125... Val Loss: 1.3082\n",
      "Epoch: 16/27... Step: 21420... Loss: 1.1706... Val Loss: 1.3182\n",
      "Epoch: 16/27... Step: 21430... Loss: 1.1105... Val Loss: 1.3098\n",
      "Epoch: 16/27... Step: 21440... Loss: 1.1364... Val Loss: 1.3044\n",
      "Epoch: 16/27... Step: 21450... Loss: 1.1462... Val Loss: 1.3035\n",
      "Epoch: 16/27... Step: 21460... Loss: 1.1008... Val Loss: 1.3104\n",
      "Epoch: 16/27... Step: 21470... Loss: 1.0863... Val Loss: 1.3156\n",
      "Epoch: 16/27... Step: 21480... Loss: 1.0909... Val Loss: 1.3121\n",
      "Epoch: 16/27... Step: 21490... Loss: 1.0567... Val Loss: 1.3080\n",
      "Epoch: 16/27... Step: 21500... Loss: 1.0919... Val Loss: 1.3087\n",
      "Epoch: 16/27... Step: 21510... Loss: 1.1277... Val Loss: 1.3083\n",
      "Epoch: 16/27... Step: 21520... Loss: 1.0883... Val Loss: 1.3109\n",
      "Epoch: 16/27... Step: 21530... Loss: 1.1440... Val Loss: 1.3126\n",
      "Epoch: 16/27... Step: 21540... Loss: 1.1087... Val Loss: 1.3111\n",
      "Epoch: 16/27... Step: 21550... Loss: 1.1796... Val Loss: 1.3111\n",
      "Epoch: 16/27... Step: 21560... Loss: 1.0964... Val Loss: 1.3151\n",
      "Epoch: 16/27... Step: 21570... Loss: 1.1856... Val Loss: 1.3132\n",
      "Epoch: 16/27... Step: 21580... Loss: 1.1433... Val Loss: 1.3102\n",
      "Epoch: 16/27... Step: 21590... Loss: 1.1290... Val Loss: 1.3099\n",
      "Epoch: 16/27... Step: 21600... Loss: 1.1778... Val Loss: 1.3129\n",
      "Epoch: 16/27... Step: 21610... Loss: 1.1319... Val Loss: 1.3088\n",
      "Epoch: 16/27... Step: 21620... Loss: 1.1556... Val Loss: 1.3077\n",
      "Epoch: 16/27... Step: 21630... Loss: 1.1545... Val Loss: 1.3073\n",
      "Epoch: 16/27... Step: 21640... Loss: 1.1483... Val Loss: 1.3099\n",
      "Epoch: 16/27... Step: 21650... Loss: 1.1243... Val Loss: 1.3087\n",
      "Epoch: 16/27... Step: 21660... Loss: 1.1316... Val Loss: 1.3125\n",
      "Epoch: 16/27... Step: 21670... Loss: 1.1917... Val Loss: 1.3111\n",
      "Epoch: 16/27... Step: 21680... Loss: 1.1005... Val Loss: 1.3072\n",
      "Epoch: 16/27... Step: 21690... Loss: 1.1190... Val Loss: 1.3067\n",
      "Epoch: 16/27... Step: 21700... Loss: 1.1170... Val Loss: 1.3129\n",
      "Epoch: 16/27... Step: 21710... Loss: 1.1748... Val Loss: 1.3089\n",
      "Epoch: 16/27... Step: 21720... Loss: 1.1576... Val Loss: 1.3053\n",
      "Epoch: 16/27... Step: 21730... Loss: 1.1337... Val Loss: 1.3053\n",
      "Epoch: 16/27... Step: 21740... Loss: 1.1058... Val Loss: 1.3052\n",
      "Epoch: 16/27... Step: 21750... Loss: 1.0961... Val Loss: 1.3088\n",
      "Epoch: 16/27... Step: 21760... Loss: 1.1193... Val Loss: 1.3120\n",
      "Epoch: 16/27... Step: 21770... Loss: 1.2285... Val Loss: 1.3081\n",
      "Epoch: 16/27... Step: 21780... Loss: 1.1085... Val Loss: 1.3042\n",
      "Epoch: 16/27... Step: 21790... Loss: 1.1319... Val Loss: 1.3062\n",
      "Epoch: 16/27... Step: 21800... Loss: 1.1526... Val Loss: 1.3108\n",
      "Epoch: 16/27... Step: 21810... Loss: 1.2013... Val Loss: 1.3097\n",
      "Epoch: 16/27... Step: 21820... Loss: 1.1144... Val Loss: 1.3082\n",
      "Epoch: 16/27... Step: 21830... Loss: 1.1252... Val Loss: 1.3078\n",
      "Epoch: 16/27... Step: 21840... Loss: 1.1069... Val Loss: 1.3068\n",
      "Epoch: 16/27... Step: 21850... Loss: 1.1233... Val Loss: 1.3053\n",
      "Epoch: 16/27... Step: 21860... Loss: 1.1385... Val Loss: 1.3072\n",
      "Epoch: 16/27... Step: 21870... Loss: 1.1670... Val Loss: 1.3097\n",
      "Epoch: 16/27... Step: 21880... Loss: 1.1160... Val Loss: 1.3095\n",
      "Epoch: 16/27... Step: 21890... Loss: 1.1558... Val Loss: 1.3088\n",
      "Epoch: 16/27... Step: 21900... Loss: 1.1381... Val Loss: 1.3122\n",
      "Epoch: 16/27... Step: 21910... Loss: 1.1722... Val Loss: 1.3125\n",
      "Epoch: 16/27... Step: 21920... Loss: 1.1663... Val Loss: 1.3111\n",
      "Epoch: 16/27... Step: 21930... Loss: 1.1320... Val Loss: 1.3090\n",
      "Epoch: 16/27... Step: 21940... Loss: 1.1253... Val Loss: 1.3080\n",
      "Epoch: 16/27... Step: 21950... Loss: 1.1396... Val Loss: 1.3084\n",
      "Epoch: 16/27... Step: 21960... Loss: 1.1801... Val Loss: 1.3102\n",
      "Epoch: 16/27... Step: 21970... Loss: 1.1798... Val Loss: 1.3149\n",
      "Epoch: 16/27... Step: 21980... Loss: 1.1408... Val Loss: 1.3128\n",
      "Epoch: 16/27... Step: 21990... Loss: 1.1725... Val Loss: 1.3151\n",
      "Epoch: 16/27... Step: 22000... Loss: 1.1315... Val Loss: 1.3159\n",
      "Epoch: 16/27... Step: 22010... Loss: 1.1804... Val Loss: 1.3114\n",
      "Epoch: 16/27... Step: 22020... Loss: 1.0938... Val Loss: 1.3119\n",
      "Epoch: 16/27... Step: 22030... Loss: 1.0631... Val Loss: 1.3141\n",
      "Epoch: 16/27... Step: 22040... Loss: 1.1310... Val Loss: 1.3136\n",
      "Epoch: 16/27... Step: 22050... Loss: 1.0812... Val Loss: 1.3158\n",
      "Epoch: 16/27... Step: 22060... Loss: 1.1146... Val Loss: 1.3154\n",
      "Epoch: 16/27... Step: 22070... Loss: 1.1314... Val Loss: 1.3138\n",
      "Epoch: 16/27... Step: 22080... Loss: 1.1188... Val Loss: 1.3128\n",
      "Epoch: 16/27... Step: 22090... Loss: 1.1109... Val Loss: 1.3128\n",
      "Epoch: 16/27... Step: 22100... Loss: 1.1075... Val Loss: 1.3126\n",
      "Epoch: 16/27... Step: 22110... Loss: 1.1216... Val Loss: 1.3132\n",
      "Epoch: 16/27... Step: 22120... Loss: 1.1244... Val Loss: 1.3121\n",
      "Epoch: 16/27... Step: 22130... Loss: 1.1081... Val Loss: 1.3074\n",
      "Epoch: 16/27... Step: 22140... Loss: 1.0560... Val Loss: 1.3074\n",
      "Epoch: 16/27... Step: 22150... Loss: 1.2015... Val Loss: 1.3123\n",
      "Epoch: 16/27... Step: 22160... Loss: 1.1626... Val Loss: 1.3131\n",
      "Epoch: 16/27... Step: 22170... Loss: 1.1467... Val Loss: 1.3122\n",
      "Epoch: 16/27... Step: 22180... Loss: 1.1808... Val Loss: 1.3106\n",
      "Epoch: 16/27... Step: 22190... Loss: 1.1517... Val Loss: 1.3111\n",
      "Epoch: 16/27... Step: 22200... Loss: 1.2180... Val Loss: 1.3133\n",
      "Epoch: 16/27... Step: 22210... Loss: 1.1286... Val Loss: 1.3129\n",
      "Epoch: 16/27... Step: 22220... Loss: 1.0389... Val Loss: 1.3094\n",
      "Epoch: 16/27... Step: 22230... Loss: 1.1301... Val Loss: 1.3103\n",
      "Epoch: 16/27... Step: 22240... Loss: 1.1837... Val Loss: 1.3132\n",
      "Epoch: 16/27... Step: 22250... Loss: 1.0470... Val Loss: 1.3095\n",
      "Epoch: 16/27... Step: 22260... Loss: 1.1916... Val Loss: 1.3090\n",
      "Epoch: 16/27... Step: 22270... Loss: 1.1065... Val Loss: 1.3091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/27... Step: 22280... Loss: 1.0965... Val Loss: 1.3059\n",
      "Epoch: 16/27... Step: 22290... Loss: 1.0800... Val Loss: 1.3069\n",
      "Epoch: 16/27... Step: 22300... Loss: 1.0965... Val Loss: 1.3095\n",
      "Epoch: 16/27... Step: 22310... Loss: 1.0959... Val Loss: 1.3104\n",
      "Epoch: 16/27... Step: 22320... Loss: 1.7586... Val Loss: 1.3136\n",
      "Epoch: 17/27... Step: 22330... Loss: 1.1378... Val Loss: 1.3127\n",
      "Epoch: 17/27... Step: 22340... Loss: 1.1126... Val Loss: 1.3091\n",
      "Epoch: 17/27... Step: 22350... Loss: 1.1520... Val Loss: 1.3079\n",
      "Epoch: 17/27... Step: 22360... Loss: 1.1504... Val Loss: 1.3077\n",
      "Epoch: 17/27... Step: 22370... Loss: 1.1404... Val Loss: 1.3114\n",
      "Epoch: 17/27... Step: 22380... Loss: 1.1542... Val Loss: 1.3108\n",
      "Epoch: 17/27... Step: 22390... Loss: 1.0952... Val Loss: 1.3098\n",
      "Epoch: 17/27... Step: 22400... Loss: 1.0926... Val Loss: 1.3097\n",
      "Epoch: 17/27... Step: 22410... Loss: 1.1158... Val Loss: 1.3149\n",
      "Epoch: 17/27... Step: 22420... Loss: 1.1830... Val Loss: 1.3167\n",
      "Epoch: 17/27... Step: 22430... Loss: 1.1821... Val Loss: 1.3140\n",
      "Epoch: 17/27... Step: 22440... Loss: 1.1599... Val Loss: 1.3091\n",
      "Epoch: 17/27... Step: 22450... Loss: 1.1888... Val Loss: 1.3084\n",
      "Epoch: 17/27... Step: 22460... Loss: 1.1261... Val Loss: 1.3172\n",
      "Epoch: 17/27... Step: 22470... Loss: 1.1140... Val Loss: 1.3187\n",
      "Epoch: 17/27... Step: 22480... Loss: 1.0361... Val Loss: 1.3130\n",
      "Epoch: 17/27... Step: 22490... Loss: 1.1535... Val Loss: 1.3116\n",
      "Epoch: 17/27... Step: 22500... Loss: 1.1792... Val Loss: 1.3128\n",
      "Epoch: 17/27... Step: 22510... Loss: 1.1138... Val Loss: 1.3128\n",
      "Epoch: 17/27... Step: 22520... Loss: 1.0839... Val Loss: 1.3087\n",
      "Epoch: 17/27... Step: 22530... Loss: 1.1116... Val Loss: 1.3085\n",
      "Epoch: 17/27... Step: 22540... Loss: 1.1690... Val Loss: 1.3113\n",
      "Epoch: 17/27... Step: 22550... Loss: 1.1089... Val Loss: 1.3092\n",
      "Epoch: 17/27... Step: 22560... Loss: 1.1323... Val Loss: 1.3099\n",
      "Epoch: 17/27... Step: 22570... Loss: 1.0859... Val Loss: 1.3113\n",
      "Epoch: 17/27... Step: 22580... Loss: 1.0584... Val Loss: 1.3125\n",
      "Epoch: 17/27... Step: 22590... Loss: 0.9934... Val Loss: 1.3094\n",
      "Epoch: 17/27... Step: 22600... Loss: 1.1301... Val Loss: 1.3081\n",
      "Epoch: 17/27... Step: 22610... Loss: 1.1098... Val Loss: 1.3096\n",
      "Epoch: 17/27... Step: 22620... Loss: 1.2035... Val Loss: 1.3149\n",
      "Epoch: 17/27... Step: 22630... Loss: 1.1327... Val Loss: 1.3179\n",
      "Epoch: 17/27... Step: 22640... Loss: 1.1179... Val Loss: 1.3132\n",
      "Epoch: 17/27... Step: 22650... Loss: 1.1287... Val Loss: 1.3099\n",
      "Epoch: 17/27... Step: 22660... Loss: 1.1332... Val Loss: 1.3094\n",
      "Epoch: 17/27... Step: 22670... Loss: 1.1461... Val Loss: 1.3118\n",
      "Epoch: 17/27... Step: 22680... Loss: 1.1557... Val Loss: 1.3089\n",
      "Epoch: 17/27... Step: 22690... Loss: 1.1972... Val Loss: 1.3059\n",
      "Epoch: 17/27... Step: 22700... Loss: 1.1902... Val Loss: 1.3045\n",
      "Epoch: 17/27... Step: 22710... Loss: 1.1261... Val Loss: 1.3081\n",
      "Epoch: 17/27... Step: 22720... Loss: 1.0640... Val Loss: 1.3130\n",
      "Epoch: 17/27... Step: 22730... Loss: 1.1457... Val Loss: 1.3131\n",
      "Epoch: 17/27... Step: 22740... Loss: 1.1659... Val Loss: 1.3079\n",
      "Epoch: 17/27... Step: 22750... Loss: 1.1296... Val Loss: 1.3076\n",
      "Epoch: 17/27... Step: 22760... Loss: 1.1780... Val Loss: 1.3123\n",
      "Epoch: 17/27... Step: 22770... Loss: 1.1270... Val Loss: 1.3097\n",
      "Epoch: 17/27... Step: 22780... Loss: 1.1898... Val Loss: 1.3095\n",
      "Epoch: 17/27... Step: 22790... Loss: 1.1950... Val Loss: 1.3092\n",
      "Epoch: 17/27... Step: 22800... Loss: 1.0857... Val Loss: 1.3100\n",
      "Epoch: 17/27... Step: 22810... Loss: 1.0876... Val Loss: 1.3161\n",
      "Epoch: 17/27... Step: 22820... Loss: 1.1734... Val Loss: 1.3164\n",
      "Epoch: 17/27... Step: 22830... Loss: 1.0958... Val Loss: 1.3107\n",
      "Epoch: 17/27... Step: 22840... Loss: 1.1943... Val Loss: 1.3096\n",
      "Epoch: 17/27... Step: 22850... Loss: 1.0890... Val Loss: 1.3072\n",
      "Epoch: 17/27... Step: 22860... Loss: 1.0893... Val Loss: 1.3098\n",
      "Epoch: 17/27... Step: 22870... Loss: 1.1082... Val Loss: 1.3148\n",
      "Epoch: 17/27... Step: 22880... Loss: 1.1544... Val Loss: 1.3141\n",
      "Epoch: 17/27... Step: 22890... Loss: 1.2113... Val Loss: 1.3124\n",
      "Epoch: 17/27... Step: 22900... Loss: 1.1275... Val Loss: 1.3105\n",
      "Epoch: 17/27... Step: 22910... Loss: 1.1339... Val Loss: 1.3069\n",
      "Epoch: 17/27... Step: 22920... Loss: 1.1222... Val Loss: 1.3091\n",
      "Epoch: 17/27... Step: 22930... Loss: 1.1194... Val Loss: 1.3104\n",
      "Epoch: 17/27... Step: 22940... Loss: 1.1736... Val Loss: 1.3136\n",
      "Epoch: 17/27... Step: 22950... Loss: 1.1638... Val Loss: 1.3141\n",
      "Epoch: 17/27... Step: 22960... Loss: 1.1284... Val Loss: 1.3108\n",
      "Epoch: 17/27... Step: 22970... Loss: 1.0922... Val Loss: 1.3101\n",
      "Epoch: 17/27... Step: 22980... Loss: 1.1583... Val Loss: 1.3140\n",
      "Epoch: 17/27... Step: 22990... Loss: 1.0868... Val Loss: 1.3108\n",
      "Epoch: 17/27... Step: 23000... Loss: 1.2096... Val Loss: 1.3083\n",
      "Epoch: 17/27... Step: 23010... Loss: 1.1773... Val Loss: 1.3106\n",
      "Epoch: 17/27... Step: 23020... Loss: 1.2252... Val Loss: 1.3086\n",
      "Epoch: 17/27... Step: 23030... Loss: 1.0929... Val Loss: 1.3080\n",
      "Epoch: 17/27... Step: 23040... Loss: 1.1315... Val Loss: 1.3110\n",
      "Epoch: 17/27... Step: 23050... Loss: 1.1110... Val Loss: 1.3117\n",
      "Epoch: 17/27... Step: 23060... Loss: 1.1290... Val Loss: 1.3161\n",
      "Epoch: 17/27... Step: 23070... Loss: 1.1526... Val Loss: 1.3128\n",
      "Epoch: 17/27... Step: 23080... Loss: 1.1291... Val Loss: 1.3086\n",
      "Epoch: 17/27... Step: 23090... Loss: 1.0761... Val Loss: 1.3157\n",
      "Epoch: 17/27... Step: 23100... Loss: 1.2282... Val Loss: 1.3120\n",
      "Epoch: 17/27... Step: 23110... Loss: 1.1661... Val Loss: 1.3069\n",
      "Epoch: 17/27... Step: 23120... Loss: 1.1369... Val Loss: 1.3098\n",
      "Epoch: 17/27... Step: 23130... Loss: 1.1217... Val Loss: 1.3082\n",
      "Epoch: 17/27... Step: 23140... Loss: 1.1617... Val Loss: 1.3090\n",
      "Epoch: 17/27... Step: 23150... Loss: 1.1641... Val Loss: 1.3100\n",
      "Epoch: 17/27... Step: 23160... Loss: 1.0860... Val Loss: 1.3109\n",
      "Epoch: 17/27... Step: 23170... Loss: 1.1320... Val Loss: 1.3037\n",
      "Epoch: 17/27... Step: 23180... Loss: 1.0947... Val Loss: 1.3025\n",
      "Epoch: 17/27... Step: 23190... Loss: 1.1061... Val Loss: 1.3054\n",
      "Epoch: 17/27... Step: 23200... Loss: 1.2007... Val Loss: 1.3082\n",
      "Epoch: 17/27... Step: 23210... Loss: 1.1837... Val Loss: 1.3097\n",
      "Epoch: 17/27... Step: 23220... Loss: 1.1319... Val Loss: 1.3081\n",
      "Epoch: 17/27... Step: 23230... Loss: 1.1457... Val Loss: 1.3063\n",
      "Epoch: 17/27... Step: 23240... Loss: 1.2021... Val Loss: 1.3072\n",
      "Epoch: 17/27... Step: 23250... Loss: 1.1159... Val Loss: 1.3106\n",
      "Epoch: 17/27... Step: 23260... Loss: 1.0958... Val Loss: 1.3084\n",
      "Epoch: 17/27... Step: 23270... Loss: 1.1519... Val Loss: 1.3070\n",
      "Epoch: 17/27... Step: 23280... Loss: 1.1962... Val Loss: 1.3079\n",
      "Epoch: 17/27... Step: 23290... Loss: 1.0639... Val Loss: 1.3093\n",
      "Epoch: 17/27... Step: 23300... Loss: 1.1055... Val Loss: 1.3108\n",
      "Epoch: 17/27... Step: 23310... Loss: 1.1355... Val Loss: 1.3074\n",
      "Epoch: 17/27... Step: 23320... Loss: 1.1285... Val Loss: 1.3109\n",
      "Epoch: 17/27... Step: 23330... Loss: 1.1697... Val Loss: 1.3158\n",
      "Epoch: 17/27... Step: 23340... Loss: 1.1330... Val Loss: 1.3125\n",
      "Epoch: 17/27... Step: 23350... Loss: 1.1157... Val Loss: 1.3113\n",
      "Epoch: 17/27... Step: 23360... Loss: 1.0952... Val Loss: 1.3113\n",
      "Epoch: 17/27... Step: 23370... Loss: 1.0867... Val Loss: 1.3063\n",
      "Epoch: 17/27... Step: 23380... Loss: 1.1270... Val Loss: 1.3049\n",
      "Epoch: 17/27... Step: 23390... Loss: 1.1492... Val Loss: 1.3085\n",
      "Epoch: 17/27... Step: 23400... Loss: 1.1309... Val Loss: 1.3142\n",
      "Epoch: 17/27... Step: 23410... Loss: 1.0240... Val Loss: 1.3115\n",
      "Epoch: 17/27... Step: 23420... Loss: 1.1351... Val Loss: 1.3108\n",
      "Epoch: 17/27... Step: 23430... Loss: 1.1497... Val Loss: 1.3082\n",
      "Epoch: 17/27... Step: 23440... Loss: 1.0960... Val Loss: 1.3088\n",
      "Epoch: 17/27... Step: 23450... Loss: 1.0667... Val Loss: 1.3118\n",
      "Epoch: 17/27... Step: 23460... Loss: 1.0786... Val Loss: 1.3122\n",
      "Epoch: 17/27... Step: 23470... Loss: 1.1113... Val Loss: 1.3102\n",
      "Epoch: 17/27... Step: 23480... Loss: 1.1439... Val Loss: 1.3103\n",
      "Epoch: 17/27... Step: 23490... Loss: 1.1551... Val Loss: 1.3140\n",
      "Epoch: 17/27... Step: 23500... Loss: 1.1732... Val Loss: 1.3131\n",
      "Epoch: 17/27... Step: 23510... Loss: 1.1137... Val Loss: 1.3128\n",
      "Epoch: 17/27... Step: 23520... Loss: 1.1333... Val Loss: 1.3107\n",
      "Epoch: 17/27... Step: 23530... Loss: 1.0734... Val Loss: 1.3077\n",
      "Epoch: 17/27... Step: 23540... Loss: 1.1089... Val Loss: 1.3112\n",
      "Epoch: 17/27... Step: 23550... Loss: 1.1039... Val Loss: 1.3122\n",
      "Epoch: 17/27... Step: 23560... Loss: 1.1134... Val Loss: 1.3113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/27... Step: 23570... Loss: 1.0972... Val Loss: 1.3118\n",
      "Epoch: 17/27... Step: 23580... Loss: 1.1210... Val Loss: 1.3093\n",
      "Epoch: 17/27... Step: 23590... Loss: 1.1405... Val Loss: 1.3083\n",
      "Epoch: 17/27... Step: 23600... Loss: 1.1035... Val Loss: 1.3115\n",
      "Epoch: 17/27... Step: 23610... Loss: 1.1370... Val Loss: 1.3131\n",
      "Epoch: 17/27... Step: 23620... Loss: 1.1595... Val Loss: 1.3104\n",
      "Epoch: 17/27... Step: 23630... Loss: 1.1802... Val Loss: 1.3102\n",
      "Epoch: 17/27... Step: 23640... Loss: 1.1190... Val Loss: 1.3094\n",
      "Epoch: 17/27... Step: 23650... Loss: 1.0811... Val Loss: 1.3153\n",
      "Epoch: 17/27... Step: 23660... Loss: 1.1532... Val Loss: 1.3159\n",
      "Epoch: 17/27... Step: 23670... Loss: 1.1647... Val Loss: 1.3093\n",
      "Epoch: 17/27... Step: 23680... Loss: 1.0929... Val Loss: 1.3080\n",
      "Epoch: 17/27... Step: 23690... Loss: 1.1575... Val Loss: 1.3106\n",
      "Epoch: 17/27... Step: 23700... Loss: 1.1535... Val Loss: 1.3102\n",
      "Epoch: 17/27... Step: 23710... Loss: 1.1559... Val Loss: 1.3091\n",
      "Epoch: 18/27... Step: 23720... Loss: 1.1327... Val Loss: 1.3082\n",
      "Epoch: 18/27... Step: 23730... Loss: 1.0997... Val Loss: 1.3144\n",
      "Epoch: 18/27... Step: 23740... Loss: 1.1330... Val Loss: 1.3115\n",
      "Epoch: 18/27... Step: 23750... Loss: 1.0847... Val Loss: 1.3090\n",
      "Epoch: 18/27... Step: 23760... Loss: 1.1071... Val Loss: 1.3088\n",
      "Epoch: 18/27... Step: 23770... Loss: 1.0986... Val Loss: 1.3086\n",
      "Epoch: 18/27... Step: 23780... Loss: 1.1151... Val Loss: 1.3088\n",
      "Epoch: 18/27... Step: 23790... Loss: 1.0921... Val Loss: 1.3097\n",
      "Epoch: 18/27... Step: 23800... Loss: 1.0876... Val Loss: 1.3107\n",
      "Epoch: 18/27... Step: 23810... Loss: 1.1732... Val Loss: 1.3102\n",
      "Epoch: 18/27... Step: 23820... Loss: 1.0782... Val Loss: 1.3117\n",
      "Epoch: 18/27... Step: 23830... Loss: 1.1100... Val Loss: 1.3149\n",
      "Epoch: 18/27... Step: 23840... Loss: 1.1138... Val Loss: 1.3089\n",
      "Epoch: 18/27... Step: 23850... Loss: 1.1075... Val Loss: 1.3069\n",
      "Epoch: 18/27... Step: 23860... Loss: 1.1418... Val Loss: 1.3097\n",
      "Epoch: 18/27... Step: 23870... Loss: 1.1519... Val Loss: 1.3140\n",
      "Epoch: 18/27... Step: 23880... Loss: 1.1433... Val Loss: 1.3077\n",
      "Epoch: 18/27... Step: 23890... Loss: 1.1348... Val Loss: 1.3065\n",
      "Epoch: 18/27... Step: 23900... Loss: 1.1877... Val Loss: 1.3124\n",
      "Epoch: 18/27... Step: 23910... Loss: 1.1821... Val Loss: 1.3103\n",
      "Epoch: 18/27... Step: 23920... Loss: 1.1115... Val Loss: 1.3059\n",
      "Epoch: 18/27... Step: 23930... Loss: 1.1107... Val Loss: 1.3079\n",
      "Epoch: 18/27... Step: 23940... Loss: 1.1199... Val Loss: 1.3111\n",
      "Epoch: 18/27... Step: 23950... Loss: 1.0327... Val Loss: 1.3080\n",
      "Epoch: 18/27... Step: 23960... Loss: 1.0956... Val Loss: 1.3054\n",
      "Epoch: 18/27... Step: 23970... Loss: 1.1425... Val Loss: 1.3057\n",
      "Epoch: 18/27... Step: 23980... Loss: 1.1321... Val Loss: 1.3117\n",
      "Epoch: 18/27... Step: 23990... Loss: 1.1237... Val Loss: 1.3134\n",
      "Epoch: 18/27... Step: 24000... Loss: 1.1419... Val Loss: 1.3113\n",
      "Epoch: 18/27... Step: 24010... Loss: 1.1657... Val Loss: 1.3091\n",
      "Epoch: 18/27... Step: 24020... Loss: 1.1036... Val Loss: 1.3143\n",
      "Epoch: 18/27... Step: 24030... Loss: 1.1113... Val Loss: 1.3186\n",
      "Epoch: 18/27... Step: 24040... Loss: 1.1020... Val Loss: 1.3163\n",
      "Epoch: 18/27... Step: 24050... Loss: 1.1188... Val Loss: 1.3108\n",
      "Epoch: 18/27... Step: 24060... Loss: 1.1492... Val Loss: 1.3089\n",
      "Epoch: 18/27... Step: 24070... Loss: 1.1141... Val Loss: 1.3081\n",
      "Epoch: 18/27... Step: 24080... Loss: 1.1708... Val Loss: 1.3075\n",
      "Epoch: 18/27... Step: 24090... Loss: 1.1745... Val Loss: 1.3057\n",
      "Epoch: 18/27... Step: 24100... Loss: 1.1201... Val Loss: 1.3079\n",
      "Epoch: 18/27... Step: 24110... Loss: 1.1230... Val Loss: 1.3117\n",
      "Epoch: 18/27... Step: 24120... Loss: 1.1746... Val Loss: 1.3108\n",
      "Epoch: 18/27... Step: 24130... Loss: 1.0998... Val Loss: 1.3108\n",
      "Epoch: 18/27... Step: 24140... Loss: 1.0796... Val Loss: 1.3080\n",
      "Epoch: 18/27... Step: 24150... Loss: 1.1637... Val Loss: 1.3077\n",
      "Epoch: 18/27... Step: 24160... Loss: 1.0997... Val Loss: 1.3084\n",
      "Epoch: 18/27... Step: 24170... Loss: 1.1247... Val Loss: 1.3066\n",
      "Epoch: 18/27... Step: 24180... Loss: 1.1725... Val Loss: 1.3063\n",
      "Epoch: 18/27... Step: 24190... Loss: 1.0732... Val Loss: 1.3089\n",
      "Epoch: 18/27... Step: 24200... Loss: 1.1133... Val Loss: 1.3086\n",
      "Epoch: 18/27... Step: 24210... Loss: 1.1408... Val Loss: 1.3118\n",
      "Epoch: 18/27... Step: 24220... Loss: 1.0913... Val Loss: 1.3094\n",
      "Epoch: 18/27... Step: 24230... Loss: 1.1160... Val Loss: 1.3064\n",
      "Epoch: 18/27... Step: 24240... Loss: 1.1119... Val Loss: 1.3052\n",
      "Epoch: 18/27... Step: 24250... Loss: 1.0986... Val Loss: 1.3076\n",
      "Epoch: 18/27... Step: 24260... Loss: 1.0653... Val Loss: 1.3161\n",
      "Epoch: 18/27... Step: 24270... Loss: 1.0857... Val Loss: 1.3144\n",
      "Epoch: 18/27... Step: 24280... Loss: 1.0583... Val Loss: 1.3055\n",
      "Epoch: 18/27... Step: 24290... Loss: 1.0892... Val Loss: 1.3062\n",
      "Epoch: 18/27... Step: 24300... Loss: 1.1006... Val Loss: 1.3104\n",
      "Epoch: 18/27... Step: 24310... Loss: 1.0749... Val Loss: 1.3118\n",
      "Epoch: 18/27... Step: 24320... Loss: 1.1259... Val Loss: 1.3105\n",
      "Epoch: 18/27... Step: 24330... Loss: 1.0842... Val Loss: 1.3070\n",
      "Epoch: 18/27... Step: 24340... Loss: 1.1826... Val Loss: 1.3081\n",
      "Epoch: 18/27... Step: 24350... Loss: 1.1015... Val Loss: 1.3136\n",
      "Epoch: 18/27... Step: 24360... Loss: 1.1798... Val Loss: 1.3139\n",
      "Epoch: 18/27... Step: 24370... Loss: 1.1373... Val Loss: 1.3121\n",
      "Epoch: 18/27... Step: 24380... Loss: 1.1242... Val Loss: 1.3074\n",
      "Epoch: 18/27... Step: 24390... Loss: 1.1337... Val Loss: 1.3068\n",
      "Epoch: 18/27... Step: 24400... Loss: 1.1238... Val Loss: 1.3119\n",
      "Epoch: 18/27... Step: 24410... Loss: 1.1219... Val Loss: 1.3099\n",
      "Epoch: 18/27... Step: 24420... Loss: 1.1431... Val Loss: 1.3078\n",
      "Epoch: 18/27... Step: 24430... Loss: 1.1445... Val Loss: 1.3083\n",
      "Epoch: 18/27... Step: 24440... Loss: 1.1220... Val Loss: 1.3096\n",
      "Epoch: 18/27... Step: 24450... Loss: 1.0978... Val Loss: 1.3119\n",
      "Epoch: 18/27... Step: 24460... Loss: 1.1725... Val Loss: 1.3121\n",
      "Epoch: 18/27... Step: 24470... Loss: 1.0653... Val Loss: 1.3109\n",
      "Epoch: 18/27... Step: 24480... Loss: 1.1103... Val Loss: 1.3076\n",
      "Epoch: 18/27... Step: 24490... Loss: 1.1335... Val Loss: 1.3112\n",
      "Epoch: 18/27... Step: 24500... Loss: 1.1692... Val Loss: 1.3104\n",
      "Epoch: 18/27... Step: 24510... Loss: 1.1646... Val Loss: 1.3090\n",
      "Epoch: 18/27... Step: 24520... Loss: 1.1014... Val Loss: 1.3052\n",
      "Epoch: 18/27... Step: 24530... Loss: 1.1064... Val Loss: 1.3044\n",
      "Epoch: 18/27... Step: 24540... Loss: 1.1048... Val Loss: 1.3085\n",
      "Epoch: 18/27... Step: 24550... Loss: 1.1130... Val Loss: 1.3144\n",
      "Epoch: 18/27... Step: 24560... Loss: 1.2086... Val Loss: 1.3094\n",
      "Epoch: 18/27... Step: 24570... Loss: 1.1163... Val Loss: 1.3044\n",
      "Epoch: 18/27... Step: 24580... Loss: 1.1129... Val Loss: 1.3071\n",
      "Epoch: 18/27... Step: 24590... Loss: 1.1247... Val Loss: 1.3134\n",
      "Epoch: 18/27... Step: 24600... Loss: 1.1895... Val Loss: 1.3178\n",
      "Epoch: 18/27... Step: 24610... Loss: 1.1051... Val Loss: 1.3126\n",
      "Epoch: 18/27... Step: 24620... Loss: 1.1129... Val Loss: 1.3097\n",
      "Epoch: 18/27... Step: 24630... Loss: 1.0932... Val Loss: 1.3069\n",
      "Epoch: 18/27... Step: 24640... Loss: 1.1063... Val Loss: 1.3072\n",
      "Epoch: 18/27... Step: 24650... Loss: 1.1444... Val Loss: 1.3083\n",
      "Epoch: 18/27... Step: 24660... Loss: 1.1711... Val Loss: 1.3071\n",
      "Epoch: 18/27... Step: 24670... Loss: 1.1625... Val Loss: 1.3064\n",
      "Epoch: 18/27... Step: 24680... Loss: 1.1311... Val Loss: 1.3046\n",
      "Epoch: 18/27... Step: 24690... Loss: 1.1584... Val Loss: 1.3090\n",
      "Epoch: 18/27... Step: 24700... Loss: 1.1503... Val Loss: 1.3098\n",
      "Epoch: 18/27... Step: 24710... Loss: 1.1539... Val Loss: 1.3088\n",
      "Epoch: 18/27... Step: 24720... Loss: 1.1162... Val Loss: 1.3070\n",
      "Epoch: 18/27... Step: 24730... Loss: 1.0978... Val Loss: 1.3074\n",
      "Epoch: 18/27... Step: 24740... Loss: 1.1359... Val Loss: 1.3087\n",
      "Epoch: 18/27... Step: 24750... Loss: 1.1650... Val Loss: 1.3098\n",
      "Epoch: 18/27... Step: 24760... Loss: 1.1534... Val Loss: 1.3066\n",
      "Epoch: 18/27... Step: 24770... Loss: 1.1231... Val Loss: 1.3037\n",
      "Epoch: 18/27... Step: 24780... Loss: 1.1895... Val Loss: 1.3091\n",
      "Epoch: 18/27... Step: 24790... Loss: 1.1309... Val Loss: 1.3121\n",
      "Epoch: 18/27... Step: 24800... Loss: 1.1428... Val Loss: 1.3085\n",
      "Epoch: 18/27... Step: 24810... Loss: 1.0876... Val Loss: 1.3068\n",
      "Epoch: 18/27... Step: 24820... Loss: 1.0838... Val Loss: 1.3087\n",
      "Epoch: 18/27... Step: 24830... Loss: 1.1240... Val Loss: 1.3085\n",
      "Epoch: 18/27... Step: 24840... Loss: 1.0897... Val Loss: 1.3088\n",
      "Epoch: 18/27... Step: 24850... Loss: 1.0904... Val Loss: 1.3072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/27... Step: 24860... Loss: 1.1186... Val Loss: 1.3059\n",
      "Epoch: 18/27... Step: 24870... Loss: 1.1053... Val Loss: 1.3086\n",
      "Epoch: 18/27... Step: 24880... Loss: 1.1024... Val Loss: 1.3075\n",
      "Epoch: 18/27... Step: 24890... Loss: 1.0984... Val Loss: 1.3060\n",
      "Epoch: 18/27... Step: 24900... Loss: 1.1079... Val Loss: 1.3069\n",
      "Epoch: 18/27... Step: 24910... Loss: 1.1206... Val Loss: 1.3108\n",
      "Epoch: 18/27... Step: 24920... Loss: 1.0550... Val Loss: 1.3068\n",
      "Epoch: 18/27... Step: 24930... Loss: 1.0365... Val Loss: 1.3066\n",
      "Epoch: 18/27... Step: 24940... Loss: 1.1787... Val Loss: 1.3096\n",
      "Epoch: 18/27... Step: 24950... Loss: 1.1481... Val Loss: 1.3111\n",
      "Epoch: 18/27... Step: 24960... Loss: 1.1567... Val Loss: 1.3075\n",
      "Epoch: 18/27... Step: 24970... Loss: 1.1660... Val Loss: 1.3069\n",
      "Epoch: 18/27... Step: 24980... Loss: 1.1492... Val Loss: 1.3082\n",
      "Epoch: 18/27... Step: 24990... Loss: 1.2004... Val Loss: 1.3063\n",
      "Epoch: 18/27... Step: 25000... Loss: 1.1085... Val Loss: 1.3058\n",
      "Epoch: 18/27... Step: 25010... Loss: 1.0814... Val Loss: 1.3079\n",
      "Epoch: 18/27... Step: 25020... Loss: 1.1064... Val Loss: 1.3070\n",
      "Epoch: 18/27... Step: 25030... Loss: 1.1642... Val Loss: 1.3053\n",
      "Epoch: 18/27... Step: 25040... Loss: 1.0552... Val Loss: 1.3067\n",
      "Epoch: 18/27... Step: 25050... Loss: 1.1763... Val Loss: 1.3075\n",
      "Epoch: 18/27... Step: 25060... Loss: 1.0976... Val Loss: 1.3055\n",
      "Epoch: 18/27... Step: 25070... Loss: 1.0790... Val Loss: 1.3042\n",
      "Epoch: 18/27... Step: 25080... Loss: 1.0504... Val Loss: 1.3018\n",
      "Epoch: 18/27... Step: 25090... Loss: 1.1015... Val Loss: 1.3042\n",
      "Epoch: 18/27... Step: 25100... Loss: 1.0928... Val Loss: 1.3012\n",
      "Epoch: 18/27... Step: 25110... Loss: 1.7624... Val Loss: 1.3052\n",
      "Epoch: 19/27... Step: 25120... Loss: 1.1483... Val Loss: 1.3080\n",
      "Epoch: 19/27... Step: 25130... Loss: 1.0929... Val Loss: 1.3042\n",
      "Epoch: 19/27... Step: 25140... Loss: 1.1397... Val Loss: 1.3005\n",
      "Epoch: 19/27... Step: 25150... Loss: 1.1127... Val Loss: 1.3012\n",
      "Epoch: 19/27... Step: 25160... Loss: 1.1354... Val Loss: 1.3010\n",
      "Epoch: 19/27... Step: 25170... Loss: 1.1388... Val Loss: 1.3038\n",
      "Epoch: 19/27... Step: 25180... Loss: 1.0838... Val Loss: 1.3096\n",
      "Epoch: 19/27... Step: 25190... Loss: 1.1129... Val Loss: 1.3107\n",
      "Epoch: 19/27... Step: 25200... Loss: 1.0950... Val Loss: 1.3064\n",
      "Epoch: 19/27... Step: 25210... Loss: 1.1484... Val Loss: 1.3058\n",
      "Epoch: 19/27... Step: 25220... Loss: 1.1854... Val Loss: 1.3068\n",
      "Epoch: 19/27... Step: 25230... Loss: 1.1530... Val Loss: 1.3068\n",
      "Epoch: 19/27... Step: 25240... Loss: 1.1676... Val Loss: 1.3046\n",
      "Epoch: 19/27... Step: 25250... Loss: 1.1203... Val Loss: 1.3067\n",
      "Epoch: 19/27... Step: 25260... Loss: 1.1311... Val Loss: 1.3111\n",
      "Epoch: 19/27... Step: 25270... Loss: 1.0565... Val Loss: 1.3100\n",
      "Epoch: 19/27... Step: 25280... Loss: 1.1000... Val Loss: 1.3074\n",
      "Epoch: 19/27... Step: 25290... Loss: 1.1545... Val Loss: 1.3079\n",
      "Epoch: 19/27... Step: 25300... Loss: 1.1040... Val Loss: 1.3078\n",
      "Epoch: 19/27... Step: 25310... Loss: 1.1035... Val Loss: 1.3065\n",
      "Epoch: 19/27... Step: 25320... Loss: 1.0972... Val Loss: 1.3069\n",
      "Epoch: 19/27... Step: 25330... Loss: 1.1467... Val Loss: 1.3107\n",
      "Epoch: 19/27... Step: 25340... Loss: 1.1021... Val Loss: 1.3112\n",
      "Epoch: 19/27... Step: 25350... Loss: 1.1142... Val Loss: 1.3123\n",
      "Epoch: 19/27... Step: 25360... Loss: 1.0971... Val Loss: 1.3095\n",
      "Epoch: 19/27... Step: 25370... Loss: 1.0637... Val Loss: 1.3075\n",
      "Epoch: 19/27... Step: 25380... Loss: 1.0142... Val Loss: 1.3081\n",
      "Epoch: 19/27... Step: 25390... Loss: 1.1476... Val Loss: 1.3114\n",
      "Epoch: 19/27... Step: 25400... Loss: 1.1037... Val Loss: 1.3093\n",
      "Epoch: 19/27... Step: 25410... Loss: 1.2100... Val Loss: 1.3125\n",
      "Epoch: 19/27... Step: 25420... Loss: 1.1164... Val Loss: 1.3170\n",
      "Epoch: 19/27... Step: 25430... Loss: 1.1117... Val Loss: 1.3147\n",
      "Epoch: 19/27... Step: 25440... Loss: 1.1360... Val Loss: 1.3094\n",
      "Epoch: 19/27... Step: 25450... Loss: 1.1525... Val Loss: 1.3095\n",
      "Epoch: 19/27... Step: 25460... Loss: 1.1412... Val Loss: 1.3111\n",
      "Epoch: 19/27... Step: 25470... Loss: 1.1571... Val Loss: 1.3061\n",
      "Epoch: 19/27... Step: 25480... Loss: 1.1814... Val Loss: 1.3062\n",
      "Epoch: 19/27... Step: 25490... Loss: 1.1467... Val Loss: 1.3124\n",
      "Epoch: 19/27... Step: 25500... Loss: 1.1159... Val Loss: 1.3144\n",
      "Epoch: 19/27... Step: 25510... Loss: 1.0804... Val Loss: 1.3122\n",
      "Epoch: 19/27... Step: 25520... Loss: 1.1393... Val Loss: 1.3118\n",
      "Epoch: 19/27... Step: 25530... Loss: 1.1477... Val Loss: 1.3079\n",
      "Epoch: 19/27... Step: 25540... Loss: 1.1169... Val Loss: 1.3088\n",
      "Epoch: 19/27... Step: 25550... Loss: 1.1625... Val Loss: 1.3115\n",
      "Epoch: 19/27... Step: 25560... Loss: 1.1323... Val Loss: 1.3083\n",
      "Epoch: 19/27... Step: 25570... Loss: 1.1463... Val Loss: 1.3054\n",
      "Epoch: 19/27... Step: 25580... Loss: 1.1775... Val Loss: 1.3040\n",
      "Epoch: 19/27... Step: 25590... Loss: 1.0967... Val Loss: 1.3053\n",
      "Epoch: 19/27... Step: 25600... Loss: 1.0668... Val Loss: 1.3104\n",
      "Epoch: 19/27... Step: 25610... Loss: 1.1514... Val Loss: 1.3104\n",
      "Epoch: 19/27... Step: 25620... Loss: 1.1034... Val Loss: 1.3050\n",
      "Epoch: 19/27... Step: 25630... Loss: 1.1687... Val Loss: 1.3070\n",
      "Epoch: 19/27... Step: 25640... Loss: 1.0617... Val Loss: 1.3067\n",
      "Epoch: 19/27... Step: 25650... Loss: 1.0594... Val Loss: 1.3102\n",
      "Epoch: 19/27... Step: 25660... Loss: 1.0791... Val Loss: 1.3100\n",
      "Epoch: 19/27... Step: 25670... Loss: 1.1447... Val Loss: 1.3080\n",
      "Epoch: 19/27... Step: 25680... Loss: 1.2185... Val Loss: 1.3086\n",
      "Epoch: 19/27... Step: 25690... Loss: 1.1179... Val Loss: 1.3091\n",
      "Epoch: 19/27... Step: 25700... Loss: 1.1149... Val Loss: 1.3108\n",
      "Epoch: 19/27... Step: 25710... Loss: 1.1274... Val Loss: 1.3122\n",
      "Epoch: 19/27... Step: 25720... Loss: 1.1014... Val Loss: 1.3086\n",
      "Epoch: 19/27... Step: 25730... Loss: 1.1601... Val Loss: 1.3097\n",
      "Epoch: 19/27... Step: 25740... Loss: 1.1685... Val Loss: 1.3177\n",
      "Epoch: 19/27... Step: 25750... Loss: 1.0961... Val Loss: 1.3171\n",
      "Epoch: 19/27... Step: 25760... Loss: 1.0898... Val Loss: 1.3172\n",
      "Epoch: 19/27... Step: 25770... Loss: 1.1485... Val Loss: 1.3142\n",
      "Epoch: 19/27... Step: 25780... Loss: 1.1096... Val Loss: 1.3121\n",
      "Epoch: 19/27... Step: 25790... Loss: 1.1792... Val Loss: 1.3134\n",
      "Epoch: 19/27... Step: 25800... Loss: 1.1578... Val Loss: 1.3146\n",
      "Epoch: 19/27... Step: 25810... Loss: 1.2273... Val Loss: 1.3088\n",
      "Epoch: 19/27... Step: 25820... Loss: 1.0979... Val Loss: 1.3069\n",
      "Epoch: 19/27... Step: 25830... Loss: 1.1642... Val Loss: 1.3079\n",
      "Epoch: 19/27... Step: 25840... Loss: 1.0664... Val Loss: 1.3087\n",
      "Epoch: 19/27... Step: 25850... Loss: 1.1174... Val Loss: 1.3105\n",
      "Epoch: 19/27... Step: 25860... Loss: 1.1360... Val Loss: 1.3092\n",
      "Epoch: 19/27... Step: 25870... Loss: 1.1176... Val Loss: 1.3086\n",
      "Epoch: 19/27... Step: 25880... Loss: 1.0503... Val Loss: 1.3094\n",
      "Epoch: 19/27... Step: 25890... Loss: 1.1849... Val Loss: 1.3093\n",
      "Epoch: 19/27... Step: 25900... Loss: 1.1517... Val Loss: 1.3051\n",
      "Epoch: 19/27... Step: 25910... Loss: 1.1504... Val Loss: 1.3050\n",
      "Epoch: 19/27... Step: 25920... Loss: 1.1277... Val Loss: 1.3075\n",
      "Epoch: 19/27... Step: 25930... Loss: 1.1253... Val Loss: 1.3052\n",
      "Epoch: 19/27... Step: 25940... Loss: 1.1489... Val Loss: 1.3099\n",
      "Epoch: 19/27... Step: 25950... Loss: 1.0923... Val Loss: 1.3141\n",
      "Epoch: 19/27... Step: 25960... Loss: 1.1313... Val Loss: 1.3077\n",
      "Epoch: 19/27... Step: 25970... Loss: 1.1001... Val Loss: 1.3033\n",
      "Epoch: 19/27... Step: 25980... Loss: 1.0951... Val Loss: 1.3059\n",
      "Epoch: 19/27... Step: 25990... Loss: 1.2039... Val Loss: 1.3064\n",
      "Epoch: 19/27... Step: 26000... Loss: 1.1718... Val Loss: 1.3118\n",
      "Epoch: 19/27... Step: 26010... Loss: 1.1463... Val Loss: 1.3080\n",
      "Epoch: 19/27... Step: 26020... Loss: 1.1694... Val Loss: 1.3039\n",
      "Epoch: 19/27... Step: 26030... Loss: 1.2191... Val Loss: 1.3008\n",
      "Epoch: 19/27... Step: 26040... Loss: 1.1000... Val Loss: 1.3025\n",
      "Epoch: 19/27... Step: 26050... Loss: 1.0863... Val Loss: 1.3099\n",
      "Epoch: 19/27... Step: 26060... Loss: 1.1239... Val Loss: 1.3049\n",
      "Epoch: 19/27... Step: 26070... Loss: 1.1876... Val Loss: 1.3008\n",
      "Epoch: 19/27... Step: 26080... Loss: 1.0394... Val Loss: 1.2999\n",
      "Epoch: 19/27... Step: 26090... Loss: 1.1124... Val Loss: 1.3109\n",
      "Epoch: 19/27... Step: 26100... Loss: 1.1325... Val Loss: 1.3103\n",
      "Epoch: 19/27... Step: 26110... Loss: 1.1124... Val Loss: 1.3054\n",
      "Epoch: 19/27... Step: 26120... Loss: 1.1530... Val Loss: 1.3039\n",
      "Epoch: 19/27... Step: 26130... Loss: 1.1199... Val Loss: 1.3042\n",
      "Epoch: 19/27... Step: 26140... Loss: 1.1155... Val Loss: 1.3044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/27... Step: 26150... Loss: 1.0737... Val Loss: 1.3063\n",
      "Epoch: 19/27... Step: 26160... Loss: 1.0849... Val Loss: 1.3062\n",
      "Epoch: 19/27... Step: 26170... Loss: 1.1042... Val Loss: 1.3082\n",
      "Epoch: 19/27... Step: 26180... Loss: 1.1871... Val Loss: 1.3120\n",
      "Epoch: 19/27... Step: 26190... Loss: 1.1279... Val Loss: 1.3105\n",
      "Epoch: 19/27... Step: 26200... Loss: 1.0098... Val Loss: 1.3095\n",
      "Epoch: 19/27... Step: 26210... Loss: 1.1401... Val Loss: 1.3078\n",
      "Epoch: 19/27... Step: 26220... Loss: 1.1653... Val Loss: 1.3061\n",
      "Epoch: 19/27... Step: 26230... Loss: 1.1255... Val Loss: 1.3052\n",
      "Epoch: 19/27... Step: 26240... Loss: 1.0759... Val Loss: 1.3068\n",
      "Epoch: 19/27... Step: 26250... Loss: 1.0728... Val Loss: 1.3077\n",
      "Epoch: 19/27... Step: 26260... Loss: 1.0926... Val Loss: 1.3069\n",
      "Epoch: 19/27... Step: 26270... Loss: 1.1407... Val Loss: 1.3083\n",
      "Epoch: 19/27... Step: 26280... Loss: 1.1402... Val Loss: 1.3083\n",
      "Epoch: 19/27... Step: 26290... Loss: 1.1414... Val Loss: 1.3082\n",
      "Epoch: 19/27... Step: 26300... Loss: 1.1207... Val Loss: 1.3100\n",
      "Epoch: 19/27... Step: 26310... Loss: 1.1163... Val Loss: 1.3088\n",
      "Epoch: 19/27... Step: 26320... Loss: 1.0740... Val Loss: 1.3097\n",
      "Epoch: 19/27... Step: 26330... Loss: 1.1467... Val Loss: 1.3114\n",
      "Epoch: 19/27... Step: 26340... Loss: 1.0985... Val Loss: 1.3104\n",
      "Epoch: 19/27... Step: 26350... Loss: 1.0997... Val Loss: 1.3094\n",
      "Epoch: 19/27... Step: 26360... Loss: 1.1022... Val Loss: 1.3184\n",
      "Epoch: 19/27... Step: 26370... Loss: 1.0902... Val Loss: 1.3139\n",
      "Epoch: 19/27... Step: 26380... Loss: 1.1533... Val Loss: 1.3089\n",
      "Epoch: 19/27... Step: 26390... Loss: 1.1128... Val Loss: 1.3072\n",
      "Epoch: 19/27... Step: 26400... Loss: 1.1387... Val Loss: 1.3094\n",
      "Epoch: 19/27... Step: 26410... Loss: 1.1614... Val Loss: 1.3138\n",
      "Epoch: 19/27... Step: 26420... Loss: 1.1606... Val Loss: 1.3111\n",
      "Epoch: 19/27... Step: 26430... Loss: 1.1115... Val Loss: 1.3065\n",
      "Epoch: 19/27... Step: 26440... Loss: 1.0709... Val Loss: 1.3074\n",
      "Epoch: 19/27... Step: 26450... Loss: 1.1791... Val Loss: 1.3119\n",
      "Epoch: 19/27... Step: 26460... Loss: 1.1586... Val Loss: 1.3049\n",
      "Epoch: 19/27... Step: 26470... Loss: 1.0860... Val Loss: 1.3023\n",
      "Epoch: 19/27... Step: 26480... Loss: 1.1505... Val Loss: 1.3078\n",
      "Epoch: 19/27... Step: 26490... Loss: 1.1844... Val Loss: 1.3119\n",
      "Epoch: 19/27... Step: 26500... Loss: 1.1480... Val Loss: 1.3099\n",
      "Epoch: 20/27... Step: 26510... Loss: 1.1365... Val Loss: 1.3086\n",
      "Epoch: 20/27... Step: 26520... Loss: 1.1066... Val Loss: 1.3153\n",
      "Epoch: 20/27... Step: 26530... Loss: 1.1279... Val Loss: 1.3071\n",
      "Epoch: 20/27... Step: 26540... Loss: 1.0801... Val Loss: 1.3027\n",
      "Epoch: 20/27... Step: 26550... Loss: 1.1045... Val Loss: 1.3051\n",
      "Epoch: 20/27... Step: 26560... Loss: 1.0938... Val Loss: 1.3062\n",
      "Epoch: 20/27... Step: 26570... Loss: 1.1283... Val Loss: 1.3076\n",
      "Epoch: 20/27... Step: 26580... Loss: 1.0833... Val Loss: 1.3074\n",
      "Epoch: 20/27... Step: 26590... Loss: 1.0877... Val Loss: 1.3055\n",
      "Epoch: 20/27... Step: 26600... Loss: 1.1606... Val Loss: 1.3059\n",
      "Epoch: 20/27... Step: 26610... Loss: 1.0879... Val Loss: 1.3099\n",
      "Epoch: 20/27... Step: 26620... Loss: 1.1103... Val Loss: 1.3085\n",
      "Epoch: 20/27... Step: 26630... Loss: 1.1200... Val Loss: 1.3051\n",
      "Epoch: 20/27... Step: 26640... Loss: 1.1098... Val Loss: 1.3059\n",
      "Epoch: 20/27... Step: 26650... Loss: 1.1182... Val Loss: 1.3137\n",
      "Epoch: 20/27... Step: 26660... Loss: 1.1548... Val Loss: 1.3127\n",
      "Epoch: 20/27... Step: 26670... Loss: 1.1120... Val Loss: 1.3055\n",
      "Epoch: 20/27... Step: 26680... Loss: 1.1158... Val Loss: 1.3024\n",
      "Epoch: 20/27... Step: 26690... Loss: 1.1905... Val Loss: 1.3046\n",
      "Epoch: 20/27... Step: 26700... Loss: 1.1512... Val Loss: 1.3041\n",
      "Epoch: 20/27... Step: 26710... Loss: 1.0911... Val Loss: 1.3014\n",
      "Epoch: 20/27... Step: 26720... Loss: 1.1040... Val Loss: 1.3019\n",
      "Epoch: 20/27... Step: 26730... Loss: 1.0908... Val Loss: 1.3047\n",
      "Epoch: 20/27... Step: 26740... Loss: 1.0482... Val Loss: 1.3076\n",
      "Epoch: 20/27... Step: 26750... Loss: 1.0791... Val Loss: 1.3069\n",
      "Epoch: 20/27... Step: 26760... Loss: 1.1321... Val Loss: 1.3037\n",
      "Epoch: 20/27... Step: 26770... Loss: 1.1023... Val Loss: 1.3046\n",
      "Epoch: 20/27... Step: 26780... Loss: 1.1500... Val Loss: 1.3056\n",
      "Epoch: 20/27... Step: 26790... Loss: 1.1055... Val Loss: 1.3075\n",
      "Epoch: 20/27... Step: 26800... Loss: 1.1700... Val Loss: 1.3069\n",
      "Epoch: 20/27... Step: 26810... Loss: 1.0958... Val Loss: 1.3101\n",
      "Epoch: 20/27... Step: 26820... Loss: 1.1262... Val Loss: 1.3118\n",
      "Epoch: 20/27... Step: 26830... Loss: 1.0744... Val Loss: 1.3097\n",
      "Epoch: 20/27... Step: 26840... Loss: 1.1246... Val Loss: 1.3089\n",
      "Epoch: 20/27... Step: 26850... Loss: 1.1131... Val Loss: 1.3088\n",
      "Epoch: 20/27... Step: 26860... Loss: 1.1228... Val Loss: 1.3065\n",
      "Epoch: 20/27... Step: 26870... Loss: 1.1364... Val Loss: 1.3068\n",
      "Epoch: 20/27... Step: 26880... Loss: 1.1720... Val Loss: 1.3067\n",
      "Epoch: 20/27... Step: 26890... Loss: 1.1144... Val Loss: 1.3086\n",
      "Epoch: 20/27... Step: 26900... Loss: 1.0975... Val Loss: 1.3117\n",
      "Epoch: 20/27... Step: 26910... Loss: 1.1688... Val Loss: 1.3103\n",
      "Epoch: 20/27... Step: 26920... Loss: 1.0999... Val Loss: 1.3050\n",
      "Epoch: 20/27... Step: 26930... Loss: 1.0600... Val Loss: 1.3030\n",
      "Epoch: 20/27... Step: 26940... Loss: 1.1384... Val Loss: 1.3066\n",
      "Epoch: 20/27... Step: 26950... Loss: 1.0993... Val Loss: 1.3091\n",
      "Epoch: 20/27... Step: 26960... Loss: 1.1114... Val Loss: 1.3060\n",
      "Epoch: 20/27... Step: 26970... Loss: 1.1607... Val Loss: 1.3040\n",
      "Epoch: 20/27... Step: 26980... Loss: 1.0838... Val Loss: 1.3052\n",
      "Epoch: 20/27... Step: 26990... Loss: 1.1082... Val Loss: 1.3066\n",
      "Epoch: 20/27... Step: 27000... Loss: 1.1364... Val Loss: 1.3099\n",
      "Epoch: 20/27... Step: 27010... Loss: 1.0582... Val Loss: 1.3035\n",
      "Epoch: 20/27... Step: 27020... Loss: 1.1202... Val Loss: 1.3031\n",
      "Epoch: 20/27... Step: 27030... Loss: 1.1361... Val Loss: 1.3018\n",
      "Epoch: 20/27... Step: 27040... Loss: 1.0725... Val Loss: 1.3054\n",
      "Epoch: 20/27... Step: 27050... Loss: 1.0783... Val Loss: 1.3111\n",
      "Epoch: 20/27... Step: 27060... Loss: 1.0811... Val Loss: 1.3115\n",
      "Epoch: 20/27... Step: 27070... Loss: 1.0274... Val Loss: 1.3098\n",
      "Epoch: 20/27... Step: 27080... Loss: 1.0727... Val Loss: 1.3074\n",
      "Epoch: 20/27... Step: 27090... Loss: 1.0949... Val Loss: 1.3062\n",
      "Epoch: 20/27... Step: 27100... Loss: 1.0678... Val Loss: 1.3076\n",
      "Epoch: 20/27... Step: 27110... Loss: 1.1318... Val Loss: 1.3086\n",
      "Epoch: 20/27... Step: 27120... Loss: 1.0811... Val Loss: 1.3075\n",
      "Epoch: 20/27... Step: 27130... Loss: 1.1387... Val Loss: 1.3049\n",
      "Epoch: 20/27... Step: 27140... Loss: 1.1003... Val Loss: 1.3066\n",
      "Epoch: 20/27... Step: 27150... Loss: 1.1770... Val Loss: 1.3115\n",
      "Epoch: 20/27... Step: 27160... Loss: 1.1370... Val Loss: 1.3102\n",
      "Epoch: 20/27... Step: 27170... Loss: 1.1243... Val Loss: 1.3055\n",
      "Epoch: 20/27... Step: 27180... Loss: 1.1399... Val Loss: 1.3066\n",
      "Epoch: 20/27... Step: 27190... Loss: 1.1173... Val Loss: 1.3107\n",
      "Epoch: 20/27... Step: 27200... Loss: 1.1077... Val Loss: 1.3074\n",
      "Epoch: 20/27... Step: 27210... Loss: 1.1212... Val Loss: 1.3060\n",
      "Epoch: 20/27... Step: 27220... Loss: 1.1310... Val Loss: 1.3074\n",
      "Epoch: 20/27... Step: 27230... Loss: 1.0968... Val Loss: 1.3078\n",
      "Epoch: 20/27... Step: 27240... Loss: 1.0923... Val Loss: 1.3098\n",
      "Epoch: 20/27... Step: 27250... Loss: 1.1695... Val Loss: 1.3089\n",
      "Epoch: 20/27... Step: 27260... Loss: 1.0687... Val Loss: 1.3063\n",
      "Epoch: 20/27... Step: 27270... Loss: 1.1278... Val Loss: 1.3052\n",
      "Epoch: 20/27... Step: 27280... Loss: 1.1226... Val Loss: 1.3080\n",
      "Epoch: 20/27... Step: 27290... Loss: 1.1530... Val Loss: 1.3089\n",
      "Epoch: 20/27... Step: 27300... Loss: 1.1522... Val Loss: 1.3068\n",
      "Epoch: 20/27... Step: 27310... Loss: 1.0763... Val Loss: 1.3054\n",
      "Epoch: 20/27... Step: 27320... Loss: 1.0999... Val Loss: 1.3043\n",
      "Epoch: 20/27... Step: 27330... Loss: 1.1092... Val Loss: 1.3094\n",
      "Epoch: 20/27... Step: 27340... Loss: 1.0853... Val Loss: 1.3094\n",
      "Epoch: 20/27... Step: 27350... Loss: 1.1590... Val Loss: 1.3050\n",
      "Epoch: 20/27... Step: 27360... Loss: 1.0935... Val Loss: 1.3018\n",
      "Epoch: 20/27... Step: 27370... Loss: 1.1058... Val Loss: 1.3037\n",
      "Epoch: 20/27... Step: 27380... Loss: 1.1347... Val Loss: 1.3035\n",
      "Epoch: 20/27... Step: 27390... Loss: 1.1880... Val Loss: 1.3047\n",
      "Epoch: 20/27... Step: 27400... Loss: 1.0864... Val Loss: 1.3062\n",
      "Epoch: 20/27... Step: 27410... Loss: 1.1141... Val Loss: 1.3032\n",
      "Epoch: 20/27... Step: 27420... Loss: 1.0762... Val Loss: 1.3019\n",
      "Epoch: 20/27... Step: 27430... Loss: 1.0889... Val Loss: 1.3037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/27... Step: 27440... Loss: 1.1307... Val Loss: 1.3119\n",
      "Epoch: 20/27... Step: 27450... Loss: 1.1770... Val Loss: 1.3071\n",
      "Epoch: 20/27... Step: 27460... Loss: 1.1123... Val Loss: 1.3042\n",
      "Epoch: 20/27... Step: 27470... Loss: 1.1272... Val Loss: 1.3014\n",
      "Epoch: 20/27... Step: 27480... Loss: 1.1234... Val Loss: 1.3083\n",
      "Epoch: 20/27... Step: 27490... Loss: 1.1761... Val Loss: 1.3119\n",
      "Epoch: 20/27... Step: 27500... Loss: 1.1290... Val Loss: 1.3096\n",
      "Epoch: 20/27... Step: 27510... Loss: 1.0999... Val Loss: 1.3078\n",
      "Epoch: 20/27... Step: 27520... Loss: 1.0924... Val Loss: 1.3079\n",
      "Epoch: 20/27... Step: 27530... Loss: 1.1253... Val Loss: 1.3079\n",
      "Epoch: 20/27... Step: 27540... Loss: 1.1622... Val Loss: 1.3093\n",
      "Epoch: 20/27... Step: 27550... Loss: 1.1396... Val Loss: 1.3061\n",
      "Epoch: 20/27... Step: 27560... Loss: 1.1230... Val Loss: 1.3064\n",
      "Epoch: 20/27... Step: 27570... Loss: 1.1401... Val Loss: 1.3116\n",
      "Epoch: 20/27... Step: 27580... Loss: 1.1005... Val Loss: 1.3135\n",
      "Epoch: 20/27... Step: 27590... Loss: 1.1225... Val Loss: 1.3102\n",
      "Epoch: 20/27... Step: 27600... Loss: 1.0822... Val Loss: 1.3104\n",
      "Epoch: 20/27... Step: 27610... Loss: 1.0769... Val Loss: 1.3098\n",
      "Epoch: 20/27... Step: 27620... Loss: 1.1124... Val Loss: 1.3083\n",
      "Epoch: 20/27... Step: 27630... Loss: 1.0969... Val Loss: 1.3082\n",
      "Epoch: 20/27... Step: 27640... Loss: 1.0717... Val Loss: 1.3088\n",
      "Epoch: 20/27... Step: 27650... Loss: 1.1187... Val Loss: 1.3082\n",
      "Epoch: 20/27... Step: 27660... Loss: 1.1120... Val Loss: 1.3103\n",
      "Epoch: 20/27... Step: 27670... Loss: 1.0781... Val Loss: 1.3111\n",
      "Epoch: 20/27... Step: 27680... Loss: 1.0749... Val Loss: 1.3091\n",
      "Epoch: 20/27... Step: 27690... Loss: 1.1156... Val Loss: 1.3099\n",
      "Epoch: 20/27... Step: 27700... Loss: 1.1013... Val Loss: 1.3093\n",
      "Epoch: 20/27... Step: 27710... Loss: 1.0625... Val Loss: 1.3074\n",
      "Epoch: 20/27... Step: 27720... Loss: 1.0515... Val Loss: 1.3076\n",
      "Epoch: 20/27... Step: 27730... Loss: 1.1706... Val Loss: 1.3117\n",
      "Epoch: 20/27... Step: 27740... Loss: 1.1533... Val Loss: 1.3104\n",
      "Epoch: 20/27... Step: 27750... Loss: 1.1301... Val Loss: 1.3094\n",
      "Epoch: 20/27... Step: 27760... Loss: 1.1471... Val Loss: 1.3115\n",
      "Epoch: 20/27... Step: 27770... Loss: 1.1256... Val Loss: 1.3101\n",
      "Epoch: 20/27... Step: 27780... Loss: 1.1994... Val Loss: 1.3097\n",
      "Epoch: 20/27... Step: 27790... Loss: 1.0878... Val Loss: 1.3104\n",
      "Epoch: 20/27... Step: 27800... Loss: 1.0457... Val Loss: 1.3100\n",
      "Epoch: 20/27... Step: 27810... Loss: 1.1099... Val Loss: 1.3108\n",
      "Epoch: 20/27... Step: 27820... Loss: 1.1549... Val Loss: 1.3086\n",
      "Epoch: 20/27... Step: 27830... Loss: 1.0033... Val Loss: 1.3081\n",
      "Epoch: 20/27... Step: 27840... Loss: 1.1647... Val Loss: 1.3098\n",
      "Epoch: 20/27... Step: 27850... Loss: 1.0711... Val Loss: 1.3094\n",
      "Epoch: 20/27... Step: 27860... Loss: 1.0796... Val Loss: 1.3062\n",
      "Epoch: 20/27... Step: 27870... Loss: 1.0468... Val Loss: 1.3043\n",
      "Epoch: 20/27... Step: 27880... Loss: 1.0743... Val Loss: 1.3067\n",
      "Epoch: 20/27... Step: 27890... Loss: 1.0915... Val Loss: 1.3082\n",
      "Epoch: 20/27... Step: 27900... Loss: 1.7187... Val Loss: 1.3091\n",
      "Epoch: 21/27... Step: 27910... Loss: 1.1306... Val Loss: 1.3068\n",
      "Epoch: 21/27... Step: 27920... Loss: 1.0574... Val Loss: 1.3077\n",
      "Epoch: 21/27... Step: 27930... Loss: 1.1160... Val Loss: 1.3071\n",
      "Epoch: 21/27... Step: 27940... Loss: 1.1162... Val Loss: 1.3061\n",
      "Epoch: 21/27... Step: 27950... Loss: 1.1186... Val Loss: 1.3056\n",
      "Epoch: 21/27... Step: 27960... Loss: 1.1194... Val Loss: 1.3087\n",
      "Epoch: 21/27... Step: 27970... Loss: 1.0964... Val Loss: 1.3120\n",
      "Epoch: 21/27... Step: 27980... Loss: 1.0688... Val Loss: 1.3138\n",
      "Epoch: 21/27... Step: 27990... Loss: 1.1022... Val Loss: 1.3099\n",
      "Epoch: 21/27... Step: 28000... Loss: 1.1417... Val Loss: 1.3065\n",
      "Epoch: 21/27... Step: 28010... Loss: 1.1692... Val Loss: 1.3085\n",
      "Epoch: 21/27... Step: 28020... Loss: 1.1355... Val Loss: 1.3111\n",
      "Epoch: 21/27... Step: 28030... Loss: 1.1575... Val Loss: 1.3084\n",
      "Epoch: 21/27... Step: 28040... Loss: 1.1157... Val Loss: 1.3087\n",
      "Epoch: 21/27... Step: 28050... Loss: 1.1266... Val Loss: 1.3095\n",
      "Epoch: 21/27... Step: 28060... Loss: 1.0677... Val Loss: 1.3098\n",
      "Epoch: 21/27... Step: 28070... Loss: 1.1370... Val Loss: 1.3077\n",
      "Epoch: 21/27... Step: 28080... Loss: 1.1450... Val Loss: 1.3063\n",
      "Epoch: 21/27... Step: 28090... Loss: 1.0968... Val Loss: 1.3067\n",
      "Epoch: 21/27... Step: 28100... Loss: 1.0676... Val Loss: 1.3074\n",
      "Epoch: 21/27... Step: 28110... Loss: 1.0970... Val Loss: 1.3030\n",
      "Epoch: 21/27... Step: 28120... Loss: 1.1401... Val Loss: 1.3053\n",
      "Epoch: 21/27... Step: 28130... Loss: 1.1072... Val Loss: 1.3039\n",
      "Epoch: 21/27... Step: 28140... Loss: 1.0854... Val Loss: 1.3069\n",
      "Epoch: 21/27... Step: 28150... Loss: 1.0768... Val Loss: 1.3077\n",
      "Epoch: 21/27... Step: 28160... Loss: 1.0406... Val Loss: 1.3093\n",
      "Epoch: 21/27... Step: 28170... Loss: 0.9892... Val Loss: 1.3042\n",
      "Epoch: 21/27... Step: 28180... Loss: 1.0982... Val Loss: 1.3061\n",
      "Epoch: 21/27... Step: 28190... Loss: 1.1010... Val Loss: 1.3061\n",
      "Epoch: 21/27... Step: 28200... Loss: 1.1984... Val Loss: 1.3097\n",
      "Epoch: 21/27... Step: 28210... Loss: 1.1086... Val Loss: 1.3133\n",
      "Epoch: 21/27... Step: 28220... Loss: 1.0659... Val Loss: 1.3094\n",
      "Epoch: 21/27... Step: 28230... Loss: 1.1093... Val Loss: 1.3094\n",
      "Epoch: 21/27... Step: 28240... Loss: 1.1195... Val Loss: 1.3050\n",
      "Epoch: 21/27... Step: 28250... Loss: 1.1367... Val Loss: 1.3037\n",
      "Epoch: 21/27... Step: 28260... Loss: 1.1428... Val Loss: 1.3035\n",
      "Epoch: 21/27... Step: 28270... Loss: 1.1925... Val Loss: 1.3041\n",
      "Epoch: 21/27... Step: 28280... Loss: 1.1616... Val Loss: 1.3038\n",
      "Epoch: 21/27... Step: 28290... Loss: 1.1056... Val Loss: 1.3039\n",
      "Epoch: 21/27... Step: 28300... Loss: 1.0831... Val Loss: 1.3078\n",
      "Epoch: 21/27... Step: 28310... Loss: 1.1266... Val Loss: 1.3098\n",
      "Epoch: 21/27... Step: 28320... Loss: 1.1042... Val Loss: 1.3068\n",
      "Epoch: 21/27... Step: 28330... Loss: 1.1086... Val Loss: 1.3065\n",
      "Epoch: 21/27... Step: 28340... Loss: 1.1670... Val Loss: 1.3080\n",
      "Epoch: 21/27... Step: 28350... Loss: 1.1359... Val Loss: 1.3088\n",
      "Epoch: 21/27... Step: 28360... Loss: 1.1546... Val Loss: 1.3058\n",
      "Epoch: 21/27... Step: 28370... Loss: 1.1309... Val Loss: 1.3043\n",
      "Epoch: 21/27... Step: 28380... Loss: 1.0965... Val Loss: 1.3065\n",
      "Epoch: 21/27... Step: 28390... Loss: 1.0827... Val Loss: 1.3085\n",
      "Epoch: 21/27... Step: 28400... Loss: 1.1350... Val Loss: 1.3075\n",
      "Epoch: 21/27... Step: 28410... Loss: 1.1196... Val Loss: 1.3046\n",
      "Epoch: 21/27... Step: 28420... Loss: 1.1617... Val Loss: 1.3045\n",
      "Epoch: 21/27... Step: 28430... Loss: 1.0777... Val Loss: 1.3040\n",
      "Epoch: 21/27... Step: 28440... Loss: 1.0779... Val Loss: 1.3080\n",
      "Epoch: 21/27... Step: 28450... Loss: 1.0808... Val Loss: 1.3140\n",
      "Epoch: 21/27... Step: 28460... Loss: 1.1215... Val Loss: 1.3125\n",
      "Epoch: 21/27... Step: 28470... Loss: 1.2008... Val Loss: 1.3070\n",
      "Epoch: 21/27... Step: 28480... Loss: 1.1040... Val Loss: 1.3084\n",
      "Epoch: 21/27... Step: 28490... Loss: 1.0834... Val Loss: 1.3087\n",
      "Epoch: 21/27... Step: 28500... Loss: 1.1051... Val Loss: 1.3096\n",
      "Epoch: 21/27... Step: 28510... Loss: 1.1064... Val Loss: 1.3070\n",
      "Epoch: 21/27... Step: 28520... Loss: 1.1417... Val Loss: 1.3056\n",
      "Epoch: 21/27... Step: 28530... Loss: 1.1484... Val Loss: 1.3060\n",
      "Epoch: 21/27... Step: 28540... Loss: 1.1195... Val Loss: 1.3098\n",
      "Epoch: 21/27... Step: 28550... Loss: 1.0871... Val Loss: 1.3100\n",
      "Epoch: 21/27... Step: 28560... Loss: 1.0820... Val Loss: 1.3072\n",
      "Epoch: 21/27... Step: 28570... Loss: 1.0870... Val Loss: 1.3061\n",
      "Epoch: 21/27... Step: 28580... Loss: 1.1636... Val Loss: 1.3049\n",
      "Epoch: 21/27... Step: 28590... Loss: 1.1539... Val Loss: 1.3047\n",
      "Epoch: 21/27... Step: 28600... Loss: 1.1839... Val Loss: 1.3042\n",
      "Epoch: 21/27... Step: 28610... Loss: 1.0809... Val Loss: 1.3061\n",
      "Epoch: 21/27... Step: 28620... Loss: 1.1120... Val Loss: 1.3078\n",
      "Epoch: 21/27... Step: 28630... Loss: 1.0694... Val Loss: 1.3095\n",
      "Epoch: 21/27... Step: 28640... Loss: 1.1017... Val Loss: 1.3116\n",
      "Epoch: 21/27... Step: 28650... Loss: 1.1184... Val Loss: 1.3093\n",
      "Epoch: 21/27... Step: 28660... Loss: 1.1317... Val Loss: 1.3084\n",
      "Epoch: 21/27... Step: 28670... Loss: 1.0615... Val Loss: 1.3092\n",
      "Epoch: 21/27... Step: 28680... Loss: 1.1871... Val Loss: 1.3125\n",
      "Epoch: 21/27... Step: 28690... Loss: 1.1423... Val Loss: 1.3070\n",
      "Epoch: 21/27... Step: 28700... Loss: 1.0980... Val Loss: 1.3048\n",
      "Epoch: 21/27... Step: 28710... Loss: 1.1143... Val Loss: 1.3050\n",
      "Epoch: 21/27... Step: 28720... Loss: 1.1425... Val Loss: 1.3061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/27... Step: 28730... Loss: 1.1207... Val Loss: 1.3082\n",
      "Epoch: 21/27... Step: 28740... Loss: 1.0590... Val Loss: 1.3126\n",
      "Epoch: 21/27... Step: 28750... Loss: 1.1096... Val Loss: 1.3065\n",
      "Epoch: 21/27... Step: 28760... Loss: 1.0914... Val Loss: 1.3039\n",
      "Epoch: 21/27... Step: 28770... Loss: 1.1042... Val Loss: 1.3088\n",
      "Epoch: 21/27... Step: 28780... Loss: 1.1665... Val Loss: 1.3107\n",
      "Epoch: 21/27... Step: 28790... Loss: 1.1582... Val Loss: 1.3133\n",
      "Epoch: 21/27... Step: 28800... Loss: 1.1356... Val Loss: 1.3125\n",
      "Epoch: 21/27... Step: 28810... Loss: 1.1285... Val Loss: 1.3094\n",
      "Epoch: 21/27... Step: 28820... Loss: 1.1926... Val Loss: 1.3076\n",
      "Epoch: 21/27... Step: 28830... Loss: 1.1204... Val Loss: 1.3080\n",
      "Epoch: 21/27... Step: 28840... Loss: 1.0845... Val Loss: 1.3071\n",
      "Epoch: 21/27... Step: 28850... Loss: 1.1047... Val Loss: 1.3058\n",
      "Epoch: 21/27... Step: 28860... Loss: 1.2018... Val Loss: 1.3067\n",
      "Epoch: 21/27... Step: 28870... Loss: 1.0657... Val Loss: 1.3049\n",
      "Epoch: 21/27... Step: 28880... Loss: 1.1168... Val Loss: 1.3080\n",
      "Epoch: 21/27... Step: 28890... Loss: 1.1186... Val Loss: 1.3097\n",
      "Epoch: 21/27... Step: 28900... Loss: 1.1099... Val Loss: 1.3106\n",
      "Epoch: 21/27... Step: 28910... Loss: 1.1450... Val Loss: 1.3093\n",
      "Epoch: 21/27... Step: 28920... Loss: 1.1211... Val Loss: 1.3115\n",
      "Epoch: 21/27... Step: 28930... Loss: 1.1225... Val Loss: 1.3138\n",
      "Epoch: 21/27... Step: 28940... Loss: 1.0571... Val Loss: 1.3126\n",
      "Epoch: 21/27... Step: 28950... Loss: 1.0596... Val Loss: 1.3064\n",
      "Epoch: 21/27... Step: 28960... Loss: 1.0958... Val Loss: 1.3054\n",
      "Epoch: 21/27... Step: 28970... Loss: 1.1685... Val Loss: 1.3126\n",
      "Epoch: 21/27... Step: 28980... Loss: 1.1218... Val Loss: 1.3154\n",
      "Epoch: 21/27... Step: 28990... Loss: 1.0392... Val Loss: 1.3158\n",
      "Epoch: 21/27... Step: 29000... Loss: 1.1128... Val Loss: 1.3133\n",
      "Epoch: 21/27... Step: 29010... Loss: 1.1464... Val Loss: 1.3101\n",
      "Epoch: 21/27... Step: 29020... Loss: 1.1207... Val Loss: 1.3092\n",
      "Epoch: 21/27... Step: 29030... Loss: 1.0554... Val Loss: 1.3119\n",
      "Epoch: 21/27... Step: 29040... Loss: 1.0502... Val Loss: 1.3120\n",
      "Epoch: 21/27... Step: 29050... Loss: 1.1166... Val Loss: 1.3123\n",
      "Epoch: 21/27... Step: 29060... Loss: 1.1279... Val Loss: 1.3120\n",
      "Epoch: 21/27... Step: 29070... Loss: 1.1490... Val Loss: 1.3127\n",
      "Epoch: 21/27... Step: 29080... Loss: 1.1302... Val Loss: 1.3129\n",
      "Epoch: 21/27... Step: 29090... Loss: 1.1057... Val Loss: 1.3124\n",
      "Epoch: 21/27... Step: 29100... Loss: 1.1197... Val Loss: 1.3097\n",
      "Epoch: 21/27... Step: 29110... Loss: 1.0555... Val Loss: 1.3068\n",
      "Epoch: 21/27... Step: 29120... Loss: 1.1039... Val Loss: 1.3096\n",
      "Epoch: 21/27... Step: 29130... Loss: 1.0944... Val Loss: 1.3146\n",
      "Epoch: 21/27... Step: 29140... Loss: 1.1012... Val Loss: 1.3133\n",
      "Epoch: 21/27... Step: 29150... Loss: 1.0851... Val Loss: 1.3081\n",
      "Epoch: 21/27... Step: 29160... Loss: 1.0864... Val Loss: 1.3076\n",
      "Epoch: 21/27... Step: 29170... Loss: 1.1309... Val Loss: 1.3113\n",
      "Epoch: 21/27... Step: 29180... Loss: 1.0885... Val Loss: 1.3111\n",
      "Epoch: 21/27... Step: 29190... Loss: 1.1350... Val Loss: 1.3114\n",
      "Epoch: 21/27... Step: 29200... Loss: 1.1575... Val Loss: 1.3135\n",
      "Epoch: 21/27... Step: 29210... Loss: 1.1604... Val Loss: 1.3126\n",
      "Epoch: 21/27... Step: 29220... Loss: 1.0924... Val Loss: 1.3118\n",
      "Epoch: 21/27... Step: 29230... Loss: 1.0813... Val Loss: 1.3150\n",
      "Epoch: 21/27... Step: 29240... Loss: 1.1527... Val Loss: 1.3126\n",
      "Epoch: 21/27... Step: 29250... Loss: 1.1410... Val Loss: 1.3064\n",
      "Epoch: 21/27... Step: 29260... Loss: 1.0440... Val Loss: 1.3038\n",
      "Epoch: 21/27... Step: 29270... Loss: 1.1363... Val Loss: 1.3091\n",
      "Epoch: 21/27... Step: 29280... Loss: 1.1639... Val Loss: 1.3117\n",
      "Epoch: 21/27... Step: 29290... Loss: 1.1645... Val Loss: 1.3083\n",
      "Epoch: 22/27... Step: 29300... Loss: 1.1292... Val Loss: 1.3055\n",
      "Epoch: 22/27... Step: 29310... Loss: 1.0653... Val Loss: 1.3116\n",
      "Epoch: 22/27... Step: 29320... Loss: 1.1244... Val Loss: 1.3107\n",
      "Epoch: 22/27... Step: 29330... Loss: 1.0646... Val Loss: 1.3058\n",
      "Epoch: 22/27... Step: 29340... Loss: 1.1103... Val Loss: 1.3051\n",
      "Epoch: 22/27... Step: 29350... Loss: 1.1084... Val Loss: 1.3062\n",
      "Epoch: 22/27... Step: 29360... Loss: 1.0968... Val Loss: 1.3064\n",
      "Epoch: 22/27... Step: 29370... Loss: 1.0843... Val Loss: 1.3064\n",
      "Epoch: 22/27... Step: 29380... Loss: 1.0656... Val Loss: 1.3075\n",
      "Epoch: 22/27... Step: 29390... Loss: 1.1361... Val Loss: 1.3048\n",
      "Epoch: 22/27... Step: 29400... Loss: 1.0723... Val Loss: 1.3038\n",
      "Epoch: 22/27... Step: 29410... Loss: 1.1372... Val Loss: 1.3071\n",
      "Epoch: 22/27... Step: 29420... Loss: 1.0939... Val Loss: 1.3068\n",
      "Epoch: 22/27... Step: 29430... Loss: 1.0789... Val Loss: 1.3052\n",
      "Epoch: 22/27... Step: 29440... Loss: 1.1099... Val Loss: 1.3087\n",
      "Epoch: 22/27... Step: 29450... Loss: 1.1407... Val Loss: 1.3082\n",
      "Epoch: 22/27... Step: 29460... Loss: 1.1002... Val Loss: 1.3077\n",
      "Epoch: 22/27... Step: 29470... Loss: 1.1145... Val Loss: 1.3054\n",
      "Epoch: 22/27... Step: 29480... Loss: 1.1757... Val Loss: 1.3059\n",
      "Epoch: 22/27... Step: 29490... Loss: 1.1339... Val Loss: 1.3083\n",
      "Epoch: 22/27... Step: 29500... Loss: 1.1133... Val Loss: 1.3072\n",
      "Epoch: 22/27... Step: 29510... Loss: 1.1080... Val Loss: 1.3055\n",
      "Epoch: 22/27... Step: 29520... Loss: 1.1118... Val Loss: 1.3031\n",
      "Epoch: 22/27... Step: 29530... Loss: 1.0547... Val Loss: 1.3066\n",
      "Epoch: 22/27... Step: 29540... Loss: 1.0936... Val Loss: 1.3066\n",
      "Epoch: 22/27... Step: 29550... Loss: 1.1276... Val Loss: 1.3056\n",
      "Epoch: 22/27... Step: 29560... Loss: 1.1106... Val Loss: 1.3063\n",
      "Epoch: 22/27... Step: 29570... Loss: 1.1242... Val Loss: 1.3050\n",
      "Epoch: 22/27... Step: 29580... Loss: 1.1334... Val Loss: 1.3067\n",
      "Epoch: 22/27... Step: 29590... Loss: 1.1657... Val Loss: 1.3073\n",
      "Epoch: 22/27... Step: 29600... Loss: 1.0781... Val Loss: 1.3113\n",
      "Epoch: 22/27... Step: 29610... Loss: 1.0735... Val Loss: 1.3147\n",
      "Epoch: 22/27... Step: 29620... Loss: 1.0630... Val Loss: 1.3116\n",
      "Epoch: 22/27... Step: 29630... Loss: 1.0989... Val Loss: 1.3059\n",
      "Epoch: 22/27... Step: 29640... Loss: 1.1276... Val Loss: 1.3029\n",
      "Epoch: 22/27... Step: 29650... Loss: 1.1095... Val Loss: 1.3019\n",
      "Epoch: 22/27... Step: 29660... Loss: 1.0967... Val Loss: 1.3027\n",
      "Epoch: 22/27... Step: 29670... Loss: 1.1580... Val Loss: 1.3034\n",
      "Epoch: 22/27... Step: 29680... Loss: 1.1115... Val Loss: 1.3078\n",
      "Epoch: 22/27... Step: 29690... Loss: 1.1131... Val Loss: 1.3127\n",
      "Epoch: 22/27... Step: 29700... Loss: 1.1309... Val Loss: 1.3125\n",
      "Epoch: 22/27... Step: 29710... Loss: 1.1047... Val Loss: 1.3107\n",
      "Epoch: 22/27... Step: 29720... Loss: 1.0419... Val Loss: 1.3072\n",
      "Epoch: 22/27... Step: 29730... Loss: 1.1567... Val Loss: 1.3094\n",
      "Epoch: 22/27... Step: 29740... Loss: 1.0822... Val Loss: 1.3098\n",
      "Epoch: 22/27... Step: 29750... Loss: 1.0907... Val Loss: 1.3085\n",
      "Epoch: 22/27... Step: 29760... Loss: 1.1445... Val Loss: 1.3080\n",
      "Epoch: 22/27... Step: 29770... Loss: 1.0657... Val Loss: 1.3091\n",
      "Epoch: 22/27... Step: 29780... Loss: 1.1122... Val Loss: 1.3112\n",
      "Epoch: 22/27... Step: 29790... Loss: 1.1252... Val Loss: 1.3128\n",
      "Epoch: 22/27... Step: 29800... Loss: 1.0644... Val Loss: 1.3062\n",
      "Epoch: 22/27... Step: 29810... Loss: 1.1217... Val Loss: 1.3068\n",
      "Epoch: 22/27... Step: 29820... Loss: 1.1112... Val Loss: 1.3082\n",
      "Epoch: 22/27... Step: 29830... Loss: 1.0618... Val Loss: 1.3094\n",
      "Epoch: 22/27... Step: 29840... Loss: 1.0415... Val Loss: 1.3120\n",
      "Epoch: 22/27... Step: 29850... Loss: 1.0640... Val Loss: 1.3138\n",
      "Epoch: 22/27... Step: 29860... Loss: 1.0253... Val Loss: 1.3098\n",
      "Epoch: 22/27... Step: 29870... Loss: 1.0909... Val Loss: 1.3090\n",
      "Epoch: 22/27... Step: 29880... Loss: 1.0956... Val Loss: 1.3098\n",
      "Epoch: 22/27... Step: 29890... Loss: 1.0608... Val Loss: 1.3095\n",
      "Epoch: 22/27... Step: 29900... Loss: 1.1391... Val Loss: 1.3104\n",
      "Epoch: 22/27... Step: 29910... Loss: 1.0911... Val Loss: 1.3107\n",
      "Epoch: 22/27... Step: 29920... Loss: 1.1611... Val Loss: 1.3129\n",
      "Epoch: 22/27... Step: 29930... Loss: 1.0836... Val Loss: 1.3150\n",
      "Epoch: 22/27... Step: 29940... Loss: 1.1649... Val Loss: 1.3175\n",
      "Epoch: 22/27... Step: 29950... Loss: 1.1079... Val Loss: 1.3190\n",
      "Epoch: 22/27... Step: 29960... Loss: 1.1228... Val Loss: 1.3114\n",
      "Epoch: 22/27... Step: 29970... Loss: 1.1530... Val Loss: 1.3071\n",
      "Epoch: 22/27... Step: 29980... Loss: 1.1070... Val Loss: 1.3114\n",
      "Epoch: 22/27... Step: 29990... Loss: 1.1160... Val Loss: 1.3153\n",
      "Epoch: 22/27... Step: 30000... Loss: 1.1211... Val Loss: 1.3106\n",
      "Epoch: 22/27... Step: 30010... Loss: 1.1178... Val Loss: 1.3105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/27... Step: 30020... Loss: 1.1112... Val Loss: 1.3103\n",
      "Epoch: 22/27... Step: 30030... Loss: 1.0967... Val Loss: 1.3114\n",
      "Epoch: 22/27... Step: 30040... Loss: 1.1353... Val Loss: 1.3120\n",
      "Epoch: 22/27... Step: 30050... Loss: 1.0664... Val Loss: 1.3107\n",
      "Epoch: 22/27... Step: 30060... Loss: 1.1092... Val Loss: 1.3063\n",
      "Epoch: 22/27... Step: 30070... Loss: 1.0836... Val Loss: 1.3098\n",
      "Epoch: 22/27... Step: 30080... Loss: 1.1462... Val Loss: 1.3096\n",
      "Epoch: 22/27... Step: 30090... Loss: 1.0826... Val Loss: 1.3081\n",
      "Epoch: 22/27... Step: 30100... Loss: 1.1140... Val Loss: 1.3076\n",
      "Epoch: 22/27... Step: 30110... Loss: 1.1163... Val Loss: 1.3043\n",
      "Epoch: 22/27... Step: 30120... Loss: 1.1056... Val Loss: 1.3045\n",
      "Epoch: 22/27... Step: 30130... Loss: 1.0911... Val Loss: 1.3059\n",
      "Epoch: 22/27... Step: 30140... Loss: 1.1514... Val Loss: 1.3082\n",
      "Epoch: 22/27... Step: 30150... Loss: 1.0817... Val Loss: 1.3042\n",
      "Epoch: 22/27... Step: 30160... Loss: 1.1246... Val Loss: 1.3044\n",
      "Epoch: 22/27... Step: 30170... Loss: 1.1523... Val Loss: 1.3046\n",
      "Epoch: 22/27... Step: 30180... Loss: 1.1703... Val Loss: 1.3089\n",
      "Epoch: 22/27... Step: 30190... Loss: 1.0559... Val Loss: 1.3112\n",
      "Epoch: 22/27... Step: 30200... Loss: 1.0981... Val Loss: 1.3083\n",
      "Epoch: 22/27... Step: 30210... Loss: 1.1019... Val Loss: 1.3054\n",
      "Epoch: 22/27... Step: 30220... Loss: 1.0892... Val Loss: 1.3033\n",
      "Epoch: 22/27... Step: 30230... Loss: 1.1334... Val Loss: 1.3074\n",
      "Epoch: 22/27... Step: 30240... Loss: 1.1724... Val Loss: 1.3087\n",
      "Epoch: 22/27... Step: 30250... Loss: 1.1329... Val Loss: 1.3057\n",
      "Epoch: 22/27... Step: 30260... Loss: 1.1265... Val Loss: 1.3061\n",
      "Epoch: 22/27... Step: 30270... Loss: 1.1136... Val Loss: 1.3089\n",
      "Epoch: 22/27... Step: 30280... Loss: 1.1245... Val Loss: 1.3081\n",
      "Epoch: 22/27... Step: 30290... Loss: 1.1413... Val Loss: 1.3075\n",
      "Epoch: 22/27... Step: 30300... Loss: 1.0974... Val Loss: 1.3062\n",
      "Epoch: 22/27... Step: 30310... Loss: 1.0705... Val Loss: 1.3085\n",
      "Epoch: 22/27... Step: 30320... Loss: 1.1083... Val Loss: 1.3099\n",
      "Epoch: 22/27... Step: 30330... Loss: 1.1374... Val Loss: 1.3110\n",
      "Epoch: 22/27... Step: 30340... Loss: 1.1298... Val Loss: 1.3092\n",
      "Epoch: 22/27... Step: 30350... Loss: 1.0825... Val Loss: 1.3075\n",
      "Epoch: 22/27... Step: 30360... Loss: 1.1501... Val Loss: 1.3116\n",
      "Epoch: 22/27... Step: 30370... Loss: 1.1020... Val Loss: 1.3161\n",
      "Epoch: 22/27... Step: 30380... Loss: 1.1206... Val Loss: 1.3159\n",
      "Epoch: 22/27... Step: 30390... Loss: 1.0546... Val Loss: 1.3108\n",
      "Epoch: 22/27... Step: 30400... Loss: 1.0584... Val Loss: 1.3084\n",
      "Epoch: 22/27... Step: 30410... Loss: 1.1050... Val Loss: 1.3074\n",
      "Epoch: 22/27... Step: 30420... Loss: 1.0874... Val Loss: 1.3084\n",
      "Epoch: 22/27... Step: 30430... Loss: 1.0712... Val Loss: 1.3112\n",
      "Epoch: 22/27... Step: 30440... Loss: 1.1100... Val Loss: 1.3105\n",
      "Epoch: 22/27... Step: 30450... Loss: 1.1053... Val Loss: 1.3107\n",
      "Epoch: 22/27... Step: 30460... Loss: 1.0949... Val Loss: 1.3123\n",
      "Epoch: 22/27... Step: 30470... Loss: 1.0631... Val Loss: 1.3101\n",
      "Epoch: 22/27... Step: 30480... Loss: 1.1029... Val Loss: 1.3070\n",
      "Epoch: 22/27... Step: 30490... Loss: 1.1063... Val Loss: 1.3106\n",
      "Epoch: 22/27... Step: 30500... Loss: 1.0609... Val Loss: 1.3110\n",
      "Epoch: 22/27... Step: 30510... Loss: 1.0406... Val Loss: 1.3113\n",
      "Epoch: 22/27... Step: 30520... Loss: 1.1595... Val Loss: 1.3115\n",
      "Epoch: 22/27... Step: 30530... Loss: 1.1279... Val Loss: 1.3110\n",
      "Epoch: 22/27... Step: 30540... Loss: 1.1373... Val Loss: 1.3127\n",
      "Epoch: 22/27... Step: 30550... Loss: 1.1566... Val Loss: 1.3118\n",
      "Epoch: 22/27... Step: 30560... Loss: 1.1237... Val Loss: 1.3088\n",
      "Epoch: 22/27... Step: 30570... Loss: 1.1683... Val Loss: 1.3100\n",
      "Epoch: 22/27... Step: 30580... Loss: 1.0841... Val Loss: 1.3111\n",
      "Epoch: 22/27... Step: 30590... Loss: 1.0345... Val Loss: 1.3118\n",
      "Epoch: 22/27... Step: 30600... Loss: 1.1271... Val Loss: 1.3104\n",
      "Epoch: 22/27... Step: 30610... Loss: 1.1396... Val Loss: 1.3071\n",
      "Epoch: 22/27... Step: 30620... Loss: 1.0223... Val Loss: 1.3068\n",
      "Epoch: 22/27... Step: 30630... Loss: 1.1641... Val Loss: 1.3099\n",
      "Epoch: 22/27... Step: 30640... Loss: 1.1011... Val Loss: 1.3108\n",
      "Epoch: 22/27... Step: 30650... Loss: 1.0582... Val Loss: 1.3049\n",
      "Epoch: 22/27... Step: 30660... Loss: 1.0455... Val Loss: 1.3036\n",
      "Epoch: 22/27... Step: 30670... Loss: 1.0869... Val Loss: 1.3077\n",
      "Epoch: 22/27... Step: 30680... Loss: 1.0741... Val Loss: 1.3070\n",
      "Epoch: 22/27... Step: 30690... Loss: 1.7564... Val Loss: 1.3051\n",
      "Epoch: 23/27... Step: 30700... Loss: 1.1344... Val Loss: 1.3050\n",
      "Epoch: 23/27... Step: 30710... Loss: 1.0893... Val Loss: 1.3116\n",
      "Epoch: 23/27... Step: 30720... Loss: 1.1319... Val Loss: 1.3079\n",
      "Epoch: 23/27... Step: 30730... Loss: 1.1318... Val Loss: 1.3069\n",
      "Epoch: 23/27... Step: 30740... Loss: 1.1032... Val Loss: 1.3089\n",
      "Epoch: 23/27... Step: 30750... Loss: 1.1051... Val Loss: 1.3084\n",
      "Epoch: 23/27... Step: 30760... Loss: 1.0385... Val Loss: 1.3115\n",
      "Epoch: 23/27... Step: 30770... Loss: 1.0556... Val Loss: 1.3126\n",
      "Epoch: 23/27... Step: 30780... Loss: 1.1137... Val Loss: 1.3107\n",
      "Epoch: 23/27... Step: 30790... Loss: 1.1440... Val Loss: 1.3084\n",
      "Epoch: 23/27... Step: 30800... Loss: 1.1565... Val Loss: 1.3075\n",
      "Epoch: 23/27... Step: 30810... Loss: 1.1259... Val Loss: 1.3109\n",
      "Epoch: 23/27... Step: 30820... Loss: 1.1510... Val Loss: 1.3082\n",
      "Epoch: 23/27... Step: 30830... Loss: 1.0765... Val Loss: 1.3094\n",
      "Epoch: 23/27... Step: 30840... Loss: 1.1181... Val Loss: 1.3104\n",
      "Epoch: 23/27... Step: 30850... Loss: 1.0302... Val Loss: 1.3074\n",
      "Epoch: 23/27... Step: 30860... Loss: 1.0750... Val Loss: 1.3089\n",
      "Epoch: 23/27... Step: 30870... Loss: 1.1101... Val Loss: 1.3111\n",
      "Epoch: 23/27... Step: 30880... Loss: 1.1233... Val Loss: 1.3089\n",
      "Epoch: 23/27... Step: 30890... Loss: 1.0623... Val Loss: 1.3061\n",
      "Epoch: 23/27... Step: 30900... Loss: 1.1075... Val Loss: 1.3070\n",
      "Epoch: 23/27... Step: 30910... Loss: 1.1183... Val Loss: 1.3081\n",
      "Epoch: 23/27... Step: 30920... Loss: 1.0776... Val Loss: 1.3072\n",
      "Epoch: 23/27... Step: 30930... Loss: 1.0966... Val Loss: 1.3099\n",
      "Epoch: 23/27... Step: 30940... Loss: 1.0398... Val Loss: 1.3087\n",
      "Epoch: 23/27... Step: 30950... Loss: 1.0447... Val Loss: 1.3100\n",
      "Epoch: 23/27... Step: 30960... Loss: 0.9987... Val Loss: 1.3116\n",
      "Epoch: 23/27... Step: 30970... Loss: 1.0927... Val Loss: 1.3134\n",
      "Epoch: 23/27... Step: 30980... Loss: 1.0778... Val Loss: 1.3101\n",
      "Epoch: 23/27... Step: 30990... Loss: 1.1969... Val Loss: 1.3092\n",
      "Epoch: 23/27... Step: 31000... Loss: 1.1342... Val Loss: 1.3127\n",
      "Epoch: 23/27... Step: 31010... Loss: 1.0975... Val Loss: 1.3161\n",
      "Epoch: 23/27... Step: 31020... Loss: 1.0868... Val Loss: 1.3136\n",
      "Epoch: 23/27... Step: 31030... Loss: 1.1299... Val Loss: 1.3110\n",
      "Epoch: 23/27... Step: 31040... Loss: 1.1489... Val Loss: 1.3072\n",
      "Epoch: 23/27... Step: 31050... Loss: 1.1180... Val Loss: 1.3052\n",
      "Epoch: 23/27... Step: 31060... Loss: 1.1770... Val Loss: 1.3061\n",
      "Epoch: 23/27... Step: 31070... Loss: 1.1440... Val Loss: 1.3072\n",
      "Epoch: 23/27... Step: 31080... Loss: 1.0987... Val Loss: 1.3089\n",
      "Epoch: 23/27... Step: 31090... Loss: 1.0590... Val Loss: 1.3121\n",
      "Epoch: 23/27... Step: 31100... Loss: 1.1424... Val Loss: 1.3147\n",
      "Epoch: 23/27... Step: 31110... Loss: 1.1239... Val Loss: 1.3105\n",
      "Epoch: 23/27... Step: 31120... Loss: 1.1008... Val Loss: 1.3079\n",
      "Epoch: 23/27... Step: 31130... Loss: 1.1498... Val Loss: 1.3123\n",
      "Epoch: 23/27... Step: 31140... Loss: 1.1246... Val Loss: 1.3095\n",
      "Epoch: 23/27... Step: 31150... Loss: 1.1124... Val Loss: 1.3098\n",
      "Epoch: 23/27... Step: 31160... Loss: 1.1530... Val Loss: 1.3087\n",
      "Epoch: 23/27... Step: 31170... Loss: 1.0603... Val Loss: 1.3104\n",
      "Epoch: 23/27... Step: 31180... Loss: 1.0809... Val Loss: 1.3188\n",
      "Epoch: 23/27... Step: 31190... Loss: 1.1541... Val Loss: 1.3118\n",
      "Epoch: 23/27... Step: 31200... Loss: 1.1057... Val Loss: 1.3062\n",
      "Epoch: 23/27... Step: 31210... Loss: 1.1506... Val Loss: 1.3080\n",
      "Epoch: 23/27... Step: 31220... Loss: 1.0745... Val Loss: 1.3100\n",
      "Epoch: 23/27... Step: 31230... Loss: 1.0606... Val Loss: 1.3111\n",
      "Epoch: 23/27... Step: 31240... Loss: 1.0843... Val Loss: 1.3141\n",
      "Epoch: 23/27... Step: 31250... Loss: 1.0862... Val Loss: 1.3124\n",
      "Epoch: 23/27... Step: 31260... Loss: 1.2031... Val Loss: 1.3116\n",
      "Epoch: 23/27... Step: 31270... Loss: 1.0821... Val Loss: 1.3103\n",
      "Epoch: 23/27... Step: 31280... Loss: 1.1016... Val Loss: 1.3110\n",
      "Epoch: 23/27... Step: 31290... Loss: 1.0915... Val Loss: 1.3107\n",
      "Epoch: 23/27... Step: 31300... Loss: 1.0751... Val Loss: 1.3134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/27... Step: 31310... Loss: 1.1460... Val Loss: 1.3114\n",
      "Epoch: 23/27... Step: 31320... Loss: 1.1697... Val Loss: 1.3109\n",
      "Epoch: 23/27... Step: 31330... Loss: 1.1110... Val Loss: 1.3139\n",
      "Epoch: 23/27... Step: 31340... Loss: 1.0831... Val Loss: 1.3195\n",
      "Epoch: 23/27... Step: 31350... Loss: 1.1128... Val Loss: 1.3145\n",
      "Epoch: 23/27... Step: 31360... Loss: 1.0904... Val Loss: 1.3081\n",
      "Epoch: 23/27... Step: 31370... Loss: 1.1516... Val Loss: 1.3077\n",
      "Epoch: 23/27... Step: 31380... Loss: 1.1465... Val Loss: 1.3142\n",
      "Epoch: 23/27... Step: 31390... Loss: 1.1674... Val Loss: 1.3142\n",
      "Epoch: 23/27... Step: 31400... Loss: 1.0891... Val Loss: 1.3106\n",
      "Epoch: 23/27... Step: 31410... Loss: 1.1201... Val Loss: 1.3098\n",
      "Epoch: 23/27... Step: 31420... Loss: 1.0522... Val Loss: 1.3097\n",
      "Epoch: 23/27... Step: 31430... Loss: 1.0803... Val Loss: 1.3130\n",
      "Epoch: 23/27... Step: 31440... Loss: 1.1343... Val Loss: 1.3134\n",
      "Epoch: 23/27... Step: 31450... Loss: 1.1058... Val Loss: 1.3107\n",
      "Epoch: 23/27... Step: 31460... Loss: 1.0242... Val Loss: 1.3118\n",
      "Epoch: 23/27... Step: 31470... Loss: 1.1992... Val Loss: 1.3143\n",
      "Epoch: 23/27... Step: 31480... Loss: 1.1091... Val Loss: 1.3106\n",
      "Epoch: 23/27... Step: 31490... Loss: 1.0968... Val Loss: 1.3083\n",
      "Epoch: 23/27... Step: 31500... Loss: 1.0692... Val Loss: 1.3077\n",
      "Epoch: 23/27... Step: 31510... Loss: 1.1375... Val Loss: 1.3089\n",
      "Epoch: 23/27... Step: 31520... Loss: 1.1308... Val Loss: 1.3079\n",
      "Epoch: 23/27... Step: 31530... Loss: 1.0542... Val Loss: 1.3098\n",
      "Epoch: 23/27... Step: 31540... Loss: 1.1184... Val Loss: 1.3048\n",
      "Epoch: 23/27... Step: 31550... Loss: 1.0741... Val Loss: 1.3050\n",
      "Epoch: 23/27... Step: 31560... Loss: 1.1090... Val Loss: 1.3092\n",
      "Epoch: 23/27... Step: 31570... Loss: 1.1665... Val Loss: 1.3061\n",
      "Epoch: 23/27... Step: 31580... Loss: 1.1624... Val Loss: 1.3061\n",
      "Epoch: 23/27... Step: 31590... Loss: 1.1292... Val Loss: 1.3078\n",
      "Epoch: 23/27... Step: 31600... Loss: 1.1047... Val Loss: 1.3071\n",
      "Epoch: 23/27... Step: 31610... Loss: 1.1953... Val Loss: 1.3066\n",
      "Epoch: 23/27... Step: 31620... Loss: 1.1226... Val Loss: 1.3083\n",
      "Epoch: 23/27... Step: 31630... Loss: 1.0918... Val Loss: 1.3129\n",
      "Epoch: 23/27... Step: 31640... Loss: 1.1132... Val Loss: 1.3123\n",
      "Epoch: 23/27... Step: 31650... Loss: 1.1740... Val Loss: 1.3090\n",
      "Epoch: 23/27... Step: 31660... Loss: 1.0608... Val Loss: 1.3059\n",
      "Epoch: 23/27... Step: 31670... Loss: 1.0973... Val Loss: 1.3078\n",
      "Epoch: 23/27... Step: 31680... Loss: 1.1091... Val Loss: 1.3120\n",
      "Epoch: 23/27... Step: 31690... Loss: 1.1235... Val Loss: 1.3130\n",
      "Epoch: 23/27... Step: 31700... Loss: 1.1490... Val Loss: 1.3097\n",
      "Epoch: 23/27... Step: 31710... Loss: 1.1109... Val Loss: 1.3096\n",
      "Epoch: 23/27... Step: 31720... Loss: 1.0994... Val Loss: 1.3116\n",
      "Epoch: 23/27... Step: 31730... Loss: 1.0703... Val Loss: 1.3128\n",
      "Epoch: 23/27... Step: 31740... Loss: 1.0690... Val Loss: 1.3082\n",
      "Epoch: 23/27... Step: 31750... Loss: 1.0631... Val Loss: 1.3098\n",
      "Epoch: 23/27... Step: 31760... Loss: 1.1818... Val Loss: 1.3161\n",
      "Epoch: 23/27... Step: 31770... Loss: 1.1146... Val Loss: 1.3179\n",
      "Epoch: 23/27... Step: 31780... Loss: 1.0103... Val Loss: 1.3163\n",
      "Epoch: 23/27... Step: 31790... Loss: 1.1257... Val Loss: 1.3160\n",
      "Epoch: 23/27... Step: 31800... Loss: 1.1056... Val Loss: 1.3120\n",
      "Epoch: 23/27... Step: 31810... Loss: 1.0830... Val Loss: 1.3099\n",
      "Epoch: 23/27... Step: 31820... Loss: 1.0600... Val Loss: 1.3087\n",
      "Epoch: 23/27... Step: 31830... Loss: 1.0404... Val Loss: 1.3095\n",
      "Epoch: 23/27... Step: 31840... Loss: 1.0870... Val Loss: 1.3115\n",
      "Epoch: 23/27... Step: 31850... Loss: 1.1127... Val Loss: 1.3113\n",
      "Epoch: 23/27... Step: 31860... Loss: 1.1195... Val Loss: 1.3124\n",
      "Epoch: 23/27... Step: 31870... Loss: 1.1281... Val Loss: 1.3135\n",
      "Epoch: 23/27... Step: 31880... Loss: 1.1049... Val Loss: 1.3133\n",
      "Epoch: 23/27... Step: 31890... Loss: 1.1032... Val Loss: 1.3122\n",
      "Epoch: 23/27... Step: 31900... Loss: 1.0577... Val Loss: 1.3113\n",
      "Epoch: 23/27... Step: 31910... Loss: 1.0952... Val Loss: 1.3129\n",
      "Epoch: 23/27... Step: 31920... Loss: 1.0678... Val Loss: 1.3133\n",
      "Epoch: 23/27... Step: 31930... Loss: 1.0903... Val Loss: 1.3095\n",
      "Epoch: 23/27... Step: 31940... Loss: 1.0775... Val Loss: 1.3100\n",
      "Epoch: 23/27... Step: 31950... Loss: 1.0828... Val Loss: 1.3107\n",
      "Epoch: 23/27... Step: 31960... Loss: 1.1092... Val Loss: 1.3112\n",
      "Epoch: 23/27... Step: 31970... Loss: 1.0661... Val Loss: 1.3104\n",
      "Epoch: 23/27... Step: 31980... Loss: 1.1284... Val Loss: 1.3131\n",
      "Epoch: 23/27... Step: 31990... Loss: 1.1660... Val Loss: 1.3121\n",
      "Epoch: 23/27... Step: 32000... Loss: 1.1338... Val Loss: 1.3097\n",
      "Epoch: 23/27... Step: 32010... Loss: 1.0973... Val Loss: 1.3061\n",
      "Epoch: 23/27... Step: 32020... Loss: 1.0604... Val Loss: 1.3079\n",
      "Epoch: 23/27... Step: 32030... Loss: 1.1389... Val Loss: 1.3135\n",
      "Epoch: 23/27... Step: 32040... Loss: 1.1377... Val Loss: 1.3106\n",
      "Epoch: 23/27... Step: 32050... Loss: 1.0589... Val Loss: 1.3061\n",
      "Epoch: 23/27... Step: 32060... Loss: 1.1096... Val Loss: 1.3070\n",
      "Epoch: 23/27... Step: 32070... Loss: 1.1373... Val Loss: 1.3121\n",
      "Epoch: 23/27... Step: 32080... Loss: 1.1109... Val Loss: 1.3105\n",
      "Epoch: 24/27... Step: 32090... Loss: 1.1148... Val Loss: 1.3065\n",
      "Epoch: 24/27... Step: 32100... Loss: 1.0703... Val Loss: 1.3071\n",
      "Epoch: 24/27... Step: 32110... Loss: 1.1476... Val Loss: 1.3089\n",
      "Epoch: 24/27... Step: 32120... Loss: 1.0557... Val Loss: 1.3061\n",
      "Epoch: 24/27... Step: 32130... Loss: 1.0910... Val Loss: 1.3083\n",
      "Epoch: 24/27... Step: 32140... Loss: 1.0900... Val Loss: 1.3080\n",
      "Epoch: 24/27... Step: 32150... Loss: 1.1047... Val Loss: 1.3124\n",
      "Epoch: 24/27... Step: 32160... Loss: 1.0846... Val Loss: 1.3146\n",
      "Epoch: 24/27... Step: 32170... Loss: 1.0775... Val Loss: 1.3142\n",
      "Epoch: 24/27... Step: 32180... Loss: 1.1204... Val Loss: 1.3117\n",
      "Epoch: 24/27... Step: 32190... Loss: 1.0736... Val Loss: 1.3126\n",
      "Epoch: 24/27... Step: 32200... Loss: 1.1336... Val Loss: 1.3140\n",
      "Epoch: 24/27... Step: 32210... Loss: 1.1072... Val Loss: 1.3113\n",
      "Epoch: 24/27... Step: 32220... Loss: 1.0919... Val Loss: 1.3107\n",
      "Epoch: 24/27... Step: 32230... Loss: 1.1092... Val Loss: 1.3137\n",
      "Epoch: 24/27... Step: 32240... Loss: 1.1214... Val Loss: 1.3127\n",
      "Epoch: 24/27... Step: 32250... Loss: 1.0966... Val Loss: 1.3127\n",
      "Epoch: 24/27... Step: 32260... Loss: 1.0921... Val Loss: 1.3100\n",
      "Epoch: 24/27... Step: 32270... Loss: 1.1554... Val Loss: 1.3084\n",
      "Epoch: 24/27... Step: 32280... Loss: 1.1307... Val Loss: 1.3081\n",
      "Epoch: 24/27... Step: 32290... Loss: 1.0783... Val Loss: 1.3060\n",
      "Epoch: 24/27... Step: 32300... Loss: 1.1184... Val Loss: 1.3099\n",
      "Epoch: 24/27... Step: 32310... Loss: 1.0955... Val Loss: 1.3071\n",
      "Epoch: 24/27... Step: 32320... Loss: 1.0385... Val Loss: 1.3063\n",
      "Epoch: 24/27... Step: 32330... Loss: 1.0867... Val Loss: 1.3110\n",
      "Epoch: 24/27... Step: 32340... Loss: 1.1094... Val Loss: 1.3105\n",
      "Epoch: 24/27... Step: 32350... Loss: 1.1023... Val Loss: 1.3112\n",
      "Epoch: 24/27... Step: 32360... Loss: 1.1102... Val Loss: 1.3103\n",
      "Epoch: 24/27... Step: 32370... Loss: 1.1067... Val Loss: 1.3089\n",
      "Epoch: 24/27... Step: 32380... Loss: 1.1537... Val Loss: 1.3088\n",
      "Epoch: 24/27... Step: 32390... Loss: 1.0857... Val Loss: 1.3106\n",
      "Epoch: 24/27... Step: 32400... Loss: 1.0919... Val Loss: 1.3119\n",
      "Epoch: 24/27... Step: 32410... Loss: 1.0698... Val Loss: 1.3129\n",
      "Epoch: 24/27... Step: 32420... Loss: 1.0980... Val Loss: 1.3090\n",
      "Epoch: 24/27... Step: 32430... Loss: 1.1174... Val Loss: 1.3041\n",
      "Epoch: 24/27... Step: 32440... Loss: 1.1185... Val Loss: 1.3023\n",
      "Epoch: 24/27... Step: 32450... Loss: 1.1454... Val Loss: 1.3031\n",
      "Epoch: 24/27... Step: 32460... Loss: 1.1818... Val Loss: 1.3031\n",
      "Epoch: 24/27... Step: 32470... Loss: 1.0834... Val Loss: 1.3029\n",
      "Epoch: 24/27... Step: 32480... Loss: 1.1057... Val Loss: 1.3057\n",
      "Epoch: 24/27... Step: 32490... Loss: 1.1307... Val Loss: 1.3078\n",
      "Epoch: 24/27... Step: 32500... Loss: 1.1123... Val Loss: 1.3077\n",
      "Epoch: 24/27... Step: 32510... Loss: 1.0435... Val Loss: 1.3044\n",
      "Epoch: 24/27... Step: 32520... Loss: 1.1454... Val Loss: 1.3055\n",
      "Epoch: 24/27... Step: 32530... Loss: 1.0560... Val Loss: 1.3076\n",
      "Epoch: 24/27... Step: 32540... Loss: 1.1037... Val Loss: 1.3067\n",
      "Epoch: 24/27... Step: 32550... Loss: 1.1156... Val Loss: 1.3050\n",
      "Epoch: 24/27... Step: 32560... Loss: 1.0644... Val Loss: 1.3052\n",
      "Epoch: 24/27... Step: 32570... Loss: 1.0843... Val Loss: 1.3072\n",
      "Epoch: 24/27... Step: 32580... Loss: 1.1168... Val Loss: 1.3127\n",
      "Epoch: 24/27... Step: 32590... Loss: 1.0730... Val Loss: 1.3087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/27... Step: 32600... Loss: 1.0990... Val Loss: 1.3043\n",
      "Epoch: 24/27... Step: 32610... Loss: 1.0996... Val Loss: 1.3036\n",
      "Epoch: 24/27... Step: 32620... Loss: 1.0646... Val Loss: 1.3052\n",
      "Epoch: 24/27... Step: 32630... Loss: 1.0499... Val Loss: 1.3120\n",
      "Epoch: 24/27... Step: 32640... Loss: 1.0414... Val Loss: 1.3161\n",
      "Epoch: 24/27... Step: 32650... Loss: 1.0384... Val Loss: 1.3127\n",
      "Epoch: 24/27... Step: 32660... Loss: 1.0393... Val Loss: 1.3085\n",
      "Epoch: 24/27... Step: 32670... Loss: 1.0911... Val Loss: 1.3095\n",
      "Epoch: 24/27... Step: 32680... Loss: 1.0532... Val Loss: 1.3143\n",
      "Epoch: 24/27... Step: 32690... Loss: 1.1075... Val Loss: 1.3140\n",
      "Epoch: 24/27... Step: 32700... Loss: 1.0686... Val Loss: 1.3108\n",
      "Epoch: 24/27... Step: 32710... Loss: 1.1164... Val Loss: 1.3104\n",
      "Epoch: 24/27... Step: 32720... Loss: 1.0599... Val Loss: 1.3102\n",
      "Epoch: 24/27... Step: 32730... Loss: 1.1664... Val Loss: 1.3126\n",
      "Epoch: 24/27... Step: 32740... Loss: 1.1175... Val Loss: 1.3177\n",
      "Epoch: 24/27... Step: 32750... Loss: 1.1084... Val Loss: 1.3132\n",
      "Epoch: 24/27... Step: 32760... Loss: 1.1212... Val Loss: 1.3094\n",
      "Epoch: 24/27... Step: 32770... Loss: 1.0898... Val Loss: 1.3102\n",
      "Epoch: 24/27... Step: 32780... Loss: 1.1000... Val Loss: 1.3106\n",
      "Epoch: 24/27... Step: 32790... Loss: 1.1054... Val Loss: 1.3097\n",
      "Epoch: 24/27... Step: 32800... Loss: 1.1454... Val Loss: 1.3137\n",
      "Epoch: 24/27... Step: 32810... Loss: 1.0980... Val Loss: 1.3134\n",
      "Epoch: 24/27... Step: 32820... Loss: 1.0763... Val Loss: 1.3146\n",
      "Epoch: 24/27... Step: 32830... Loss: 1.1606... Val Loss: 1.3142\n",
      "Epoch: 24/27... Step: 32840... Loss: 1.0473... Val Loss: 1.3126\n",
      "Epoch: 24/27... Step: 32850... Loss: 1.1059... Val Loss: 1.3126\n",
      "Epoch: 24/27... Step: 32860... Loss: 1.1053... Val Loss: 1.3125\n",
      "Epoch: 24/27... Step: 32870... Loss: 1.1507... Val Loss: 1.3106\n",
      "Epoch: 24/27... Step: 32880... Loss: 1.1198... Val Loss: 1.3081\n",
      "Epoch: 24/27... Step: 32890... Loss: 1.0801... Val Loss: 1.3094\n",
      "Epoch: 24/27... Step: 32900... Loss: 1.1306... Val Loss: 1.3068\n",
      "Epoch: 24/27... Step: 32910... Loss: 1.0698... Val Loss: 1.3079\n",
      "Epoch: 24/27... Step: 32920... Loss: 1.0791... Val Loss: 1.3082\n",
      "Epoch: 24/27... Step: 32930... Loss: 1.1847... Val Loss: 1.3077\n",
      "Epoch: 24/27... Step: 32940... Loss: 1.0651... Val Loss: 1.3017\n",
      "Epoch: 24/27... Step: 32950... Loss: 1.0767... Val Loss: 1.3047\n",
      "Epoch: 24/27... Step: 32960... Loss: 1.1339... Val Loss: 1.3060\n",
      "Epoch: 24/27... Step: 32970... Loss: 1.1573... Val Loss: 1.3081\n",
      "Epoch: 24/27... Step: 32980... Loss: 1.0564... Val Loss: 1.3094\n",
      "Epoch: 24/27... Step: 32990... Loss: 1.0891... Val Loss: 1.3092\n",
      "Epoch: 24/27... Step: 33000... Loss: 1.0617... Val Loss: 1.3056\n",
      "Epoch: 24/27... Step: 33010... Loss: 1.0851... Val Loss: 1.3042\n",
      "Epoch: 24/27... Step: 33020... Loss: 1.1158... Val Loss: 1.3052\n",
      "Epoch: 24/27... Step: 33030... Loss: 1.1456... Val Loss: 1.3079\n",
      "Epoch: 24/27... Step: 33040... Loss: 1.0997... Val Loss: 1.3085\n",
      "Epoch: 24/27... Step: 33050... Loss: 1.1343... Val Loss: 1.3068\n",
      "Epoch: 24/27... Step: 33060... Loss: 1.1299... Val Loss: 1.3086\n",
      "Epoch: 24/27... Step: 33070... Loss: 1.1495... Val Loss: 1.3093\n",
      "Epoch: 24/27... Step: 33080... Loss: 1.1178... Val Loss: 1.3114\n",
      "Epoch: 24/27... Step: 33090... Loss: 1.1008... Val Loss: 1.3103\n",
      "Epoch: 24/27... Step: 33100... Loss: 1.0815... Val Loss: 1.3109\n",
      "Epoch: 24/27... Step: 33110... Loss: 1.1131... Val Loss: 1.3112\n",
      "Epoch: 24/27... Step: 33120... Loss: 1.1353... Val Loss: 1.3097\n",
      "Epoch: 24/27... Step: 33130... Loss: 1.1419... Val Loss: 1.3066\n",
      "Epoch: 24/27... Step: 33140... Loss: 1.0979... Val Loss: 1.3051\n",
      "Epoch: 24/27... Step: 33150... Loss: 1.1096... Val Loss: 1.3111\n",
      "Epoch: 24/27... Step: 33160... Loss: 1.0734... Val Loss: 1.3137\n",
      "Epoch: 24/27... Step: 33170... Loss: 1.1137... Val Loss: 1.3123\n",
      "Epoch: 24/27... Step: 33180... Loss: 1.0451... Val Loss: 1.3127\n",
      "Epoch: 24/27... Step: 33190... Loss: 1.0690... Val Loss: 1.3082\n",
      "Epoch: 24/27... Step: 33200... Loss: 1.0834... Val Loss: 1.3062\n",
      "Epoch: 24/27... Step: 33210... Loss: 1.0768... Val Loss: 1.3077\n",
      "Epoch: 24/27... Step: 33220... Loss: 1.0729... Val Loss: 1.3100\n",
      "Epoch: 24/27... Step: 33230... Loss: 1.0888... Val Loss: 1.3098\n",
      "Epoch: 24/27... Step: 33240... Loss: 1.0792... Val Loss: 1.3080\n",
      "Epoch: 24/27... Step: 33250... Loss: 1.0968... Val Loss: 1.3098\n",
      "Epoch: 24/27... Step: 33260... Loss: 1.0760... Val Loss: 1.3095\n",
      "Epoch: 24/27... Step: 33270... Loss: 1.1140... Val Loss: 1.3112\n",
      "Epoch: 24/27... Step: 33280... Loss: 1.1166... Val Loss: 1.3128\n",
      "Epoch: 24/27... Step: 33290... Loss: 1.0496... Val Loss: 1.3106\n",
      "Epoch: 24/27... Step: 33300... Loss: 1.0362... Val Loss: 1.3101\n",
      "Epoch: 24/27... Step: 33310... Loss: 1.1642... Val Loss: 1.3118\n",
      "Epoch: 24/27... Step: 33320... Loss: 1.1237... Val Loss: 1.3109\n",
      "Epoch: 24/27... Step: 33330... Loss: 1.1365... Val Loss: 1.3085\n",
      "Epoch: 24/27... Step: 33340... Loss: 1.1435... Val Loss: 1.3083\n",
      "Epoch: 24/27... Step: 33350... Loss: 1.1109... Val Loss: 1.3098\n",
      "Epoch: 24/27... Step: 33360... Loss: 1.1651... Val Loss: 1.3122\n",
      "Epoch: 24/27... Step: 33370... Loss: 1.0710... Val Loss: 1.3109\n",
      "Epoch: 24/27... Step: 33380... Loss: 1.0373... Val Loss: 1.3075\n",
      "Epoch: 24/27... Step: 33390... Loss: 1.1278... Val Loss: 1.3103\n",
      "Epoch: 24/27... Step: 33400... Loss: 1.1254... Val Loss: 1.3110\n",
      "Epoch: 24/27... Step: 33410... Loss: 1.0104... Val Loss: 1.3112\n",
      "Epoch: 24/27... Step: 33420... Loss: 1.1531... Val Loss: 1.3129\n",
      "Epoch: 24/27... Step: 33430... Loss: 1.0815... Val Loss: 1.3136\n",
      "Epoch: 24/27... Step: 33440... Loss: 1.0698... Val Loss: 1.3098\n",
      "Epoch: 24/27... Step: 33450... Loss: 1.0394... Val Loss: 1.3068\n",
      "Epoch: 24/27... Step: 33460... Loss: 1.0687... Val Loss: 1.3067\n",
      "Epoch: 24/27... Step: 33470... Loss: 1.0863... Val Loss: 1.3089\n",
      "Epoch: 24/27... Step: 33480... Loss: 1.7284... Val Loss: 1.3077\n",
      "Epoch: 25/27... Step: 33490... Loss: 1.1348... Val Loss: 1.3075\n",
      "Epoch: 25/27... Step: 33500... Loss: 1.0712... Val Loss: 1.3117\n",
      "Epoch: 25/27... Step: 33510... Loss: 1.1144... Val Loss: 1.3085\n",
      "Epoch: 25/27... Step: 33520... Loss: 1.1059... Val Loss: 1.3065\n",
      "Epoch: 25/27... Step: 33530... Loss: 1.1059... Val Loss: 1.3040\n",
      "Epoch: 25/27... Step: 33540... Loss: 1.1194... Val Loss: 1.3055\n",
      "Epoch: 25/27... Step: 33550... Loss: 1.0762... Val Loss: 1.3110\n",
      "Epoch: 25/27... Step: 33560... Loss: 1.0913... Val Loss: 1.3121\n",
      "Epoch: 25/27... Step: 33570... Loss: 1.0909... Val Loss: 1.3071\n",
      "Epoch: 25/27... Step: 33580... Loss: 1.1145... Val Loss: 1.3054\n",
      "Epoch: 25/27... Step: 33590... Loss: 1.1712... Val Loss: 1.3109\n",
      "Epoch: 25/27... Step: 33600... Loss: 1.1377... Val Loss: 1.3130\n",
      "Epoch: 25/27... Step: 33610... Loss: 1.1356... Val Loss: 1.3072\n",
      "Epoch: 25/27... Step: 33620... Loss: 1.0685... Val Loss: 1.3068\n",
      "Epoch: 25/27... Step: 33630... Loss: 1.0999... Val Loss: 1.3093\n",
      "Epoch: 25/27... Step: 33640... Loss: 1.0527... Val Loss: 1.3101\n",
      "Epoch: 25/27... Step: 33650... Loss: 1.1139... Val Loss: 1.3091\n",
      "Epoch: 25/27... Step: 33660... Loss: 1.1561... Val Loss: 1.3097\n",
      "Epoch: 25/27... Step: 33670... Loss: 1.0855... Val Loss: 1.3108\n",
      "Epoch: 25/27... Step: 33680... Loss: 1.0421... Val Loss: 1.3053\n",
      "Epoch: 25/27... Step: 33690... Loss: 1.0706... Val Loss: 1.3054\n",
      "Epoch: 25/27... Step: 33700... Loss: 1.1252... Val Loss: 1.3080\n",
      "Epoch: 25/27... Step: 33710... Loss: 1.0907... Val Loss: 1.3065\n",
      "Epoch: 25/27... Step: 33720... Loss: 1.0801... Val Loss: 1.3063\n",
      "Epoch: 25/27... Step: 33730... Loss: 1.0479... Val Loss: 1.3081\n",
      "Epoch: 25/27... Step: 33740... Loss: 1.0579... Val Loss: 1.3106\n",
      "Epoch: 25/27... Step: 33750... Loss: 0.9597... Val Loss: 1.3110\n",
      "Epoch: 25/27... Step: 33760... Loss: 1.1038... Val Loss: 1.3117\n",
      "Epoch: 25/27... Step: 33770... Loss: 1.0826... Val Loss: 1.3070\n",
      "Epoch: 25/27... Step: 33780... Loss: 1.1656... Val Loss: 1.3093\n",
      "Epoch: 25/27... Step: 33790... Loss: 1.0808... Val Loss: 1.3137\n",
      "Epoch: 25/27... Step: 33800... Loss: 1.0598... Val Loss: 1.3095\n",
      "Epoch: 25/27... Step: 33810... Loss: 1.0966... Val Loss: 1.3086\n",
      "Epoch: 25/27... Step: 33820... Loss: 1.1153... Val Loss: 1.3094\n",
      "Epoch: 25/27... Step: 33830... Loss: 1.1268... Val Loss: 1.3113\n",
      "Epoch: 25/27... Step: 33840... Loss: 1.1091... Val Loss: 1.3096\n",
      "Epoch: 25/27... Step: 33850... Loss: 1.1494... Val Loss: 1.3055\n",
      "Epoch: 25/27... Step: 33860... Loss: 1.1460... Val Loss: 1.3056\n",
      "Epoch: 25/27... Step: 33870... Loss: 1.0909... Val Loss: 1.3093\n",
      "Epoch: 25/27... Step: 33880... Loss: 1.0622... Val Loss: 1.3123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/27... Step: 33890... Loss: 1.1154... Val Loss: 1.3116\n",
      "Epoch: 25/27... Step: 33900... Loss: 1.1135... Val Loss: 1.3085\n",
      "Epoch: 25/27... Step: 33910... Loss: 1.1174... Val Loss: 1.3077\n",
      "Epoch: 25/27... Step: 33920... Loss: 1.1591... Val Loss: 1.3095\n",
      "Epoch: 25/27... Step: 33930... Loss: 1.1332... Val Loss: 1.3058\n",
      "Epoch: 25/27... Step: 33940... Loss: 1.1494... Val Loss: 1.3039\n",
      "Epoch: 25/27... Step: 33950... Loss: 1.1757... Val Loss: 1.3042\n",
      "Epoch: 25/27... Step: 33960... Loss: 1.0713... Val Loss: 1.3087\n",
      "Epoch: 25/27... Step: 33970... Loss: 1.0347... Val Loss: 1.3108\n",
      "Epoch: 25/27... Step: 33980... Loss: 1.1171... Val Loss: 1.3106\n",
      "Epoch: 25/27... Step: 33990... Loss: 1.0919... Val Loss: 1.3057\n",
      "Epoch: 25/27... Step: 34000... Loss: 1.1430... Val Loss: 1.3065\n",
      "Epoch: 25/27... Step: 34010... Loss: 1.0496... Val Loss: 1.3061\n",
      "Epoch: 25/27... Step: 34020... Loss: 1.0558... Val Loss: 1.3101\n",
      "Epoch: 25/27... Step: 34030... Loss: 1.1164... Val Loss: 1.3150\n",
      "Epoch: 25/27... Step: 34040... Loss: 1.0985... Val Loss: 1.3134\n",
      "Epoch: 25/27... Step: 34050... Loss: 1.1914... Val Loss: 1.3093\n",
      "Epoch: 25/27... Step: 34060... Loss: 1.1175... Val Loss: 1.3089\n",
      "Epoch: 25/27... Step: 34070... Loss: 1.1024... Val Loss: 1.3132\n",
      "Epoch: 25/27... Step: 34080... Loss: 1.1033... Val Loss: 1.3162\n",
      "Epoch: 25/27... Step: 34090... Loss: 1.1042... Val Loss: 1.3137\n",
      "Epoch: 25/27... Step: 34100... Loss: 1.1345... Val Loss: 1.3116\n",
      "Epoch: 25/27... Step: 34110... Loss: 1.1210... Val Loss: 1.3087\n",
      "Epoch: 25/27... Step: 34120... Loss: 1.1322... Val Loss: 1.3132\n",
      "Epoch: 25/27... Step: 34130... Loss: 1.0545... Val Loss: 1.3175\n",
      "Epoch: 25/27... Step: 34140... Loss: 1.1087... Val Loss: 1.3151\n",
      "Epoch: 25/27... Step: 34150... Loss: 1.1088... Val Loss: 1.3074\n",
      "Epoch: 25/27... Step: 34160... Loss: 1.1391... Val Loss: 1.3054\n",
      "Epoch: 25/27... Step: 34170... Loss: 1.1099... Val Loss: 1.3077\n",
      "Epoch: 25/27... Step: 34180... Loss: 1.1603... Val Loss: 1.3078\n",
      "Epoch: 25/27... Step: 34190... Loss: 1.0723... Val Loss: 1.3080\n",
      "Epoch: 25/27... Step: 34200... Loss: 1.0986... Val Loss: 1.3092\n",
      "Epoch: 25/27... Step: 34210... Loss: 1.0497... Val Loss: 1.3109\n",
      "Epoch: 25/27... Step: 34220... Loss: 1.0658... Val Loss: 1.3142\n",
      "Epoch: 25/27... Step: 34230... Loss: 1.0889... Val Loss: 1.3142\n",
      "Epoch: 25/27... Step: 34240... Loss: 1.0933... Val Loss: 1.3139\n",
      "Epoch: 25/27... Step: 34250... Loss: 1.0251... Val Loss: 1.3129\n",
      "Epoch: 25/27... Step: 34260... Loss: 1.1772... Val Loss: 1.3119\n",
      "Epoch: 25/27... Step: 34270... Loss: 1.1057... Val Loss: 1.3088\n",
      "Epoch: 25/27... Step: 34280... Loss: 1.0885... Val Loss: 1.3083\n",
      "Epoch: 25/27... Step: 34290... Loss: 1.0933... Val Loss: 1.3077\n",
      "Epoch: 25/27... Step: 34300... Loss: 1.1238... Val Loss: 1.3099\n",
      "Epoch: 25/27... Step: 34310... Loss: 1.1408... Val Loss: 1.3117\n",
      "Epoch: 25/27... Step: 34320... Loss: 1.0720... Val Loss: 1.3142\n",
      "Epoch: 25/27... Step: 34330... Loss: 1.1210... Val Loss: 1.3050\n",
      "Epoch: 25/27... Step: 34340... Loss: 1.1045... Val Loss: 1.3028\n",
      "Epoch: 25/27... Step: 34350... Loss: 1.0847... Val Loss: 1.3093\n",
      "Epoch: 25/27... Step: 34360... Loss: 1.1938... Val Loss: 1.3105\n",
      "Epoch: 25/27... Step: 34370... Loss: 1.1303... Val Loss: 1.3077\n",
      "Epoch: 25/27... Step: 34380... Loss: 1.0981... Val Loss: 1.3083\n",
      "Epoch: 25/27... Step: 34390... Loss: 1.1353... Val Loss: 1.3078\n",
      "Epoch: 25/27... Step: 34400... Loss: 1.1933... Val Loss: 1.3100\n",
      "Epoch: 25/27... Step: 34410... Loss: 1.0879... Val Loss: 1.3075\n",
      "Epoch: 25/27... Step: 34420... Loss: 1.0881... Val Loss: 1.3094\n",
      "Epoch: 25/27... Step: 34430... Loss: 1.0889... Val Loss: 1.3124\n",
      "Epoch: 25/27... Step: 34440... Loss: 1.1469... Val Loss: 1.3097\n",
      "Epoch: 25/27... Step: 34450... Loss: 1.0230... Val Loss: 1.3043\n",
      "Epoch: 25/27... Step: 34460... Loss: 1.0909... Val Loss: 1.3097\n",
      "Epoch: 25/27... Step: 34470... Loss: 1.1137... Val Loss: 1.3118\n",
      "Epoch: 25/27... Step: 34480... Loss: 1.0716... Val Loss: 1.3121\n",
      "Epoch: 25/27... Step: 34490... Loss: 1.1428... Val Loss: 1.3121\n",
      "Epoch: 25/27... Step: 34500... Loss: 1.1249... Val Loss: 1.3147\n",
      "Epoch: 25/27... Step: 34510... Loss: 1.1182... Val Loss: 1.3150\n",
      "Epoch: 25/27... Step: 34520... Loss: 1.0376... Val Loss: 1.3119\n",
      "Epoch: 25/27... Step: 34530... Loss: 1.0385... Val Loss: 1.3101\n",
      "Epoch: 25/27... Step: 34540... Loss: 1.0772... Val Loss: 1.3104\n",
      "Epoch: 25/27... Step: 34550... Loss: 1.1553... Val Loss: 1.3155\n",
      "Epoch: 25/27... Step: 34560... Loss: 1.1076... Val Loss: 1.3145\n",
      "Epoch: 25/27... Step: 34570... Loss: 1.0182... Val Loss: 1.3150\n",
      "Epoch: 25/27... Step: 34580... Loss: 1.1089... Val Loss: 1.3148\n",
      "Epoch: 25/27... Step: 34590... Loss: 1.1100... Val Loss: 1.3112\n",
      "Epoch: 25/27... Step: 34600... Loss: 1.0893... Val Loss: 1.3099\n",
      "Epoch: 25/27... Step: 34610... Loss: 1.0495... Val Loss: 1.3091\n",
      "Epoch: 25/27... Step: 34620... Loss: 1.0501... Val Loss: 1.3107\n",
      "Epoch: 25/27... Step: 34630... Loss: 1.0873... Val Loss: 1.3127\n",
      "Epoch: 25/27... Step: 34640... Loss: 1.1060... Val Loss: 1.3132\n",
      "Epoch: 25/27... Step: 34650... Loss: 1.1381... Val Loss: 1.3113\n",
      "Epoch: 25/27... Step: 34660... Loss: 1.1235... Val Loss: 1.3088\n",
      "Epoch: 25/27... Step: 34670... Loss: 1.1014... Val Loss: 1.3093\n",
      "Epoch: 25/27... Step: 34680... Loss: 1.1225... Val Loss: 1.3107\n",
      "Epoch: 25/27... Step: 34690... Loss: 1.0558... Val Loss: 1.3099\n",
      "Epoch: 25/27... Step: 34700... Loss: 1.0730... Val Loss: 1.3104\n",
      "Epoch: 25/27... Step: 34710... Loss: 1.1007... Val Loss: 1.3104\n",
      "Epoch: 25/27... Step: 34720... Loss: 1.0697... Val Loss: 1.3129\n",
      "Epoch: 25/27... Step: 34730... Loss: 1.0952... Val Loss: 1.3113\n",
      "Epoch: 25/27... Step: 34740... Loss: 1.0856... Val Loss: 1.3092\n",
      "Epoch: 25/27... Step: 34750... Loss: 1.1146... Val Loss: 1.3111\n",
      "Epoch: 25/27... Step: 34760... Loss: 1.0637... Val Loss: 1.3102\n",
      "Epoch: 25/27... Step: 34770... Loss: 1.1313... Val Loss: 1.3111\n",
      "Epoch: 25/27... Step: 34780... Loss: 1.1406... Val Loss: 1.3113\n",
      "Epoch: 25/27... Step: 34790... Loss: 1.1382... Val Loss: 1.3107\n",
      "Epoch: 25/27... Step: 34800... Loss: 1.0976... Val Loss: 1.3055\n",
      "Epoch: 25/27... Step: 34810... Loss: 1.0620... Val Loss: 1.3071\n",
      "Epoch: 25/27... Step: 34820... Loss: 1.1125... Val Loss: 1.3104\n",
      "Epoch: 25/27... Step: 34830... Loss: 1.1433... Val Loss: 1.3075\n",
      "Epoch: 25/27... Step: 34840... Loss: 1.0568... Val Loss: 1.3067\n",
      "Epoch: 25/27... Step: 34850... Loss: 1.1050... Val Loss: 1.3091\n",
      "Epoch: 25/27... Step: 34860... Loss: 1.1708... Val Loss: 1.3097\n",
      "Epoch: 25/27... Step: 34870... Loss: 1.1411... Val Loss: 1.3069\n",
      "Epoch: 26/27... Step: 34880... Loss: 1.1135... Val Loss: 1.3083\n",
      "Epoch: 26/27... Step: 34890... Loss: 1.0919... Val Loss: 1.3120\n",
      "Epoch: 26/27... Step: 34900... Loss: 1.1107... Val Loss: 1.3089\n",
      "Epoch: 26/27... Step: 34910... Loss: 1.0530... Val Loss: 1.3093\n",
      "Epoch: 26/27... Step: 34920... Loss: 1.0848... Val Loss: 1.3116\n",
      "Epoch: 26/27... Step: 34930... Loss: 1.0675... Val Loss: 1.3098\n",
      "Epoch: 26/27... Step: 34940... Loss: 1.1054... Val Loss: 1.3124\n",
      "Epoch: 26/27... Step: 34950... Loss: 1.0946... Val Loss: 1.3159\n",
      "Epoch: 26/27... Step: 34960... Loss: 1.0658... Val Loss: 1.3135\n",
      "Epoch: 26/27... Step: 34970... Loss: 1.1224... Val Loss: 1.3116\n",
      "Epoch: 26/27... Step: 34980... Loss: 1.0850... Val Loss: 1.3119\n",
      "Epoch: 26/27... Step: 34990... Loss: 1.1178... Val Loss: 1.3125\n",
      "Epoch: 26/27... Step: 35000... Loss: 1.0951... Val Loss: 1.3113\n",
      "Epoch: 26/27... Step: 35010... Loss: 1.0757... Val Loss: 1.3096\n",
      "Epoch: 26/27... Step: 35020... Loss: 1.0927... Val Loss: 1.3133\n",
      "Epoch: 26/27... Step: 35030... Loss: 1.1397... Val Loss: 1.3144\n",
      "Epoch: 26/27... Step: 35040... Loss: 1.0829... Val Loss: 1.3091\n",
      "Epoch: 26/27... Step: 35050... Loss: 1.0885... Val Loss: 1.3083\n",
      "Epoch: 26/27... Step: 35060... Loss: 1.1531... Val Loss: 1.3093\n",
      "Epoch: 26/27... Step: 35070... Loss: 1.1327... Val Loss: 1.3071\n",
      "Epoch: 26/27... Step: 35080... Loss: 1.0748... Val Loss: 1.3067\n",
      "Epoch: 26/27... Step: 35090... Loss: 1.0931... Val Loss: 1.3082\n",
      "Epoch: 26/27... Step: 35100... Loss: 1.0763... Val Loss: 1.3061\n",
      "Epoch: 26/27... Step: 35110... Loss: 1.0323... Val Loss: 1.3076\n",
      "Epoch: 26/27... Step: 35120... Loss: 1.0640... Val Loss: 1.3078\n",
      "Epoch: 26/27... Step: 35130... Loss: 1.0941... Val Loss: 1.3089\n",
      "Epoch: 26/27... Step: 35140... Loss: 1.0971... Val Loss: 1.3109\n",
      "Epoch: 26/27... Step: 35150... Loss: 1.1300... Val Loss: 1.3123\n",
      "Epoch: 26/27... Step: 35160... Loss: 1.0838... Val Loss: 1.3104\n",
      "Epoch: 26/27... Step: 35170... Loss: 1.1175... Val Loss: 1.3098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/27... Step: 35180... Loss: 1.0655... Val Loss: 1.3120\n",
      "Epoch: 26/27... Step: 35190... Loss: 1.0927... Val Loss: 1.3155\n",
      "Epoch: 26/27... Step: 35200... Loss: 1.0587... Val Loss: 1.3149\n",
      "Epoch: 26/27... Step: 35210... Loss: 1.0707... Val Loss: 1.3097\n",
      "Epoch: 26/27... Step: 35220... Loss: 1.0836... Val Loss: 1.3070\n",
      "Epoch: 26/27... Step: 35230... Loss: 1.0734... Val Loss: 1.3053\n",
      "Epoch: 26/27... Step: 35240... Loss: 1.1127... Val Loss: 1.3049\n",
      "Epoch: 26/27... Step: 35250... Loss: 1.1390... Val Loss: 1.3053\n",
      "Epoch: 26/27... Step: 35260... Loss: 1.0695... Val Loss: 1.3062\n",
      "Epoch: 26/27... Step: 35270... Loss: 1.0887... Val Loss: 1.3124\n",
      "Epoch: 26/27... Step: 35280... Loss: 1.1316... Val Loss: 1.3145\n",
      "Epoch: 26/27... Step: 35290... Loss: 1.0941... Val Loss: 1.3116\n",
      "Epoch: 26/27... Step: 35300... Loss: 1.0382... Val Loss: 1.3060\n",
      "Epoch: 26/27... Step: 35310... Loss: 1.1510... Val Loss: 1.3059\n",
      "Epoch: 26/27... Step: 35320... Loss: 1.0717... Val Loss: 1.3089\n",
      "Epoch: 26/27... Step: 35330... Loss: 1.0726... Val Loss: 1.3083\n",
      "Epoch: 26/27... Step: 35340... Loss: 1.1430... Val Loss: 1.3057\n",
      "Epoch: 26/27... Step: 35350... Loss: 1.0622... Val Loss: 1.3037\n",
      "Epoch: 26/27... Step: 35360... Loss: 1.0873... Val Loss: 1.3074\n",
      "Epoch: 26/27... Step: 35370... Loss: 1.1085... Val Loss: 1.3121\n",
      "Epoch: 26/27... Step: 35380... Loss: 1.0747... Val Loss: 1.3074\n",
      "Epoch: 26/27... Step: 35390... Loss: 1.1050... Val Loss: 1.3060\n",
      "Epoch: 26/27... Step: 35400... Loss: 1.1106... Val Loss: 1.3041\n",
      "Epoch: 26/27... Step: 35410... Loss: 1.0502... Val Loss: 1.3065\n",
      "Epoch: 26/27... Step: 35420... Loss: 1.0426... Val Loss: 1.3116\n",
      "Epoch: 26/27... Step: 35430... Loss: 1.0388... Val Loss: 1.3138\n",
      "Epoch: 26/27... Step: 35440... Loss: 1.0164... Val Loss: 1.3114\n",
      "Epoch: 26/27... Step: 35450... Loss: 1.0450... Val Loss: 1.3083\n",
      "Epoch: 26/27... Step: 35460... Loss: 1.0827... Val Loss: 1.3104\n",
      "Epoch: 26/27... Step: 35470... Loss: 1.0458... Val Loss: 1.3134\n",
      "Epoch: 26/27... Step: 35480... Loss: 1.1036... Val Loss: 1.3160\n",
      "Epoch: 26/27... Step: 35490... Loss: 1.0641... Val Loss: 1.3142\n",
      "Epoch: 26/27... Step: 35500... Loss: 1.1114... Val Loss: 1.3101\n",
      "Epoch: 26/27... Step: 35510... Loss: 1.0632... Val Loss: 1.3099\n",
      "Epoch: 26/27... Step: 35520... Loss: 1.1511... Val Loss: 1.3153\n",
      "Epoch: 26/27... Step: 35530... Loss: 1.0871... Val Loss: 1.3157\n",
      "Epoch: 26/27... Step: 35540... Loss: 1.1155... Val Loss: 1.3100\n",
      "Epoch: 26/27... Step: 35550... Loss: 1.1202... Val Loss: 1.3067\n",
      "Epoch: 26/27... Step: 35560... Loss: 1.1073... Val Loss: 1.3079\n",
      "Epoch: 26/27... Step: 35570... Loss: 1.0990... Val Loss: 1.3080\n",
      "Epoch: 26/27... Step: 35580... Loss: 1.1068... Val Loss: 1.3081\n",
      "Epoch: 26/27... Step: 35590... Loss: 1.1157... Val Loss: 1.3067\n",
      "Epoch: 26/27... Step: 35600... Loss: 1.0810... Val Loss: 1.3070\n",
      "Epoch: 26/27... Step: 35610... Loss: 1.0798... Val Loss: 1.3110\n",
      "Epoch: 26/27... Step: 35620... Loss: 1.1258... Val Loss: 1.3110\n",
      "Epoch: 26/27... Step: 35630... Loss: 1.0581... Val Loss: 1.3108\n",
      "Epoch: 26/27... Step: 35640... Loss: 1.0761... Val Loss: 1.3086\n",
      "Epoch: 26/27... Step: 35650... Loss: 1.1091... Val Loss: 1.3105\n",
      "Epoch: 26/27... Step: 35660... Loss: 1.1195... Val Loss: 1.3101\n",
      "Epoch: 26/27... Step: 35670... Loss: 1.1389... Val Loss: 1.3080\n",
      "Epoch: 26/27... Step: 35680... Loss: 1.0564... Val Loss: 1.3058\n",
      "Epoch: 26/27... Step: 35690... Loss: 1.0715... Val Loss: 1.3059\n",
      "Epoch: 26/27... Step: 35700... Loss: 1.0631... Val Loss: 1.3078\n",
      "Epoch: 26/27... Step: 35710... Loss: 1.0593... Val Loss: 1.3121\n",
      "Epoch: 26/27... Step: 35720... Loss: 1.1514... Val Loss: 1.3099\n",
      "Epoch: 26/27... Step: 35730... Loss: 1.0570... Val Loss: 1.3030\n",
      "Epoch: 26/27... Step: 35740... Loss: 1.0910... Val Loss: 1.3044\n",
      "Epoch: 26/27... Step: 35750... Loss: 1.0906... Val Loss: 1.3069\n",
      "Epoch: 26/27... Step: 35760... Loss: 1.1542... Val Loss: 1.3081\n",
      "Epoch: 26/27... Step: 35770... Loss: 1.0475... Val Loss: 1.3079\n",
      "Epoch: 26/27... Step: 35780... Loss: 1.0955... Val Loss: 1.3071\n",
      "Epoch: 26/27... Step: 35790... Loss: 1.0624... Val Loss: 1.3059\n",
      "Epoch: 26/27... Step: 35800... Loss: 1.0641... Val Loss: 1.3056\n",
      "Epoch: 26/27... Step: 35810... Loss: 1.1171... Val Loss: 1.3078\n",
      "Epoch: 26/27... Step: 35820... Loss: 1.1492... Val Loss: 1.3082\n",
      "Epoch: 26/27... Step: 35830... Loss: 1.1150... Val Loss: 1.3070\n",
      "Epoch: 26/27... Step: 35840... Loss: 1.1053... Val Loss: 1.3053\n",
      "Epoch: 26/27... Step: 35850... Loss: 1.0955... Val Loss: 1.3062\n",
      "Epoch: 26/27... Step: 35860... Loss: 1.1209... Val Loss: 1.3091\n",
      "Epoch: 26/27... Step: 35870... Loss: 1.1220... Val Loss: 1.3113\n",
      "Epoch: 26/27... Step: 35880... Loss: 1.0738... Val Loss: 1.3146\n",
      "Epoch: 26/27... Step: 35890... Loss: 1.0508... Val Loss: 1.3125\n",
      "Epoch: 26/27... Step: 35900... Loss: 1.1223... Val Loss: 1.3110\n",
      "Epoch: 26/27... Step: 35910... Loss: 1.1266... Val Loss: 1.3102\n",
      "Epoch: 26/27... Step: 35920... Loss: 1.1215... Val Loss: 1.3102\n",
      "Epoch: 26/27... Step: 35930... Loss: 1.0831... Val Loss: 1.3076\n",
      "Epoch: 26/27... Step: 35940... Loss: 1.1346... Val Loss: 1.3100\n",
      "Epoch: 26/27... Step: 35950... Loss: 1.1082... Val Loss: 1.3122\n",
      "Epoch: 26/27... Step: 35960... Loss: 1.1075... Val Loss: 1.3116\n",
      "Epoch: 26/27... Step: 35970... Loss: 1.0575... Val Loss: 1.3143\n",
      "Epoch: 26/27... Step: 35980... Loss: 1.0422... Val Loss: 1.3106\n",
      "Epoch: 26/27... Step: 35990... Loss: 1.0683... Val Loss: 1.3075\n",
      "Epoch: 26/27... Step: 36000... Loss: 1.0901... Val Loss: 1.3091\n",
      "Epoch: 26/27... Step: 36010... Loss: 1.0447... Val Loss: 1.3114\n",
      "Epoch: 26/27... Step: 36020... Loss: 1.0847... Val Loss: 1.3116\n",
      "Epoch: 26/27... Step: 36030... Loss: 1.0978... Val Loss: 1.3100\n",
      "Epoch: 26/27... Step: 36040... Loss: 1.0817... Val Loss: 1.3130\n",
      "Epoch: 26/27... Step: 36050... Loss: 1.0784... Val Loss: 1.3155\n",
      "Epoch: 26/27... Step: 36060... Loss: 1.0982... Val Loss: 1.3155\n",
      "Epoch: 26/27... Step: 36070... Loss: 1.0626... Val Loss: 1.3120\n",
      "Epoch: 26/27... Step: 36080... Loss: 1.0570... Val Loss: 1.3095\n",
      "Epoch: 26/27... Step: 36090... Loss: 1.0193... Val Loss: 1.3093\n",
      "Epoch: 26/27... Step: 36100... Loss: 1.1205... Val Loss: 1.3116\n",
      "Epoch: 26/27... Step: 36110... Loss: 1.1164... Val Loss: 1.3100\n",
      "Epoch: 26/27... Step: 36120... Loss: 1.1296... Val Loss: 1.3108\n",
      "Epoch: 26/27... Step: 36130... Loss: 1.1486... Val Loss: 1.3124\n",
      "Epoch: 26/27... Step: 36140... Loss: 1.1173... Val Loss: 1.3100\n",
      "Epoch: 26/27... Step: 36150... Loss: 1.1474... Val Loss: 1.3083\n",
      "Epoch: 26/27... Step: 36160... Loss: 1.0750... Val Loss: 1.3101\n",
      "Epoch: 26/27... Step: 36170... Loss: 1.0170... Val Loss: 1.3139\n",
      "Epoch: 26/27... Step: 36180... Loss: 1.1065... Val Loss: 1.3171\n",
      "Epoch: 26/27... Step: 36190... Loss: 1.1416... Val Loss: 1.3087\n",
      "Epoch: 26/27... Step: 36200... Loss: 1.0025... Val Loss: 1.3064\n",
      "Epoch: 26/27... Step: 36210... Loss: 1.1202... Val Loss: 1.3107\n",
      "Epoch: 26/27... Step: 36220... Loss: 1.0728... Val Loss: 1.3116\n",
      "Epoch: 26/27... Step: 36230... Loss: 1.0519... Val Loss: 1.3064\n",
      "Epoch: 26/27... Step: 36240... Loss: 1.0215... Val Loss: 1.3037\n",
      "Epoch: 26/27... Step: 36250... Loss: 1.0866... Val Loss: 1.3057\n",
      "Epoch: 26/27... Step: 36260... Loss: 1.0725... Val Loss: 1.3079\n",
      "Epoch: 26/27... Step: 36270... Loss: 1.7102... Val Loss: 1.3078\n",
      "Epoch: 27/27... Step: 36280... Loss: 1.1129... Val Loss: 1.3087\n",
      "Epoch: 27/27... Step: 36290... Loss: 1.0483... Val Loss: 1.3120\n",
      "Epoch: 27/27... Step: 36300... Loss: 1.1016... Val Loss: 1.3105\n",
      "Epoch: 27/27... Step: 36310... Loss: 1.0752... Val Loss: 1.3086\n",
      "Epoch: 27/27... Step: 36320... Loss: 1.1200... Val Loss: 1.3083\n",
      "Epoch: 27/27... Step: 36330... Loss: 1.1300... Val Loss: 1.3082\n",
      "Epoch: 27/27... Step: 36340... Loss: 1.0497... Val Loss: 1.3119\n",
      "Epoch: 27/27... Step: 36350... Loss: 1.0494... Val Loss: 1.3113\n",
      "Epoch: 27/27... Step: 36360... Loss: 1.0733... Val Loss: 1.3082\n",
      "Epoch: 27/27... Step: 36370... Loss: 1.1024... Val Loss: 1.3078\n",
      "Epoch: 27/27... Step: 36380... Loss: 1.1571... Val Loss: 1.3094\n",
      "Epoch: 27/27... Step: 36390... Loss: 1.0956... Val Loss: 1.3098\n",
      "Epoch: 27/27... Step: 36400... Loss: 1.1222... Val Loss: 1.3069\n",
      "Epoch: 27/27... Step: 36410... Loss: 1.0578... Val Loss: 1.3078\n",
      "Epoch: 27/27... Step: 36420... Loss: 1.1017... Val Loss: 1.3111\n",
      "Epoch: 27/27... Step: 36430... Loss: 1.0430... Val Loss: 1.3095\n",
      "Epoch: 27/27... Step: 36440... Loss: 1.0852... Val Loss: 1.3087\n",
      "Epoch: 27/27... Step: 36450... Loss: 1.1024... Val Loss: 1.3067\n",
      "Epoch: 27/27... Step: 36460... Loss: 1.0863... Val Loss: 1.3051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/27... Step: 36470... Loss: 1.0573... Val Loss: 1.3056\n",
      "Epoch: 27/27... Step: 36480... Loss: 1.0441... Val Loss: 1.3064\n",
      "Epoch: 27/27... Step: 36490... Loss: 1.0905... Val Loss: 1.3103\n",
      "Epoch: 27/27... Step: 36500... Loss: 1.0850... Val Loss: 1.3074\n",
      "Epoch: 27/27... Step: 36510... Loss: 1.1027... Val Loss: 1.3064\n",
      "Epoch: 27/27... Step: 36520... Loss: 1.0297... Val Loss: 1.3045\n",
      "Epoch: 27/27... Step: 36530... Loss: 1.0164... Val Loss: 1.3061\n",
      "Epoch: 27/27... Step: 36540... Loss: 0.9933... Val Loss: 1.3077\n",
      "Epoch: 27/27... Step: 36550... Loss: 1.1102... Val Loss: 1.3088\n",
      "Epoch: 27/27... Step: 36560... Loss: 1.0813... Val Loss: 1.3079\n",
      "Epoch: 27/27... Step: 36570... Loss: 1.1390... Val Loss: 1.3091\n",
      "Epoch: 27/27... Step: 36580... Loss: 1.0921... Val Loss: 1.3146\n",
      "Epoch: 27/27... Step: 36590... Loss: 1.0571... Val Loss: 1.3164\n",
      "Epoch: 27/27... Step: 36600... Loss: 1.0983... Val Loss: 1.3113\n",
      "Epoch: 27/27... Step: 36610... Loss: 1.1031... Val Loss: 1.3076\n",
      "Epoch: 27/27... Step: 36620... Loss: 1.1167... Val Loss: 1.3057\n",
      "Epoch: 27/27... Step: 36630... Loss: 1.1136... Val Loss: 1.3049\n",
      "Epoch: 27/27... Step: 36640... Loss: 1.1358... Val Loss: 1.3044\n",
      "Epoch: 27/27... Step: 36650... Loss: 1.1180... Val Loss: 1.3042\n",
      "Epoch: 27/27... Step: 36660... Loss: 1.0942... Val Loss: 1.3074\n",
      "Epoch: 27/27... Step: 36670... Loss: 1.0604... Val Loss: 1.3102\n",
      "Epoch: 27/27... Step: 36680... Loss: 1.1270... Val Loss: 1.3096\n",
      "Epoch: 27/27... Step: 36690... Loss: 1.1009... Val Loss: 1.3057\n",
      "Epoch: 27/27... Step: 36700... Loss: 1.0706... Val Loss: 1.3063\n",
      "Epoch: 27/27... Step: 36710... Loss: 1.1622... Val Loss: 1.3083\n",
      "Epoch: 27/27... Step: 36720... Loss: 1.1008... Val Loss: 1.3066\n",
      "Epoch: 27/27... Step: 36730... Loss: 1.1468... Val Loss: 1.3084\n",
      "Epoch: 27/27... Step: 36740... Loss: 1.1591... Val Loss: 1.3082\n",
      "Epoch: 27/27... Step: 36750... Loss: 1.0464... Val Loss: 1.3065\n",
      "Epoch: 27/27... Step: 36760... Loss: 1.0534... Val Loss: 1.3109\n",
      "Epoch: 27/27... Step: 36770... Loss: 1.1149... Val Loss: 1.3164\n",
      "Epoch: 27/27... Step: 36780... Loss: 1.0802... Val Loss: 1.3114\n",
      "Epoch: 27/27... Step: 36790... Loss: 1.1460... Val Loss: 1.3099\n",
      "Epoch: 27/27... Step: 36800... Loss: 1.0662... Val Loss: 1.3095\n",
      "Epoch: 27/27... Step: 36810... Loss: 1.0658... Val Loss: 1.3139\n",
      "Epoch: 27/27... Step: 36820... Loss: 1.0858... Val Loss: 1.3158\n",
      "Epoch: 27/27... Step: 36830... Loss: 1.0972... Val Loss: 1.3126\n",
      "Epoch: 27/27... Step: 36840... Loss: 1.1560... Val Loss: 1.3112\n",
      "Epoch: 27/27... Step: 36850... Loss: 1.0924... Val Loss: 1.3146\n",
      "Epoch: 27/27... Step: 36860... Loss: 1.0941... Val Loss: 1.3161\n",
      "Epoch: 27/27... Step: 36870... Loss: 1.1010... Val Loss: 1.3139\n",
      "Epoch: 27/27... Step: 36880... Loss: 1.0827... Val Loss: 1.3122\n",
      "Epoch: 27/27... Step: 36890... Loss: 1.1271... Val Loss: 1.3108\n",
      "Epoch: 27/27... Step: 36900... Loss: 1.1215... Val Loss: 1.3138\n",
      "Epoch: 27/27... Step: 36910... Loss: 1.0969... Val Loss: 1.3160\n",
      "Epoch: 27/27... Step: 36920... Loss: 1.0664... Val Loss: 1.3168\n",
      "Epoch: 27/27... Step: 36930... Loss: 1.1031... Val Loss: 1.3152\n",
      "Epoch: 27/27... Step: 36940... Loss: 1.0703... Val Loss: 1.3129\n",
      "Epoch: 27/27... Step: 36950... Loss: 1.1030... Val Loss: 1.3119\n",
      "Epoch: 27/27... Step: 36960... Loss: 1.1200... Val Loss: 1.3143\n",
      "Epoch: 27/27... Step: 36970... Loss: 1.1544... Val Loss: 1.3130\n",
      "Epoch: 27/27... Step: 36980... Loss: 1.0641... Val Loss: 1.3103\n",
      "Epoch: 27/27... Step: 36990... Loss: 1.1191... Val Loss: 1.3096\n",
      "Epoch: 27/27... Step: 37000... Loss: 1.0604... Val Loss: 1.3101\n",
      "Epoch: 27/27... Step: 37010... Loss: 1.0754... Val Loss: 1.3144\n",
      "Epoch: 27/27... Step: 37020... Loss: 1.0998... Val Loss: 1.3170\n",
      "Epoch: 27/27... Step: 37030... Loss: 1.0745... Val Loss: 1.3153\n",
      "Epoch: 27/27... Step: 37040... Loss: 1.0235... Val Loss: 1.3138\n",
      "Epoch: 27/27... Step: 37050... Loss: 1.1907... Val Loss: 1.3159\n",
      "Epoch: 27/27... Step: 37060... Loss: 1.1331... Val Loss: 1.3127\n",
      "Epoch: 27/27... Step: 37070... Loss: 1.0870... Val Loss: 1.3100\n",
      "Epoch: 27/27... Step: 37080... Loss: 1.0556... Val Loss: 1.3090\n",
      "Epoch: 27/27... Step: 37090... Loss: 1.1127... Val Loss: 1.3131\n",
      "Epoch: 27/27... Step: 37100... Loss: 1.1146... Val Loss: 1.3153\n",
      "Epoch: 27/27... Step: 37110... Loss: 1.0554... Val Loss: 1.3163\n",
      "Epoch: 27/27... Step: 37120... Loss: 1.0956... Val Loss: 1.3121\n",
      "Epoch: 27/27... Step: 37130... Loss: 1.0745... Val Loss: 1.3095\n",
      "Epoch: 27/27... Step: 37140... Loss: 1.0641... Val Loss: 1.3104\n",
      "Epoch: 27/27... Step: 37150... Loss: 1.1711... Val Loss: 1.3096\n",
      "Epoch: 27/27... Step: 37160... Loss: 1.1413... Val Loss: 1.3124\n",
      "Epoch: 27/27... Step: 37170... Loss: 1.0834... Val Loss: 1.3139\n",
      "Epoch: 27/27... Step: 37180... Loss: 1.1103... Val Loss: 1.3116\n",
      "Epoch: 27/27... Step: 37190... Loss: 1.1650... Val Loss: 1.3109\n",
      "Epoch: 27/27... Step: 37200... Loss: 1.1122... Val Loss: 1.3101\n",
      "Epoch: 27/27... Step: 37210... Loss: 1.0635... Val Loss: 1.3127\n",
      "Epoch: 27/27... Step: 37220... Loss: 1.0810... Val Loss: 1.3151\n",
      "Epoch: 27/27... Step: 37230... Loss: 1.1546... Val Loss: 1.3117\n",
      "Epoch: 27/27... Step: 37240... Loss: 1.0388... Val Loss: 1.3088\n",
      "Epoch: 27/27... Step: 37250... Loss: 1.0711... Val Loss: 1.3143\n",
      "Epoch: 27/27... Step: 37260... Loss: 1.1133... Val Loss: 1.3139\n",
      "Epoch: 27/27... Step: 37270... Loss: 1.0933... Val Loss: 1.3137\n",
      "Epoch: 27/27... Step: 37280... Loss: 1.1290... Val Loss: 1.3116\n",
      "Epoch: 27/27... Step: 37290... Loss: 1.0916... Val Loss: 1.3099\n",
      "Epoch: 27/27... Step: 37300... Loss: 1.0998... Val Loss: 1.3107\n",
      "Epoch: 27/27... Step: 37310... Loss: 1.0262... Val Loss: 1.3101\n",
      "Epoch: 27/27... Step: 37320... Loss: 1.0278... Val Loss: 1.3092\n",
      "Epoch: 27/27... Step: 37330... Loss: 1.0600... Val Loss: 1.3100\n",
      "Epoch: 27/27... Step: 37340... Loss: 1.1346... Val Loss: 1.3119\n",
      "Epoch: 27/27... Step: 37350... Loss: 1.1187... Val Loss: 1.3110\n",
      "Epoch: 27/27... Step: 37360... Loss: 0.9960... Val Loss: 1.3115\n",
      "Epoch: 27/27... Step: 37370... Loss: 1.0960... Val Loss: 1.3124\n",
      "Epoch: 27/27... Step: 37380... Loss: 1.0924... Val Loss: 1.3090\n",
      "Epoch: 27/27... Step: 37390... Loss: 1.0977... Val Loss: 1.3084\n",
      "Epoch: 27/27... Step: 37400... Loss: 1.0649... Val Loss: 1.3102\n",
      "Epoch: 27/27... Step: 37410... Loss: 1.0208... Val Loss: 1.3147\n",
      "Epoch: 27/27... Step: 37420... Loss: 1.0467... Val Loss: 1.3170\n",
      "Epoch: 27/27... Step: 37430... Loss: 1.0861... Val Loss: 1.3161\n",
      "Epoch: 27/27... Step: 37440... Loss: 1.1000... Val Loss: 1.3142\n",
      "Epoch: 27/27... Step: 37450... Loss: 1.1304... Val Loss: 1.3117\n",
      "Epoch: 27/27... Step: 37460... Loss: 1.1123... Val Loss: 1.3106\n",
      "Epoch: 27/27... Step: 37470... Loss: 1.1159... Val Loss: 1.3096\n",
      "Epoch: 27/27... Step: 37480... Loss: 1.0442... Val Loss: 1.3106\n",
      "Epoch: 27/27... Step: 37490... Loss: 1.0934... Val Loss: 1.3126\n",
      "Epoch: 27/27... Step: 37500... Loss: 1.0959... Val Loss: 1.3098\n",
      "Epoch: 27/27... Step: 37510... Loss: 1.0939... Val Loss: 1.3081\n",
      "Epoch: 27/27... Step: 37520... Loss: 1.0851... Val Loss: 1.3085\n",
      "Epoch: 27/27... Step: 37530... Loss: 1.0684... Val Loss: 1.3075\n",
      "Epoch: 27/27... Step: 37540... Loss: 1.1055... Val Loss: 1.3105\n",
      "Epoch: 27/27... Step: 37550... Loss: 1.0522... Val Loss: 1.3100\n",
      "Epoch: 27/27... Step: 37560... Loss: 1.1024... Val Loss: 1.3115\n",
      "Epoch: 27/27... Step: 37570... Loss: 1.1227... Val Loss: 1.3136\n",
      "Epoch: 27/27... Step: 37580... Loss: 1.1194... Val Loss: 1.3143\n",
      "Epoch: 27/27... Step: 37590... Loss: 1.1160... Val Loss: 1.3100\n",
      "Epoch: 27/27... Step: 37600... Loss: 1.0338... Val Loss: 1.3112\n",
      "Epoch: 27/27... Step: 37610... Loss: 1.1415... Val Loss: 1.3149\n",
      "Epoch: 27/27... Step: 37620... Loss: 1.1212... Val Loss: 1.3119\n",
      "Epoch: 27/27... Step: 37630... Loss: 1.0606... Val Loss: 1.3060\n",
      "Epoch: 27/27... Step: 37640... Loss: 1.1101... Val Loss: 1.3057\n",
      "Epoch: 27/27... Step: 37650... Loss: 1.1553... Val Loss: 1.3067\n",
      "Epoch: 27/27... Step: 37660... Loss: 1.1138... Val Loss: 1.3076\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 10\n",
    "n_epochs = 27 # start smaller just to testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priming and generating text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train they can study in the world was\n",
      "the first time had before a look with the child, so that those thought to be a crop when\n",
      "she was asleep\n",
      "he had not answered what he was seeing him to anyone that\n",
      "he was ashamed, the footman\n",
      "and\n",
      "this party had time to see him that she was\n",
      "so satisfied, were in a solemal and the feeling of carriage and the compenit entering a sting, who had still\n",
      "more, and the sick man had. That would have spent the music and drawing her all these weight, and he did\n",
      "not see what to do all, his wife had been\n",
      "carrying at the same time, and he had seen them. He did not say that he diversed the portfolio, and had thruwted her he was not to speak of all that he had arrived, though their\n",
      "family\n",
      "she was seriously that he had seen his study, and serious and a merchant's secret clack of his strange thing absorbed him any one of the count, who had\n",
      "answered shrills on the pitiful position, and he\n",
      "had been a sense of tone, the future still had so the point of clever her husband was at\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='train', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have loaded in a model\n",
    "with open('rnn.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said, and the carriage he had an understand himself, as he went off to the little conception.\n",
      "\n",
      "\"Oh, yes, this woman is so a langer he doesn't say something,\" he said, sitting down by\n",
      "her.\n",
      "\n",
      "\"Yes, that's nothing?\" she asked herself.\n",
      "\"I was not so left alone, but I'm such a little, and though I can did not think of you.\"\n",
      "\n",
      "\"Oh, yes, that you\n",
      "more about it. And what,\n",
      "if I do not advise you to do, I'm going to\n",
      "the memaronity and thoughts of it....\n",
      "I'd better be the second and telling you to\n",
      "them to blame, and so introduces itself. It's not this, if I would be in subjects of\n",
      "my life, but when she came to tell her anything\n",
      "without the steps to her again, and he had\n",
      "to be dull for the same position; but and he was stupid\n",
      "indeed and that there was not telling the stream of any opportonity of the same\n",
      "past of the strange few\n",
      "masters. He felt that he had\n",
      "talked\n",
      "without stayed on at the sick man, he had better never accessing him.\n",
      "\n",
      "\"I'm not given in that woman. What's true to that, but you say. The worse he would be no mine, and her husband was the first time to say to herself that\n",
      "the contempt, and that the princess\n",
      "would not have been seeing that, when he did not let him say it instead for\n",
      "such a complete fingers with the children, which\n",
      "was stung to\n",
      "all when they were calmer, and was not thinking that his white few heavy letter.\n",
      "\n",
      "\"Then you don't understand all the coat of a man that your heart had happened. This time went out of his soul to Levin.\n",
      "And she\n",
      "went on and all went towards the little head, to bring her to the sensation was the princess, as it\n",
      "were, as he called her in a string of\n",
      "account\n",
      "of her hand on\n",
      "horseback; but she\n",
      "sat down and see all the personage,\n",
      "but they sat in\n",
      "all his family. She had not asked what wanted to do with\n",
      "several success and creatures.\n",
      "\n",
      "\"I didn't know,\" the princess asked shoulder and always trying to\n",
      "think about, and having said it, but this were too looked at her. His family straight beside the door to the children. But\n",
      "the\n",
      "painful he had been\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
